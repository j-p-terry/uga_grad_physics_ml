{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9e94a75-f431-431e-bc1f-3ca8e7d9a382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm \n",
    "from matplotlib import rc as mplrc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import PIL\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint, progress\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger, TensorBoardLogger\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, random_split\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51feab6e-a744-475a-be91-b7442966093b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1376b6c10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Set seed for reproducibility\n",
    "np.random.seed(123)\n",
    "random.seed(123)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8eb29f-e777-40ac-b933-e2326830bb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d476e7c3-373c-4ed4-8b57-a6324e1eccc1",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "### Latent Space\n",
    "A latent space is a mathematical representation of data that captures its important features or patterns. It's a way to simplify complex data and extract useful information. The information is, in a sense, compressed into a lower dimensional space that still contains enough information to perform whatever task you want. By encoding into a latent space, essential features from the data are extracted while superfluous data is ignored. Decoding from the latent space uses the essential features to reconstruct a higher dimensional output. This is the basis for many computer vision and natural language processing tasks.\n",
    "\n",
    "A common task is attempting to output the input. This can be useful for anomaly detection or denoising (with slight modifications to input data). I can, for example, take a 28x28 image, compress it through a series of convolutions into a 16 component latent space vector, then upsample it as I convolve to output a 28x28 image. In this way, I learn how to map input data into a useful representation (encoding) and map this representation into a desired ouput (decoding). This is also the basis for many language models, such as sequence-to-sequence models (translation - universal grammar?)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5abfca-c8b8-455b-989c-0ae9cb20c9cf",
   "metadata": {},
   "source": [
    "<img src=\"imgs/autoencoder_visualization.svg\" style=\"height:600px\" class=\"center\" alt=\"ae\"/><br>\n",
    "\n",
    "A simple representation of an autoencoder. Think of it as a bottle kneck into a smaller latent space and an upsample out of this space back into the output space (whatever that may be). In the above case, it's attempting to recreate the input. This is also how you can do anomaly detection. If you train on non-anomalous data, the autoencoder won't know what to do with anomalies, so there will be a larger error there. Note that this is unsupervised.\n",
    "\n",
    "<img src=\"imgs/anomaly_map.png\" style=\"height:400px\" class=\"center\" alt=\"anomaly\"/><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f202e41-8649-431d-af20-354e8a5c5a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3d5e0f5-4576-4244-9cdc-03350275e8ec",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f0e8ada-8b2b-4f4b-8946-39e621286bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download fashion MNIST\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root='./data', train=True, download=True,)\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root='./data', train=False, download=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c70fcd63-0333-4421-b09f-fda5c6b39c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Download CIFAR10\n",
    "# train_dataset = datasets.CIFAR10(\n",
    "#     root='./data', train=True, download=True,)\n",
    "# test_dataset = datasets.CIFAR10(\n",
    "#     root='./data', train=False, download=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b8b348-05aa-45de-8930-53c12e1c9b37",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e060aa7-ba26-4ce2-bd88-03a66d1df4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MNIST\n",
    "X_train = train_dataset.data.detach().numpy()\n",
    "X_test = test_dataset.data.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d657df3e-abee-49b1-a39f-07cd88100a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## cifar\n",
    "# X_train = train_dataset.data.copy()\n",
    "# X_test = test_dataset.data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5564ed80-2554-4688-98a3-e5dcbf028dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "X_train = (X_train - np.min(X_train)) / np.max(X_train - np.min(X_train))\n",
    "X_test = (X_test - np.min(X_test)) / np.max(X_test - np.min(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f54be8cb-3a89-4765-837d-43c4e8f2591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## cifar\n",
    "# X_train = np.moveaxis(X_train, -1, 1)\n",
    "# X_test = np.moveaxis(X_test, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54723644-f271-4abc-be3c-ff28a3edce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PyTorch uses Dataset objects to load the data during training and testing\n",
    "class ImageDataset(Dataset):\n",
    "\n",
    "    \"\"\"Data set\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        noisy_X: np.ndarray,\n",
    "        accelerator_name: str = \"mps\",\n",
    "        \n",
    "    ):\n",
    "        '''Assign data'''\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.noisy_X = noisy_X.astype(np.float32)\n",
    "\n",
    "        if accelerator_name == \"mps\":\n",
    "            self.device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        elif accelerator_name == \"cuda:0\":\n",
    "            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        '''function to get the length of the dataset'''\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        '''return an x, y pair'''\n",
    "        x_, noisy_x_ = self.X[idx].astype(np.float32), self.noisy_X[idx].astype(np.float32)\n",
    "\n",
    "        return torch.from_numpy(x_).float().to(self.device), torch.from_numpy(noisy_x_).float().to(self.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30beec86-be98-40cb-8135-050e48d5ddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into validation and training data\n",
    "val_split = 0.2\n",
    "X_train, X_val = train_test_split(X_train,\n",
    "                                  test_size=val_split,\n",
    "                                  random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3dba08c7-4955-42bf-9f15-5399c90d5c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### fashion MNIST\n",
    "# add channel axes (N, H, W) -> (N, C, H, W) because C = 1 in this case\n",
    "X_train = X_train[:, np.newaxis, :, :].astype(np.float32)\n",
    "X_test = X_test[:, np.newaxis, :, :].astype(np.float32)\n",
    "X_val = X_val[:, np.newaxis, :, :].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f0d61d9-d254-4ddc-9e9d-c56de802e47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_X_train = X_train.copy().astype(np.float32)\n",
    "noisy_X_test = X_test.copy().astype(np.float32)\n",
    "noisy_X_val = X_val.copy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e48aca1b-9a7c-40cc-adad-1e134bdc8fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### fashion mnist\n",
    "noise_level = 0.1\n",
    "\n",
    "## cifar\n",
    "# noise_level = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fd1d857-2178-40fe-9686-30685fdd2ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_X_train += noise_level * np.random.standard_normal(X_train.shape)\n",
    "noisy_X_train -= np.min(noisy_X_train)\n",
    "noisy_X_train /= np.max(noisy_X_train)\n",
    "\n",
    "noisy_X_val += noise_level * np.random.standard_normal(X_val.shape)\n",
    "noisy_X_val -= np.min(noisy_X_val)\n",
    "noisy_X_val /= np.max(noisy_X_val)\n",
    "\n",
    "noisy_X_test += noise_level * np.random.standard_normal(X_test.shape)\n",
    "noisy_X_test -= np.min(noisy_X_test)\n",
    "noisy_X_test /= np.max(noisy_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "519d273b-4cfe-4d4f-81a1-6e8354de48c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get H = W\n",
    "input_xy = X_train.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6437c506-d28f-4989-bfcd-b46405c4b37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7c5a18c-2f62-4f81-91db-31d315b653aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Now we actually make the dataset and dataloader in PyTorch fashion\n",
    "train_data = ImageDataset(X_train, noisy_X_train)\n",
    "val_data = ImageDataset(X_val, noisy_X_val)\n",
    "test_data = ImageDataset(X_test, noisy_X_test)\n",
    "\n",
    "# make the loader\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd4cfee-7105-413b-998c-7cad00fac219",
   "metadata": {},
   "source": [
    "# Define Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9df722-b663-4969-8131-6e17bddce346",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b570c9fd-003c-47b4-844a-1535586077ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, \n",
    "                 kernel_size: int = 3, \n",
    "                 dropout: float = 0.25,\n",
    "                 cnn_layer_dims: list = [126, 64, 32],\n",
    "                 padding: int = 1, \n",
    "                 stride: int = 2,\n",
    "                 n_channels: int = 1,\n",
    "                 input_xy: int = 28, \n",
    "                 lr: float = 1e-4, \n",
    "                 weight_decay: float = 0., \n",
    "                 eps: float = 5e-7, \n",
    "                 activation: torch.nn.modules.activation = nn.GELU(),\n",
    "                 use_wandb: bool=False,\n",
    "                 scheduler_name: str = \"none\",\n",
    "                 step_size: int = 5,\n",
    "                 gamma: float = 0.5,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        ### Always need to call above function first in order\n",
    "        ### to properly initialize a model\n",
    "        '''Basic CNN to classify fashion MNIST\n",
    "        We aren't going to both with some of the fancier stuff from the MLP, but it's easy enough to apply here too\n",
    "        '''\n",
    "        \n",
    "        # model parameters\n",
    "        self.cnn_dims = cnn_layer_dims\n",
    "        self.activation = activation\n",
    "        self.lr = lr\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.dropout = dropout\n",
    "        self.n_cnn_layers = len(cnn_layer_dims)\n",
    "        self.scheduler_name = scheduler_name\n",
    "        # if using a scheduler\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # log using WandB or TensorBoard\n",
    "        self.use_wandb = use_wandb\n",
    "        \n",
    "        # what the input data looks like (allows construction of graph for logging)\n",
    "        # (batch_size, channels, height, width)\n",
    "        self.example_input_array = torch.zeros(\n",
    "            (1, n_channels, input_xy, input_xy,),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        #### Construct the layers #####\n",
    "        ## get input layer\n",
    "        ## shape = (C, H, W) -> (cnn_layer_dim, H, W)\n",
    "        self.input_cnn = nn.Conv2d(n_channels, cnn_layer_dims[0], \n",
    "                                   kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        \n",
    "        ## make CNN hidden layers\n",
    "        self.encoding_layers = []\n",
    "        for i in range(1, self.n_cnn_layers):\n",
    "            self.encoding_layers.append(nn.Conv2d(cnn_layer_dims[i-1], cnn_layer_dims[i], \n",
    "                                       kernel_size=kernel_size, stride=stride, padding=padding))\n",
    "\n",
    "        self.encoding_layers = nn.ModuleList(self.encoding_layers)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''Determines how data is passed through the network, \n",
    "           i.e creates the connectivity of the network'''\n",
    "        \n",
    "        ## send through input layer and activate\n",
    "        x = self.activation(self.input_cnn(x))\n",
    "            \n",
    "        # pass through CNN\n",
    "        for layer in self.encoding_layers:\n",
    "            # pass through layer and activate\n",
    "            x = layer(x)\n",
    "            x = self.activation(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self) -> (list, list):\n",
    "        \"\"\"Set up the optimizer and potential learning rate scheduler\"\"\"\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            eps=self.eps,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "\n",
    "        if self.scheduler_name == \"none\":\n",
    "            return self.optimizer\n",
    "\n",
    "        ### this decreases the learning rate by a factor of gamma every step_size\n",
    "        self.scheduler = MultiStepLR(\n",
    "            self.optimizer,\n",
    "            list(range(0, self.trainer.max_epochs, self.step_size)),\n",
    "            gamma=self.gamma,\n",
    "        )\n",
    "\n",
    "        return [self.optimizer], [{\"scheduler\": self.scheduler, \"interval\": \"epoch\"}]\n",
    "        \n",
    "    #### need to add these two things in case the scheduler is used ####\n",
    "    def lr_scheduler_step(self, scheduler, metric) -> None:\n",
    "        if self.scheduler_name != \"none\":\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def on_epoch_end(self) -> None:\n",
    "        if self.scheduler_name != \"none\":\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def process_batch(self, batch, step: str = \"train\"):\n",
    "        \"\"\"Passes and logs a batch for a given type of step (test, train, validation)\"\"\"\n",
    "        \n",
    "        # get data (batch includes labels, which we don't need)\n",
    "        _, noisy_x = batch\n",
    "\n",
    "        # pass through network\n",
    "        # logits have no activation applied\n",
    "        return self(noisy_x)\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"What do do with a training batch\"\"\"\n",
    "        \n",
    "        return self.process_batch(batch, step=\"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        '''Validation step (at the end of each epoch)'''\n",
    "        \n",
    "        return self.process_batch(batch, step=\"val\")\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \n",
    "        '''Test step is essentially the same as a validation step in this instance'''\n",
    "        return self.process_batch(batch, step=\"test\")\n",
    "\n",
    "    def activation_maps(self, x, depth: int=0) -> np.ndarray:\n",
    "        \n",
    "        '''Gets output activation of an arbitary CNN layer'''\n",
    "        \n",
    "        i = 0\n",
    "        # input layer\n",
    "        x = self.activation(self.input_cnn(x))\n",
    "        \n",
    "        if depth == 0:\n",
    "            return x.detach().numpy()\n",
    "        \n",
    "        i += 1\n",
    "        x = F.max_pool2d(x, 2)\n",
    "            \n",
    "        # pass through CNN and return when you reach the appropriate depth\n",
    "        for layer in self.encoding_layers:\n",
    "            x = self.activation(layer(x))\n",
    "            if i == depth:\n",
    "                return x.detach().numpy()\n",
    "            i += 1\n",
    "            x = F.max_pool2d(x, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7caf409-ab0a-4df8-b54b-0ed5a4efeae5",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c46f1e2-36bf-4c4c-8071-61079deddf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, \n",
    "                 kernel_size: int = 3, \n",
    "                 dropout: float = 0.25,\n",
    "                 cnn_layer_dims: list = [32, 64, 128],\n",
    "                 padding: int = 1, \n",
    "                 stride: int = 2,\n",
    "                 image_input_channels: int = 1,\n",
    "                 n_input_channels: int = 128,\n",
    "                 input_xy: int = 4, \n",
    "                 lr: float = 1e-4, \n",
    "                 weight_decay: float = 0., \n",
    "                 eps: float = 5e-7, \n",
    "                 activation: torch.nn.modules.activation = F.gelu,\n",
    "                 use_wandb: bool=False,\n",
    "                 scheduler_name: str = \"none\",\n",
    "                 step_size: int = 5,\n",
    "                 gamma: float = 0.5,\n",
    "                 output_padding: int = 1,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        ### Always need to call above function first in order\n",
    "        ### to properly initialize a model\n",
    "        '''Basic CNN to classify fashion MNIST\n",
    "        We aren't going to both with some of the fancier stuff from the MLP, but it's easy enough to apply here too\n",
    "        '''\n",
    "        \n",
    "        # model parameters\n",
    "        self.cnn_dims = cnn_layer_dims\n",
    "        self.image_input_channels = image_input_channels\n",
    "        self.activation = activation\n",
    "        self.lr = lr\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.dropout = dropout\n",
    "        self.n_cnn_layers = len(cnn_layer_dims)\n",
    "        self.scheduler_name = scheduler_name\n",
    "        # if using a scheduler\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # log using WandB or TensorBoard\n",
    "        self.use_wandb = use_wandb\n",
    "\n",
    "        # what the input data looks like (allows construction of graph for logging)\n",
    "        # (batch_size, channels, height, width)\n",
    "        self.example_input_array = torch.zeros(\n",
    "            (1, n_input_channels, input_xy, input_xy,),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        #### Construct the layers #####\n",
    "        ## get input layer\n",
    "        ## shape = (C, H, W) -> (cnn_layer_dim, H, W)\n",
    "        # self.input_cnn = nn.ConvTranspose2d(n_input_channels,\n",
    "        #                                     cnn_layer_dims[0], \n",
    "        #                                     kernel_size=kernel_size, \n",
    "        #                                     # output_padding=output_padding, \n",
    "        #                                     padding=padding, stride=stride\n",
    "        #                                   )\n",
    "\n",
    "        ## make CNN hidden layers\n",
    "        self.decoding_layers = []\n",
    "        for i in range(0, self.n_cnn_layers):\n",
    "\n",
    "            ### ConvTranspose2d __upscales__ the data with output padding\n",
    "            self.decoding_layers.append(nn.ConvTranspose2d(cnn_layer_dims[i-1] if i != 0 else n_input_channels,\n",
    "                                                           cnn_layer_dims[i] if i != self.n_cnn_layers - 1 else self.image_input_channels, \n",
    "                                                           kernel_size=kernel_size, \n",
    "                                                           output_padding=output_padding, \n",
    "                                                           padding=padding, \n",
    "                                                           stride=stride\n",
    "                                                          ))\n",
    "\n",
    "            # self.decoding_layers.append(nn.Conv2d(cnn_layer_dims[i], cnn_layer_dims[i], \n",
    "                                       # kernel_size=kernel_size, stride=stride, padding=padding))\n",
    "\n",
    "        # self.decoding_layers.append(nn.ConvTranspose2d(cnn_layer_dims[-1],\n",
    "        #                                                self.image_input_channels, \n",
    "        #                                                kernel_size=kernel_size, \n",
    "        #                                                output_padding=output_padding, \n",
    "        #                                                # padding=padding, \n",
    "        #                                                stride=stride,\n",
    "        #                                             ))\n",
    "\n",
    "        # self.decoding_layers.append(nn.Tanh())\n",
    "\n",
    "        self.decoding_layers = nn.ModuleList(self.decoding_layers)\n",
    "\n",
    "        # self.init_weights()\n",
    "\n",
    "    # def init_weights(self) -> None:\n",
    "        # ### does some fancy layer weight initialization\n",
    "        # for layer in self.decoding_layers:\n",
    "        #     if isinstance(layer, nn.Linear):\n",
    "        #         nn.init.xavier_uniform_(layer.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''Determines how data is passed through the network, \n",
    "           i.e creates the connectivity of the network'''\n",
    "        ## send through input layer and activate\n",
    "        # x = self.activation(self.input_cnn(x))\n",
    "        # pass through CNN\n",
    "        for layer in self.decoding_layers:\n",
    "            # pass through layer\n",
    "            x = layer(x)\n",
    "            x = self.activation(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self) -> (list, list):\n",
    "        \"\"\"Set up the optimizer and potential learning rate scheduler\"\"\"\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            eps=self.eps,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "\n",
    "        if self.scheduler_name == \"none\":\n",
    "            return self.optimizer\n",
    "\n",
    "        ### this decreases the learning rate by a factor of gamma every step_size\n",
    "        self.scheduler = MultiStepLR(\n",
    "            self.optimizer,\n",
    "            list(range(0, self.trainer.max_epochs, self.step_size)),\n",
    "            gamma=self.gamma,\n",
    "        )\n",
    "\n",
    "        return [self.optimizer], [{\"scheduler\": self.scheduler, \"interval\": \"epoch\"}]\n",
    "        \n",
    "    #### need to add these two things in case the scheduler is used ####\n",
    "    def lr_scheduler_step(self, scheduler, metric) -> None:\n",
    "        if self.scheduler_name != \"none\":\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def on_epoch_end(self) -> None:\n",
    "        if self.scheduler_name != \"none\":\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def process_batch(self, batch, step: str = \"train\"):\n",
    "        \"\"\"Passes and logs a batch for a given type of step (test, train, validation)\"\"\"\n",
    "        \n",
    "        # get data (batch includes labels, which we don't need)\n",
    "        _, noisy_x = batch\n",
    "\n",
    "        # pass through network\n",
    "        # logits have no activation applied\n",
    "        return self(noisy_x)\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"What do do with a training batch\"\"\"\n",
    "        \n",
    "        return self.process_batch(batch, step=\"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        '''Validation step (at the end of each epoch)'''\n",
    "        \n",
    "        return self.process_batch(batch, step=\"val\")\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \n",
    "        '''Test step is essentially the same as a validation step in this instance'''\n",
    "        return self.process_batch(batch, step=\"test\")\n",
    "\n",
    "    def activation_maps(self, x, depth: int=0) -> np.ndarray:\n",
    "        \n",
    "        '''Gets output activation of an arbitary CNN layer'''\n",
    "        \n",
    "        i = 0\n",
    "        # input layer\n",
    "        x = self.activation(self.input_cnn(x))\n",
    "        \n",
    "        if depth == 0:\n",
    "            return x.detach().numpy()\n",
    "        \n",
    "        i += 1\n",
    "        # x = F.max_pool2d(x, 2)\n",
    "            \n",
    "        # pass through CNN and return when you reach the appropriate depth\n",
    "        for layer in self.decoding_layers:\n",
    "            x = self.activation(layer(x))\n",
    "            if i == depth:\n",
    "                return x.detach().numpy()\n",
    "            i += 1\n",
    "            # x = F.max_pool2d(x, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9d8a77-00d6-4f76-abc8-df7905b3d743",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Entire network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "f34768c2-8f0b-4623-8401-2b21c5b182e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, \n",
    "                 kernel_size: int = 3, \n",
    "                 dropout: float = 0.25,\n",
    "                 cnn_layer_dims: list = [128, 64, 32],\n",
    "                 padding: int = 1, \n",
    "                 stride: int = 2,\n",
    "                 image_input_channels: int = 1,\n",
    "                 latent_dim: int = 16,\n",
    "                 input_xy: int = 28, \n",
    "                 lr: float = 1e-4, \n",
    "                 weight_decay: float = 0., \n",
    "                 eps: float = 5e-7, \n",
    "                 activation: torch.nn.modules.activation = F.gelu,\n",
    "                 use_wandb: bool=False,\n",
    "                 scheduler_name: str = \"none\",\n",
    "                 step_size: int = 5,\n",
    "                 gamma: float = 0.5,\n",
    "                 output_padding: int = 1,\n",
    "                 use_l2: bool = False,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        ### Always need to call above function first in order\n",
    "        ### to properly initialize a model\n",
    "        '''Basic CNN to denoise images\n",
    "        '''\n",
    "        \n",
    "        # model parameters\n",
    "        self.cnn_dims = cnn_layer_dims\n",
    "        self.image_input_channels = image_input_channels\n",
    "        self.activation = activation\n",
    "        self.lr = lr\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.dropout = dropout\n",
    "        self.n_channels = len(cnn_layer_dims)\n",
    "        self.scheduler_name = scheduler_name\n",
    "        # if using a scheduler\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        \n",
    "        # log using WandB or TensorBoard\n",
    "        self.use_wandb = use_wandb\n",
    "\n",
    "        # add L2 regularization\n",
    "        self.use_l2 = use_l2\n",
    "\n",
    "        # what the input data looks like (allows construction of graph for logging)\n",
    "        # (batch_size, channels, height, width)\n",
    "        self.example_input_array = torch.zeros(\n",
    "            (1, image_input_channels, input_xy, input_xy,),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        self.encoder = Encoder(\n",
    "                  cnn_layer_dims=cnn_layer_dims, \n",
    "                  input_xy=input_xy, \n",
    "                  activation=activation, \n",
    "                  n_channels=n_channels,\n",
    "                  use_wandb=use_wandb,\n",
    "                  dropout=dropout, \n",
    "                  padding=padding,\n",
    "                  eps=eps, lr=lr, \n",
    "                  weight_decay=weight_decay,\n",
    "                  scheduler_name=scheduler_name,\n",
    "                  gamma=gamma,\n",
    "                  step_size=step_size,\n",
    "        )\n",
    "\n",
    "        # get output dimensions of encoder\n",
    "        self.encoded_cxy, self.flat_dim = self._flat_layer_size()\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "                  cnn_layer_dims=cnn_layer_dims[::-1], \n",
    "                  input_xy=self.encoded_cxy[-1], \n",
    "                  activation=activation, \n",
    "                  n_input_channels=cnn_layer_dims[-1],\n",
    "                  image_input_channels=image_input_channels,\n",
    "                  use_wandb=use_wandb,\n",
    "                  dropout=dropout, \n",
    "                  padding=padding,\n",
    "                  eps=eps, lr=lr, \n",
    "                  weight_decay=weight_decay,\n",
    "                  scheduler_name=scheduler_name,\n",
    "                  gamma=gamma,\n",
    "                  step_size=step_size,\n",
    "                  output_padding=output_padding,\n",
    "        )\n",
    "\n",
    "\n",
    "        self.latent_encoder = nn.Linear(self.flat_dim, self.latent_dim)\n",
    "        self.latent_decoder = nn.Linear(self.latent_dim, self.flat_dim)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        ### does some fancy layer weight initialization\n",
    "        nn.init.xavier_uniform_(self.latent_decoder.weight)\n",
    "        nn.init.xavier_uniform_(self.latent_encoder.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''Determines how data is passed through the network, \n",
    "           i.e creates the connectivity of the network'''\n",
    "        \n",
    "        # encode\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # put into latent space\n",
    "        x = nn.Flatten()(x)\n",
    "        x = self.activation(self.latent_encoder(x))\n",
    "\n",
    "        # bring out of latent space\n",
    "        x = self.activation(self.latent_decoder(x))\n",
    "\n",
    "        # decode\n",
    "        x = x.reshape(x.shape[0], \n",
    "                      -1, \n",
    "                      self.encoded_cxy[-1], \n",
    "                      self.encoded_cxy[-1])\n",
    "        \n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return nn.Sigmoid()(x)\n",
    "\n",
    "    def configure_optimizers(self) -> (list, list):\n",
    "        \"\"\"Set up the optimizer and potential learning rate scheduler\"\"\"\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            eps=self.eps,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "\n",
    "        if self.scheduler_name == \"none\":\n",
    "            return self.optimizer\n",
    "\n",
    "        ### this decreases the learning rate by a factor of gamma every step_size\n",
    "        self.scheduler = MultiStepLR(\n",
    "            self.optimizer,\n",
    "            list(range(0, self.trainer.max_epochs, self.step_size)),\n",
    "            gamma=self.gamma,\n",
    "        )\n",
    "\n",
    "        return [self.optimizer], [{\"scheduler\": self.scheduler, \"interval\": \"epoch\"}]\n",
    "        \n",
    "    #### need to add these two things in case the scheduler is used ####\n",
    "    def lr_scheduler_step(self, scheduler, metric) -> None:\n",
    "        if self.scheduler_name != \"none\":\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def on_epoch_end(self) -> None:\n",
    "        if self.scheduler_name != \"none\":\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def process_batch(self, batch, step: str = \"train\"):\n",
    "        \"\"\"Passes and logs a batch for a given type of step (test, train, validation)\"\"\"\n",
    "        \n",
    "        # get data (batch includes labels, which we don't need)\n",
    "        x, noisy_x = batch\n",
    "\n",
    "        # pass through network\n",
    "        denoised_x = self(noisy_x)\n",
    "\n",
    "        # flatten to get easy MSE\n",
    "        denoised_x = nn.Flatten()(denoised_x)\n",
    "        x = nn.Flatten()(x)\n",
    "\n",
    "        loss = self.loss_fn(x, denoised_x)\n",
    "\n",
    "        if self.use_l2:\n",
    "            # Add L2 regularization to the loss\n",
    "            l2_regularization = 0.0  # You can adjust the regularization strength\n",
    "            for param in self.parameters():\n",
    "                l2_regularization += torch.norm(param, 2)  # L2 norm of each parameter\n",
    "            self.log(f\"{step}_L2\", l2_regularization)\n",
    "            loss += l2_regularization\n",
    "\n",
    "        self.log(f\"{step}_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"What do do with a training batch\"\"\"\n",
    "        \n",
    "        return self.process_batch(batch, step=\"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        '''Validation step (at the end of each epoch)'''\n",
    "        \n",
    "        return self.process_batch(batch, step=\"val\")\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \n",
    "        '''Test step is essentially the same as a validation step in this instance'''\n",
    "        return self.process_batch(batch, step=\"test\")\n",
    "\n",
    "    def _flat_layer_size(self) -> int:\n",
    "        \n",
    "        '''Gets the dimension of the flattened CNN output layer'''\n",
    "        \n",
    "        x = self.example_input_array\n",
    "        \n",
    "        # Pass the input tensor through the CNN layers\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        array_dim = x.size()\n",
    "        # Calculate the flattened layer dimension\n",
    "        flattened_dim = x.view(1, -1).size(1)\n",
    "\n",
    "        return array_dim, flattened_dim\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef01ded2-09ea-44bb-bbff-ed413b806860",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "id": "c3e21714-ccb9-401e-9ba6-6d077aeb863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyper parameters (fashion MNIST)\n",
    "lr = 1e-3\n",
    "eps = 1e-8\n",
    "weight_decay = 1e-6\n",
    "dropout = 0.25\n",
    "cnn_layer_dims  = [128, 64,]\n",
    "n_cnn_layers = len(cnn_layer_dims)\n",
    "latent_dim = 16\n",
    "activation = F.gelu\n",
    "n_channels = 1\n",
    "stride = 2\n",
    "kernel_size = 3\n",
    "padding = 1\n",
    "output_padding = 1\n",
    "scheduler_name = \"step\"\n",
    "gamma = 0.5\n",
    "step_size = 5\n",
    "use_l2 = False # L2 regularization\n",
    "\n",
    "## WandB stuff\n",
    "# log with WandB or TensorBoard\n",
    "use_wandb = False\n",
    "# do hyperparameter sweep with WandB\n",
    "use_sweep = False\n",
    "# WandB project name\n",
    "project_name = \"FashioMNIST_AE\"\n",
    "# WandB lab name\n",
    "entity = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "181cff3d-1554-474d-bb59-3c007845f0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model hyper parameters \n",
    "# lr = 5e-4\n",
    "# eps = 1e-8\n",
    "# weight_decay = 1e-6\n",
    "# dropout = 0.25\n",
    "# cnn_layer_dims  = [128, 64, 32,]\n",
    "# n_cnn_layers = len(cnn_layer_dims)\n",
    "# latent_dim = 16\n",
    "# activation = F.gelu\n",
    "# n_channels = 3\n",
    "# stride = 2\n",
    "# kernel_size = 3\n",
    "# padding = 1\n",
    "# output_padding = 1\n",
    "# scheduler_name = \"step\"\n",
    "# gamma = 0.5\n",
    "# step_size = 5\n",
    "# use_l2 = False # L2 regularization\n",
    "\n",
    "# ## WandB stuff\n",
    "# # log with WandB or TensorBoard\n",
    "# use_wandb = False\n",
    "# # do hyperparameter sweep with WandB\n",
    "# use_sweep = False\n",
    "# # WandB project name\n",
    "# project_name = 'CIFAR10_AE'\n",
    "# # WandB lab name\n",
    "# entity = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "6e3dcfd0-39cc-43c2-806f-be7df9ce685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder(latent_dim=latent_dim,\n",
    "                  cnn_layer_dims=cnn_layer_dims, \n",
    "                  input_xy=input_xy, \n",
    "                  activation=activation, \n",
    "                  image_input_channels=n_channels,\n",
    "                  use_wandb=use_wandb,\n",
    "                  dropout=dropout, \n",
    "                  eps=eps, \n",
    "                  lr=lr, \n",
    "                  weight_decay=weight_decay,\n",
    "                  scheduler_name=scheduler_name,\n",
    "                  gamma=gamma,\n",
    "                  step_size=step_size,\n",
    "                  stride=stride,\n",
    "                  padding=padding,\n",
    "                  output_padding=output_padding,\n",
    "                  use_l2=use_l2,\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb6a899-eb09-407d-9af0-476bc6f42fbc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "23090e19-681d-4286-867b-12598da0ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "aab58c32-b400-40ab-8eaa-b5a64160bb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator_name = \"mps\"\n",
    "# accelerator_name = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "f15ca5fe-8bd4-4e27-91e1-f35cb533df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boilerplate to get GPU if possible\n",
    "if accelerator_name == \"mps\":\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "elif accelerator_name == \"cuda\":\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "44fcca20-f958-4675-8659-e9d68ebaa791",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_wandb:\n",
    "    # %load_ext tensorboard\n",
    "    # cnn_logger = TensorBoardLogger(\"ae_logs\", name=\"simple_cifar_fashion_ae\")\n",
    "    cnn_logger = TensorBoardLogger(\"ae_logs\", name=\"simple_mnist_fashion_ae\")\n",
    "    run_name = \"ae_cnn\"\n",
    "else:\n",
    "    logger_kwargs = {\n",
    "        \"resume\": \"allow\",\n",
    "        \"config\": model_hparams,\n",
    "    }\n",
    "    cnn_logger = WandbLogger(project=project_name, entity=entity, **logger_kwargs)\n",
    "    cnn_run_name = cnn_logger.experiment.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "1cf356a7-8b6f-4b19-8a77-4240a247322e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "#### necessary for newer PTL versions\n",
    "devices = 1\n",
    "accelerator = \"gpu\" if devices == 1 else \"cpu\"\n",
    "\n",
    "# make the trainer\n",
    "trainer = pl.Trainer(\n",
    "    devices=devices,\n",
    "    accelerator=accelerator,\n",
    "    max_epochs=num_epochs,\n",
    "    log_every_n_steps=1,\n",
    "    logger=cnn_logger,\n",
    "    # reload_dataloaders_every_epoch=True,\n",
    "    callbacks=[\n",
    "        # ModelCheckpoint(\n",
    "        #     save_weights_only=False,\n",
    "        #     mode=\"min\",\n",
    "        #     monitor=\"val_acc\",\n",
    "        #     save_top_k=1,\n",
    "        #     every_n_epochs=1,\n",
    "        #     save_on_train_epoch_end=False,\n",
    "        #     dirpath=f\"/AE_Checkpoints/{run_name}/\",\n",
    "        #     filename=f\"ae_checkpoint_{run_name}\",\n",
    "        # ),\n",
    "        LearningRateMonitor(\"epoch\"),\n",
    "        progress.TQDMProgressBar(refresh_rate=1),\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            min_delta=0,\n",
    "            patience=10,\n",
    "            verbose=False,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "trainer.logger._log_graph = True\n",
    "trainer.logger._default_hp_metric = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "454572d4-d391-42b6-8ceb-757b8c5124a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "c39e4c9c-946c-4042-8a55-0292cfa27dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name           | Type    | Params | In sizes       | Out sizes     \n",
      "-----------------------------------------------------------------------------\n",
      "0 | loss_fn        | MSELoss | 0      | ?              | ?             \n",
      "1 | encoder        | Encoder | 75.1 K | [1, 1, 28, 28] | [1, 64, 7, 7] \n",
      "2 | decoder        | Decoder | 37.5 K | [1, 64, 7, 7]  | [1, 1, 28, 28]\n",
      "3 | latent_encoder | Linear  | 50.2 K | [1, 3136]      | [1, 16]       \n",
      "4 | latent_decoder | Linear  | 53.3 K | [1, 16]        | [1, 3136]     \n",
      "-----------------------------------------------------------------------------\n",
      "216 K     Trainable params\n",
      "0         Non-trainable params\n",
      "216 K     Total params\n",
      "0.864     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07337e43eb204b21bab78f7f62a88785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c2cb8d-227c-45a2-b100-23800bf3bc42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94471401-4adb-4fba-a579-2a453569cb87",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "07e8c71c-c1d1-4ab3-992f-48412ebbcbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonterry/miniforge3/envs/pytorch_python10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1907b2f16b4db3bc56443015215e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_loss          0.011740208603441715\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[{'test_loss': 0.011740208603441715}]\n"
     ]
    }
   ],
   "source": [
    "## Get test metrics\n",
    "test_results = trainer.test(model, test_loader)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "d08360f9-1b26-41d2-a981-78f861982ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-610e4ad509c47055\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-610e4ad509c47055\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## open up TensorBoard\n",
    "if not use_wandb:\n",
    "    %tensorboard --logdir ae_logs --port 6008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb10142f-3634-492b-a770-7c9d15d80735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee4841c2-4f9f-481d-abb3-34b34fc11281",
   "metadata": {},
   "source": [
    "## Look at images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "735c0a72-6b46-4a43-ad72-3e7d3cabe4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting a prameters\n",
    "scale_factor = 1.5\n",
    "\n",
    "labels = 16 * scale_factor\n",
    "ticks = 10 * scale_factor\n",
    "# ticks = 10 * scale_factor\n",
    "legends = 12 * scale_factor\n",
    "text = 14 * scale_factor\n",
    "titles = 22 * scale_factor\n",
    "lw = 3 * scale_factor\n",
    "ps = 200 * scale_factor\n",
    "cmap = 'magma'\n",
    "\n",
    "colors = ['firebrick', 'steelblue', 'darkorange', 'darkviolet', 'cyan', 'magenta', 'darkgreen', 'deeppink']\n",
    "markers = ['x', 'o', '+', '>', '*', 'D', '4']\n",
    "linestyles = ['-', '--', ':', '-.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "id": "980120f4-4106-419a-992a-232126aab4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_panel(images: list, labels: list=[\"Noisy\", \"Denoised\", \"Clean\"], cmap: str='viridis', show_ticks: bool=True):\n",
    "    \n",
    "    '''Plots a 3x1 panel of images'''\n",
    "    \n",
    "    mplrc('xtick', labelsize=ticks) \n",
    "    mplrc('ytick', labelsize=ticks)\n",
    "    mplrc('axes', titlesize=titles)\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(16., 12.))\n",
    "    \n",
    "    for (i, image) in enumerate(images):\n",
    "        ax = axs[i]\n",
    "        \n",
    "        ax.imshow(image, cmap=cmap)\n",
    "        if len(labels) > i and labels[i] != '':\n",
    "            ax.set_title(labels[i])\n",
    "            \n",
    "        if not show_ticks:\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        elif i > 0:\n",
    "            ax.set_yticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "id": "e9d76588-ba7a-4f1f-81b1-7c64eeae70dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do inference on test set\n",
    "## need to turn into torch tensor first\n",
    "# X_test_infer = torch.from_numpy(X_test).float()\n",
    "X_noisy_test_infer = torch.from_numpy(noisy_X_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "4e8e9f50-ea06-47cb-b584-6f57ed83cf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_noisy_pred_tensor = model(X_noisy_test_infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "id": "f42d9796-af35-453b-b112-82d2b852c3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_noisy_pred = X_noisy_pred_tensor.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "id": "d4009d53-c935-482a-82ef-e3a370d6ed0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1200 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 0\n",
    "plot_image_panel([noisy_X_test[i].squeeze(), X_noisy_pred[i].squeeze(), X_test[i].squeeze()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a719a387-3e36-4b8c-ad26-2ec6ff7eef1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef601107-b660-4726-b727-77d0781457b5",
   "metadata": {},
   "source": [
    "# Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79af75e9-d60e-4d6a-805d-a9f612e9715f",
   "metadata": {},
   "source": [
    "Autoencoders learn deterministic mapping into and out of the latent space. VAEs learn *probabilistic* mapping. That is, they learn a latent space distribution (often a normal distribution). The encoder maps into this (learned) distribution, and the decoder samples out of this distribution to give a final result.\n",
    "\n",
    "The extra power here comes from the probabilistic latent space. The network is (hopefully) trained such that any sample from the latent space can be decoded properly. If I put in Gaussian noise into the decoder, I should get something coherent out (like an approximate MNIST image). This allows the generation of an arbitrary number of outputs.\n",
    "\n",
    "<img src=\"imgs/vae.png\" style=\"height:300px\" class=\"center\" alt=\"vae\"/><br>\n",
    "\n",
    "Schematic of VAE. Everything is done probabilistically. The encoder outputs a mean ($\\mu$) and standard deviations ($\\sigma$) that describe a normal distrubtion, sampling outputs from this distrubtion. This is equivalent to sampling latent variables that are passed through the decoder. The decoder takes multiple samples from this latent space and decodes them into the output space. This defines an output *distrubtion*. The training objective for a VAE is to maximize the evidence lower bound (ELBO), which is a lower-bound approximation to the log-likelihood of the data (we use log-likelihood because it turns multiplication into addition). \n",
    "\n",
    "$$\\mathrm{ELBO} = \\mathbb{E}[log(p(x | z))] - KL[q(z|x) || p(z)]$$\n",
    "\n",
    "$$KL[q(z|x) || p(z)] = \\frac{1}{2} \\left[\\sum_{i = 1}^{D}(\\sigma_{i}^{2} + \\mu_{i}^{2} - log(\\sigma_{i}^{2}) -1)\\right]$$ where $\\sigma_{i}$ and $\\mu_{i}$ are the standard deviation and mean of the normal distrubtion describing the $i^{th}$ component of the latent space. The KL divergence means the divergence of $q(z|x)$ from $p(z)$, i.e., measures the distribution of the latent space given the input and the expected output (assumed normal distribution with $\\mu = 0$ and $\\sigma = 1$). This encourages the encoder to approximate a standard normal distrubtion while giving it the freedom to add some minor adjustments. \n",
    "\n",
    "$\\mathbb{E}[log(p(x | z))]$ represents the reconstruction loss (not MSE!). Reconstruction loss is an attempt to maximize the likelihood of generating the observed data from the latent space. The intuition here is that reconstruction loss attempts to recreate the data while the KL divergence attempts to create a general distribution. So there's a tradeoff between reconstruction and generalizing.\n",
    "\n",
    "After training, we can simple feed Gaussian noise into the decoder to generate novel samples. This is the first example of generative networks that we have seen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e52e014-0c89-43a0-b32e-e7ce32414b81",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4a1e4f24-b267-409d-ae3c-397e0d2a89e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kl_divergence(z: torch.Tensor, mu: torch.Tensor, sigma: torch.Tensor) -> float:\n",
    "    \"\"\"This calculates the KL divergence\"\"\"\n",
    "\n",
    "    return 0.5 * (sigma ** 2 + mu ** 2 - torch.log(sigma) - 1.).sum()\n",
    "\n",
    "\n",
    "def get_reconstruction_loss(x: torch.Tensor, decoded_x: torch.Tensor, epsilon: nn.Parameter) -> torch.Tensor:\n",
    "    \"\"\"Gets reconstruction loss of latent space from a standard distrubtion\"\"\"\n",
    "\n",
    "    # get predicted distribtion\n",
    "    exp_scale = torch.exp(epsilon)\n",
    "    dist = torch.distributions.Normal(decoded_x, exp_scale)\n",
    "\n",
    "    # measure get p(x|z) using the predicted distribution\n",
    "    log_pxz = dist.log_prob(x)\n",
    "    \n",
    "    return -log_pxz.sum(dim=(1, 2, 3)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c1ef42-278d-4944-bb9e-4e2b4b4272c1",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7903dc85-5adb-4440-be1a-fb6b6aa6bd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, \n",
    "                 kernel_size: int = 3, \n",
    "                 dropout: float = 0.25,\n",
    "                 cnn_layer_dims: list = [128, 64, 32],\n",
    "                 padding: int = 1, \n",
    "                 stride: int = 2,\n",
    "                 image_input_channels: int = 1,\n",
    "                 latent_dim: int = 32,\n",
    "                 input_xy: int = 28, \n",
    "                 lr: float = 1e-4, \n",
    "                 weight_decay: float = 0., \n",
    "                 eps: float = 5e-7, \n",
    "                 activation: torch.nn.modules.activation = F.gelu,\n",
    "                 use_wandb: bool=False,\n",
    "                 scheduler_name: str = \"none\",\n",
    "                 step_size: int = 5,\n",
    "                 gamma: float = 0.5,\n",
    "                 output_padding: int = 1,\n",
    "                 kl_weight: float = 1.,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        ### Always need to call above function first in order\n",
    "        ### to properly initialize a model\n",
    "        '''Basic CNN to classify fashion MNIST\n",
    "        We aren't going to both with some of the fancier stuff from the MLP, but it's easy enough to apply here too\n",
    "        '''\n",
    "        \n",
    "        # model parameters\n",
    "        self.cnn_dims = cnn_layer_dims\n",
    "        self.image_input_channels = image_input_channels\n",
    "        self.activation = activation\n",
    "        self.lr = lr\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.dropout = dropout\n",
    "        self.n_channels = len(cnn_layer_dims)\n",
    "        self.scheduler_name = scheduler_name\n",
    "        # if using a scheduler\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # standard normal (attempted p(z))\n",
    "        self.normal_distribtion = torch.distributions.Normal(0., 1.)\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.kl_weight = kl_weight\n",
    "        \n",
    "        # log using WandB or TensorBoard\n",
    "        self.use_wandb = use_wandb\n",
    "\n",
    "        # what the input data looks like (allows construction of graph for logging)\n",
    "        # (batch_size, channels, height, width)\n",
    "        self.example_input_array = torch.zeros(\n",
    "            (1, image_input_channels, input_xy, input_xy,),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "\n",
    "        # we still use our old encoder/decoder\n",
    "        self.encoder = Encoder(\n",
    "                  cnn_layer_dims=cnn_layer_dims, \n",
    "                  input_xy=input_xy, \n",
    "                  activation=activation, \n",
    "                  n_channels=n_channels,\n",
    "                  use_wandb=use_wandb,\n",
    "                  dropout=dropout, \n",
    "                  padding=padding,\n",
    "                  eps=eps, lr=lr, \n",
    "                  weight_decay=weight_decay,\n",
    "                  scheduler_name=scheduler_name,\n",
    "                  gamma=gamma,\n",
    "                  step_size=step_size,\n",
    "        )\n",
    "\n",
    "        # get output dimensions of encoder\n",
    "        self.encoded_cxy, self.flat_dim = self._flat_layer_size()\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "                  cnn_layer_dims=cnn_layer_dims[::-1], \n",
    "                  input_xy=self.encoded_cxy[-1], \n",
    "                  activation=activation, \n",
    "                  n_input_channels=cnn_layer_dims[-1],\n",
    "                  image_input_channels=image_input_channels,\n",
    "                  use_wandb=use_wandb,\n",
    "                  dropout=dropout, \n",
    "                  padding=padding,\n",
    "                  eps=eps, lr=lr, \n",
    "                  weight_decay=weight_decay,\n",
    "                  scheduler_name=scheduler_name,\n",
    "                  gamma=gamma,\n",
    "                  step_size=step_size,\n",
    "                  output_padding=output_padding,\n",
    "        )\n",
    "\n",
    "\n",
    "        self.mu = nn.Linear(self.flat_dim, self.latent_dim)\n",
    "        self.sigma = nn.Linear(self.flat_dim, self.latent_dim)\n",
    "        self.epsilon = nn.Parameter(torch.Tensor([0.]))\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        ### does some fancy layer weight initialization\n",
    "        nn.init.xavier_uniform_(self.mu.weight)\n",
    "        nn.init.xavier_uniform_(self.sigma.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''Determines how data is passed through the network, \n",
    "           i.e creates the connectivity of the network'''\n",
    "        \n",
    "        # encode\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # put into latent space\n",
    "        x = nn.Flatten()(x)\n",
    "\n",
    "        # get distribution of latent space\n",
    "        mu_hat = self.mu(x)\n",
    "        sigma_hat = torch.exp(self.sigma(x))\n",
    "\n",
    "        # decode\n",
    "        x = x.reshape(x.shape[0], \n",
    "                      -1, \n",
    "                      self.encoded_cxy[-1], \n",
    "                      self.encoded_cxy[-1])\n",
    "\n",
    "        # import pdb; pdb.set_trace()\n",
    "        \n",
    "        x = self.decoder(x)\n",
    "        return nn.Sigmoid()(x), mu_hat, sigma_hat\n",
    "\n",
    "    def configure_optimizers(self) -> (list, list):\n",
    "        \"\"\"Set up the optimizer and potential learning rate scheduler\"\"\"\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            eps=self.eps,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "\n",
    "        if self.scheduler_name == \"none\":\n",
    "            return self.optimizer\n",
    "\n",
    "        ### this decreases the learning rate by a factor of gamma every step_size\n",
    "        self.scheduler = MultiStepLR(\n",
    "            self.optimizer,\n",
    "            list(range(0, self.trainer.max_epochs, self.step_size)),\n",
    "            gamma=self.gamma,\n",
    "        )\n",
    "\n",
    "        return [self.optimizer], [{\"scheduler\": self.scheduler, \"interval\": \"epoch\"}]\n",
    "        \n",
    "    #### need to add these two things in case the scheduler is used ####\n",
    "    def lr_scheduler_step(self, scheduler, metric) -> None:\n",
    "        if self.scheduler_name != \"none\":\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def on_epoch_end(self) -> None:\n",
    "        if self.scheduler_name != \"none\":\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def process_batch(self, batch, step: str = \"train\"):\n",
    "        \"\"\"Passes and logs a batch for a given type of step (test, train, validation)\"\"\"\n",
    "        \n",
    "        # get data (batch includes labels, which we don't need)\n",
    "        x, _ = batch\n",
    "\n",
    "        # pass through network\n",
    "        x_pred, mu_hat, sigma_hat = self(x)\n",
    "\n",
    "        # get latent variables\n",
    "        z = mu_hat + sigma_hat * self.normal_distribtion.sample(mu_hat.shape).to(self.device)\n",
    "\n",
    "        recon_loss = get_reconstruction_loss(x, x_pred, self.epsilon)#.item()\n",
    "        kl_divergence = get_kl_divergence(z, mu_hat, sigma_hat)#.item()\n",
    "\n",
    "        loss = recon_loss + self.kl_weight * kl_divergence\n",
    "\n",
    "        self.log(f\"{step}_loss\", loss)\n",
    "        self.log(f\"{step}_avg_reconstruction_loss\", recon_loss)\n",
    "        self.log(f\"{step}_kl_divergence\", kl_divergence)\n",
    "        self.log(f\"{step}_avg_mu\", mu_hat.mean().item())\n",
    "        self.log(f\"{step}_avg_sigma\", sigma_hat.mean().item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"What do do with a training batch\"\"\"\n",
    "        \n",
    "        return self.process_batch(batch, step=\"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        '''Validation step (at the end of each epoch)'''\n",
    "        \n",
    "        return self.process_batch(batch, step=\"val\")\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \n",
    "        '''Test step is essentially the same as a validation step in this instance'''\n",
    "        return self.process_batch(batch, step=\"test\")\n",
    "\n",
    "    def _flat_layer_size(self) -> int:\n",
    "        \n",
    "        '''Gets the dimension of the flattened CNN output layer'''\n",
    "        \n",
    "        x = self.example_input_array\n",
    "        \n",
    "        # Pass the input tensor through the CNN layers\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        array_dim = x.size()\n",
    "        # Calculate the flattened layer dimension\n",
    "        flattened_dim = x.view(1, -1).size(1)\n",
    "\n",
    "        return array_dim, flattened_dim\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265a3839-81db-4479-90c4-9d7e4ec4d55e",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "119de118-354c-49e1-9b0e-0163438dcd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyper parameters (fashion MNIST)\n",
    "lr = 1e-3\n",
    "eps = 1e-8\n",
    "weight_decay = 1e-6\n",
    "dropout = 0.25\n",
    "cnn_layer_dims  = [128, 64,]\n",
    "n_cnn_layers = len(cnn_layer_dims)\n",
    "latent_dim = 32\n",
    "activation = F.gelu\n",
    "n_channels = 1\n",
    "stride = 2\n",
    "kernel_size = 3\n",
    "padding = 1\n",
    "output_padding = 1\n",
    "scheduler_name = \"step\"\n",
    "gamma = 0.5\n",
    "step_size = 5\n",
    "use_l2 = False # L2 regularization\n",
    "\n",
    "## WandB stuff\n",
    "# log with WandB or TensorBoard\n",
    "use_wandb = False\n",
    "# do hyperparameter sweep with WandB\n",
    "use_sweep = False\n",
    "# WandB project name\n",
    "project_name = \"FashioMNIST_AE\"\n",
    "# WandB lab name\n",
    "entity = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "128731a0-fe89-4316-89ef-3fb81e44b999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model hyper parameters  (cifar)\n",
    "# lr = 1e-3\n",
    "# eps = 1e-8\n",
    "# weight_decay = 1e-7\n",
    "# dropout = 0.25\n",
    "# cnn_layer_dims  = [128, 64, 32,]\n",
    "# n_cnn_layers = len(cnn_layer_dims)\n",
    "# latent_dim = 32\n",
    "# activation = F.gelu\n",
    "# n_channels = 3\n",
    "# stride = 2\n",
    "# kernel_size = 3\n",
    "# padding = 1\n",
    "# output_padding = 1\n",
    "# scheduler_name = \"step\"\n",
    "# gamma = 0.5\n",
    "# step_size = 5\n",
    "\n",
    "# ## WandB stuff\n",
    "# # log with WandB or TensorBoard\n",
    "# use_wandb = False\n",
    "# # do hyperparameter sweep with WandB\n",
    "# use_sweep = False\n",
    "# # WandB project name\n",
    "# project_name = 'CIFAR_VAE'\n",
    "# # WandB lab name\n",
    "# entity = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a28b3149-62d9-4200-8ad7-e0a7ca0177e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model = VariationalAutoencoder(latent_dim=latent_dim,\n",
    "                  cnn_layer_dims=cnn_layer_dims, \n",
    "                  input_xy=input_xy, \n",
    "                  activation=activation, \n",
    "                  image_input_channels=n_channels,\n",
    "                  use_wandb=use_wandb,\n",
    "                  dropout=dropout, \n",
    "                  eps=eps, \n",
    "                  lr=lr, \n",
    "                  weight_decay=weight_decay,\n",
    "                  scheduler_name=scheduler_name,\n",
    "                  gamma=gamma,\n",
    "                  step_size=step_size,\n",
    "                  stride=stride,\n",
    "                  padding=padding,\n",
    "                  output_padding=output_padding,\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d1e623-010b-4bbc-8fe6-2e9c8cf0ea32",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "406fb799-ba31-49b6-be22-cdefad32463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8242d068-da74-49cf-a851-c5589af1cf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator_name = \"mps\"\n",
    "# accelerator_name = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a77a820a-7bad-4647-99dd-82fbabff95f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boilerplate to get GPU if possible\n",
    "if accelerator_name == \"mps\":\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "elif accelerator_name == \"cuda\":\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ab7b73b9-571f-4f01-a551-38f145142074",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model = vae_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6729aad4-5aa1-400b-928d-04a7750c0cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VariationalAutoencoder(\n",
       "  (encoder): Encoder(\n",
       "    (input_cnn): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (encoding_layers): ModuleList(\n",
       "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (decoding_layers): ModuleList(\n",
       "      (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (1): ConvTranspose2d(64, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (mu): Linear(in_features=3136, out_features=32, bias=True)\n",
       "  (sigma): Linear(in_features=3136, out_features=32, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11fab0e8-2ab0-4645-8b51-a65038bfe2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_wandb:\n",
    "    # %load_ext tensorboard\n",
    "    vae_logger = TensorBoardLogger(\"vae_logs\", name=\"simple_cifar_vae\")\n",
    "    run_name = \"vae_cnn\"\n",
    "else:\n",
    "    logger_kwargs = {\n",
    "        \"resume\": \"allow\",\n",
    "        \"config\": model_hparams,\n",
    "    }\n",
    "    vae_logger = WandbLogger(project=project_name, entity=entity, **logger_kwargs)\n",
    "    vae_run_name = vae_logger.experiment.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "08acbadf-fd8e-47fb-b263-b7f0036604e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "#### necessary for newer PTL versions\n",
    "devices = 1\n",
    "accelerator = \"cpu\" if device.type == \"cpu\" else \"gpu\"\n",
    "\n",
    "# make the trainer\n",
    "vae_trainer = pl.Trainer(\n",
    "    devices=devices,\n",
    "    accelerator=accelerator,\n",
    "    max_epochs=num_epochs,\n",
    "    log_every_n_steps=1,\n",
    "    logger=vae_logger,\n",
    "    # reload_dataloaders_every_epoch=True,\n",
    "    callbacks=[\n",
    "        # ModelCheckpoint(\n",
    "        #     save_weights_only=False,\n",
    "        #     mode=\"min\",\n",
    "        #     monitor=\"val_acc\",\n",
    "        #     save_top_k=1,\n",
    "        #     every_n_epochs=1,\n",
    "        #     save_on_train_epoch_end=False,\n",
    "        #     dirpath=f\"/AE_Checkpoints/{run_name}/\",\n",
    "        #     filename=f\"ae_checkpoint_{run_name}\",\n",
    "        # ),\n",
    "        LearningRateMonitor(\"epoch\"),\n",
    "        progress.TQDMProgressBar(refresh_rate=1),\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            min_delta=0,\n",
    "            patience=10,\n",
    "            verbose=False,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "vae_trainer.logger._log_graph = True\n",
    "vae_trainer.logger._default_hp_metric = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e42c144-96a5-4551-9f00-2739cc1a594e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name    | Type    | Params | In sizes       | Out sizes     \n",
      "----------------------------------------------------------------------\n",
      "0 | encoder | Encoder | 75.1 K | [1, 1, 28, 28] | [1, 64, 7, 7] \n",
      "1 | decoder | Decoder | 37.5 K | [1, 64, 7, 7]  | [1, 1, 28, 28]\n",
      "2 | mu      | Linear  | 100 K  | [1, 3136]      | [1, 32]       \n",
      "3 | sigma   | Linear  | 100 K  | [1, 3136]      | [1, 32]       \n",
      "----------------------------------------------------------------------\n",
      "313 K     Trainable params\n",
      "0         Non-trainable params\n",
      "313 K     Total params\n",
      "1.253     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def7811ef5fb4d6ab36a09013aae62cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vae_trainer.fit(vae_model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456d2ab7-0352-406a-b8b6-b07f6449b458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f05e78c4-690e-408f-bcaf-6ac83b08af12",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e425c4c-7247-4622-99ff-1c4acab34f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get test metrics\n",
    "vae_test_results = vae_trainer.test(vae_model, test_loader)\n",
    "print(vae_test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c19e81-ccad-46e4-b03b-871b3fc58791",
   "metadata": {},
   "outputs": [],
   "source": [
    "## open up TensorBoard\n",
    "if not use_wandb:\n",
    "    %tensorboard --logdir vae_logs --port 6004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81081204-52be-47de-ac92-0660a6412c74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92176d09-de4b-434c-b7cf-940c5c5c2a4b",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d473cbc-5cf5-4cc2-b7a1-b971e365dca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_input = torch.randn(vae_model.encoded_cxy).float()#.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8788f32e-574b-44c3-8389-9c7388d2c7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CIFAR\n",
    "# random_output = 255 * np.moveaxis(nn.ReLU()(vae_model.decoder(random_input)).detach().cpu().numpy().squeeze(), 0, -1)\n",
    "\n",
    "## mnist\n",
    "random_output = vae_model.decoder(random_input)).detach().cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7e2000-b263-4e7c-a81b-15369db2403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(random_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6b2e38-658d-4871-b816-8f0f3ce98421",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_preds = []\n",
    "for i in range(3):\n",
    "    random_input = torch.randn(vae_model.encoded_cxy).float()#.to(device)\n",
    "    # random_output = 255 * np.moveaxis(nn.ReLU()(vae_model.decoder(random_input)).detach().cpu().numpy().squeeze(), 0, -1)\n",
    "    random_output = vae_model.decoder(random_input)).detach().cpu().numpy().squeeze()\n",
    "    random_preds.append(random_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3b8d11-bdea-4b99-bfff-bee3b39c4141",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image_panel(random_preds, labels=[\"\", \"\", \"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef085d1-73ce-47ef-a873-c621706f882e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
