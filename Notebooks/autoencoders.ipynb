{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9e94a75-f431-431e-bc1f-3ca8e7d9a382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm \n",
    "from matplotlib import rc as mplrc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import PIL\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint, progress\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger, TensorBoardLogger\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, random_split\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51feab6e-a744-475a-be91-b7442966093b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x134b424d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Set seed for reproducibility\n",
    "np.random.seed(123)\n",
    "random.seed(123)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d476e7c3-373c-4ed4-8b57-a6324e1eccc1",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "### Latent Space\n",
    "A latent space is a mathematical representation of data that captures its important features or patterns. It's a way to simplify complex data and extract useful information. The information is, in a sense, compressed into a lower dimensional space that still contains enough information to perform whatever task you want. A common task is attempting to output the input. This can be useful for anomaly detection or denoising (with slight modifications to input data). I can, for example, take a 28x28 image, compress it through a series of convolutions into a 16 component latent space vector, then upsample it as I convolve to output a 28x28 image. In this way, I learn how to map input data into a useful representation (encoding) and map this representation into a desired ouput (decoding). This is also the basis for many language models, such as sequence-to-sequence models (translation - universal grammar?)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5abfca-c8b8-455b-989c-0ae9cb20c9cf",
   "metadata": {},
   "source": [
    "<img src=\"imgs/autoencoder_visualization.svg\" style=\"height:600px\" class=\"center\" alt=\"ae\"/><br>\n",
    "\n",
    "A simple representation of an autoencoder. Think of it as a bottle kneck into a smaller latent space and an upsample out of this space back into the output space (whatever that may be). In this case, it's attempting to recreate the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f202e41-8649-431d-af20-354e8a5c5a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3d5e0f5-4576-4244-9cdc-03350275e8ec",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f0e8ada-8b2b-4f4b-8946-39e621286bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download fashion MNIST\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root='./data', train=True, download=True,)\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root='./data', train=False, download=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70fcd63-0333-4421-b09f-fda5c6b39c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78b8b348-05aa-45de-8930-53c12e1c9b37",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e060aa7-ba26-4ce2-bd88-03a66d1df4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_dataset.data.detach().numpy()\n",
    "X_test = test_dataset.data.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5564ed80-2554-4688-98a3-e5dcbf028dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "X_train = (X_train - np.min(X_train)) / np.max(X_train - np.min(X_train))\n",
    "X_test = (X_test - np.min(X_test)) / np.max(X_test - np.min(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54723644-f271-4abc-be3c-ff28a3edce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PyTorch uses Dataset objects to load the data during training and testing\n",
    "class MNISTDataset(Dataset):\n",
    "\n",
    "    \"\"\"Data set\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        noisy_X: np.ndarray,\n",
    "        accelerator_name: str = \"mps\",\n",
    "        \n",
    "    ):\n",
    "        '''Assign data'''\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.noisy_X = noisy_X.astype(np.float32)\n",
    "\n",
    "        if accelerator_name == \"mps\":\n",
    "            self.device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        elif accelerator_name == \"cuda:0\":\n",
    "            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        '''function to get the length of the dataset'''\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        '''return an x, y pair'''\n",
    "        x_, noisy_x_ = self.X[idx].astype(np.float32), self.noisy_X[idx].astype(np.float32)\n",
    "\n",
    "        return torch.from_numpy(x_).float().to(self.device), torch.from_numpy(noisy_x_).float().to(self.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30beec86-be98-40cb-8135-050e48d5ddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into validation and training data\n",
    "val_split = 0.2\n",
    "X_train, X_val = train_test_split(X_train,\n",
    "                                  test_size=val_split,\n",
    "                                  random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dba08c7-4955-42bf-9f15-5399c90d5c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add channel axes (N, H, W) -> (N, C, H, W) because C = 1 in this case\n",
    "X_train = X_train[:, np.newaxis, :, :].astype(np.float32)\n",
    "X_test = X_test[:, np.newaxis, :, :].astype(np.float32)\n",
    "X_val = X_val[:, np.newaxis, :, :].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f0d61d9-d254-4ddc-9e9d-c56de802e47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_X_train = X_train.copy().astype(np.float32)\n",
    "noisy_X_test = X_test.copy().astype(np.float32)\n",
    "noisy_X_val = X_val.copy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e48aca1b-9a7c-40cc-adad-1e134bdc8fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_level = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fd1d857-2178-40fe-9686-30685fdd2ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_X_train += noise_level * np.random.standard_normal(X_train.shape)\n",
    "noisy_X_train -= np.min(noisy_X_train)\n",
    "noisy_X_train /= np.max(noisy_X_train)\n",
    "\n",
    "noisy_X_val += noise_level * np.random.standard_normal(X_val.shape)\n",
    "noisy_X_val -= np.min(noisy_X_val)\n",
    "noisy_X_val /= np.max(noisy_X_val)\n",
    "\n",
    "noisy_X_test += noise_level * np.random.standard_normal(X_test.shape)\n",
    "noisy_X_test -= np.min(noisy_X_test)\n",
    "noisy_X_test /= np.max(noisy_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "519d273b-4cfe-4d4f-81a1-6e8354de48c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get H = W\n",
    "input_xy = X_train.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6437c506-d28f-4989-bfcd-b46405c4b37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7c5a18c-2f62-4f81-91db-31d315b653aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Now we actually make the dataset and dataloader in PyTorch fashion\n",
    "train_data = MNISTDataset(X_train, noisy_X_train)\n",
    "val_data = MNISTDataset(X_val, noisy_X_val)\n",
    "test_data = MNISTDataset(X_test, noisy_X_test)\n",
    "\n",
    "# make the loader\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd4cfee-7105-413b-998c-7cad00fac219",
   "metadata": {},
   "source": [
    "# Define Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9df722-b663-4969-8131-6e17bddce346",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b570c9fd-003c-47b4-844a-1535586077ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, \n",
    "                 kernel_size: int = 3, \n",
    "                 dropout: float = 0.25,\n",
    "                 cnn_layer_dims: list = [126, 64, 32],\n",
    "                 padding: int = 1, \n",
    "                 stride: int = 2,\n",
    "                 n_channels: int = 1,\n",
    "                 input_xy: int = 28, \n",
    "                 lr: float = 1e-4, \n",
    "                 weight_decay: float = 0., \n",
    "                 eps: float = 5e-7, \n",
    "                 activation: torch.nn.modules.activation = nn.GELU(),\n",
    "                 use_wandb: bool=False,\n",
    "                 scheduler_name: str = \"none\",\n",
    "                 step_size: int = 5,\n",
    "                 gamma: float = 0.5,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        ### Always need to call above function first in order\n",
    "        ### to properly initialize a model\n",
    "        '''Basic CNN to classify fashion MNIST\n",
    "        We aren't going to both with some of the fancier stuff from the MLP, but it's easy enough to apply here too\n",
    "        '''\n",
    "        \n",
    "        # model parameters\n",
    "        self.cnn_dims = cnn_layer_dims\n",
    "        self.activation = activation\n",
    "        self.lr = lr\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.dropout = dropout\n",
    "        self.n_channels = len(cnn_layer_dims)\n",
    "        self.scheduler_name = scheduler_name\n",
    "        # if using a scheduler\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # log using WandB or TensorBoard\n",
    "        self.use_wandb = use_wandb\n",
    "        \n",
    "        # what the input data looks like (allows construction of graph for logging)\n",
    "        # (batch_size, channels, height, width)\n",
    "        self.example_input_array = torch.zeros(\n",
    "            (1, n_channels, input_xy, input_xy,),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        #### Construct the layers #####\n",
    "        ## get input layer\n",
    "        ## shape = (C, H, W) -> (cnn_layer_dim, H, W)\n",
    "        self.input_cnn = nn.Conv2d(n_channels, cnn_layer_dims[0], \n",
    "                                   kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        \n",
    "        ## make CNN hidden layers\n",
    "        self.encoding_layers = []\n",
    "        for i in range(1, n_cnn_layers):\n",
    "            self.encoding_layers.append(nn.Conv2d(cnn_layer_dims[i-1], cnn_layer_dims[i], \n",
    "                                       kernel_size=kernel_size, stride=stride, padding=padding))\n",
    "\n",
    "        self.encoding_layers = nn.ModuleList(self.encoding_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''Determines how data is passed through the network, \n",
    "           i.e creates the connectivity of the network'''\n",
    "        \n",
    "        ## send through input layer and activate\n",
    "        x = self.activation(self.input_cnn(x))\n",
    "        ## do max pooling (2x2)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "            \n",
    "        # pass through CNN\n",
    "        for layer in self.encoding_layers:\n",
    "            # pass through layer and activate\n",
    "            x = layer(x)\n",
    "            x = self.activation(x)\n",
    "            # pool\n",
    "            x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self) -> (list, list):\n",
    "        \"\"\"Set up the optimizer and potential learning rate scheduler\"\"\"\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            eps=self.eps,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "\n",
    "        if self.scheduler_name == \"none\":\n",
    "            return self.optimizer\n",
    "\n",
    "        ### this decreases the learning rate by a factor of gamma every step_size\n",
    "        self.scheduler = MultiStepLR(\n",
    "            self.optimizer,\n",
    "            list(range(0, self.trainer.max_epochs, self.step_size)),\n",
    "            gamma=self.gamma,\n",
    "        )\n",
    "\n",
    "        return [self.optimizer], [{\"scheduler\": self.scheduler, \"interval\": \"epoch\"}]\n",
    "        \n",
    "    #### need to add these two things in case the scheduler is used ####\n",
    "    def lr_scheduler_step(self, scheduler, metric) -> None:\n",
    "        if self.scheduler_name != \"none\":\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def on_epoch_end(self) -> None:\n",
    "        if self.scheduler_name != \"none\":\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def process_batch(self, batch, step: str = \"train\"):\n",
    "        \"\"\"Passes and logs a batch for a given type of step (test, train, validation)\"\"\"\n",
    "        \n",
    "        # get data (batch includes labels, which we don't need)\n",
    "        _, noisy_x = batch\n",
    "\n",
    "        # pass through network\n",
    "        # logits have no activation applied\n",
    "        return self(noisy_x)\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"What do do with a training batch\"\"\"\n",
    "        \n",
    "        return self.process_batch(batch, step=\"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        '''Validation step (at the end of each epoch)'''\n",
    "        \n",
    "        return self.process_batch(batch, step=\"val\")\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \n",
    "        '''Test step is essentially the same as a validation step in this instance'''\n",
    "        return self.process_batch(batch, step=\"test\")\n",
    "\n",
    "    def activation_maps(self, x, depth: int=0) -> np.ndarray:\n",
    "        \n",
    "        '''Gets output activation of an arbitary CNN layer'''\n",
    "        \n",
    "        i = 0\n",
    "        # input layer\n",
    "        x = self.activation(self.input_cnn(x))\n",
    "        \n",
    "        if depth == 0:\n",
    "            return x.detach().numpy()\n",
    "        \n",
    "        i += 1\n",
    "        x = F.max_pool2d(x, 2)\n",
    "            \n",
    "        # pass through CNN and return when you reach the appropriate depth\n",
    "        for layer in self.encoding_layers:\n",
    "            x = self.activation(layer(x))\n",
    "            if i == depth:\n",
    "                return x.detach().numpy()\n",
    "            i += 1\n",
    "            x = F.max_pool2d(x, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7caf409-ab0a-4df8-b54b-0ed5a4efeae5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c46f1e2-36bf-4c4c-8071-61079deddf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, \n",
    "                 kernel_size: int = 3, \n",
    "                 dropout: float = 0.25,\n",
    "                 cnn_layer_dims: list = [32, 64, 128],\n",
    "                 padding: int = 1, \n",
    "                 stride: int = 2,\n",
    "                 image_input_channels: int = 1,\n",
    "                 n_input_channels: int = 128,\n",
    "                 input_xy: int = 4, \n",
    "                 lr: float = 1e-4, \n",
    "                 weight_decay: float = 0., \n",
    "                 eps: float = 5e-7, \n",
    "                 activation: torch.nn.modules.activation = F.gelu,\n",
    "                 use_wandb: bool=False,\n",
    "                 scheduler_name: str = \"none\",\n",
    "                 step_size: int = 5,\n",
    "                 gamma: float = 0.5,\n",
    "                 output_padding: int = 1,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        ### Always need to call above function first in order\n",
    "        ### to properly initialize a model\n",
    "        '''Basic CNN to classify fashion MNIST\n",
    "        We aren't going to both with some of the fancier stuff from the MLP, but it's easy enough to apply here too\n",
    "        '''\n",
    "        \n",
    "        # model parameters\n",
    "        self.cnn_dims = cnn_layer_dims\n",
    "        self.image_input_channels = image_input_channels\n",
    "        self.activation = activation\n",
    "        self.lr = lr\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.dropout = dropout\n",
    "        self.n_channels = len(cnn_layer_dims)\n",
    "        self.scheduler_name = scheduler_name\n",
    "        # if using a scheduler\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # log using WandB or TensorBoard\n",
    "        self.use_wandb = use_wandb\n",
    "\n",
    "        # what the input data looks like (allows construction of graph for logging)\n",
    "        # (batch_size, channels, height, width)\n",
    "        self.example_input_array = torch.zeros(\n",
    "            (1, n_input_channels, input_xy, input_xy,),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        #### Construct the layers #####\n",
    "        ## get input layer\n",
    "        ## shape = (C, H, W) -> (cnn_layer_dim, H, W)\n",
    "        self.input_cnn = nn.ConvTranspose2d(n_input_channels,\n",
    "                                            cnn_layer_dims[0], \n",
    "                                            kernel_size=kernel_size, \n",
    "                                            # output_padding=output_padding, \n",
    "                                            padding=padding, stride=stride\n",
    "                                          )\n",
    "\n",
    "        ## make CNN hidden layers\n",
    "        self.decoding_layers = []\n",
    "        for i in range(1, n_cnn_layers):\n",
    "\n",
    "            ### ConvTranspose2d __upscales__ the data with output padding\n",
    "            self.decoding_layers.append(nn.ConvTranspose2d(cnn_layer_dims[i-1],\n",
    "                                                           cnn_layer_dims[i], \n",
    "                                                           kernel_size=kernel_size, \n",
    "                                                           output_padding=output_padding, \n",
    "                                                           padding=padding, stride=stride\n",
    "                                                          ))\n",
    "\n",
    "            # self.decoding_layers.append(nn.Conv2d(cnn_layer_dims[i], cnn_layer_dims[i], \n",
    "                                       # kernel_size=kernel_size, stride=stride, padding=padding))\n",
    "\n",
    "        self.decoding_layers.append(nn.ConvTranspose2d(cnn_layer_dims[-1],\n",
    "                                                       self.image_input_channels, \n",
    "                                                       kernel_size=kernel_size, \n",
    "                                                       output_padding=output_padding, \n",
    "                                                       padding=padding, stride=stride,\n",
    "                                                    ))\n",
    "\n",
    "        # self.decoding_layers.append(nn.Tanh())\n",
    "\n",
    "        self.decoding_layers = nn.ModuleList(self.decoding_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''Determines how data is passed through the network, \n",
    "           i.e creates the connectivity of the network'''\n",
    "        ## send through input layer and activate\n",
    "        x = self.activation(self.input_cnn(x))\n",
    "        # pass through CNN\n",
    "        for layer in self.decoding_layers:\n",
    "            # pass through layer\n",
    "            x = layer(x)\n",
    "            x = self.activation(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self) -> (list, list):\n",
    "        \"\"\"Set up the optimizer and potential learning rate scheduler\"\"\"\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            eps=self.eps,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "\n",
    "        if self.scheduler_name == \"none\":\n",
    "            return self.optimizer\n",
    "\n",
    "        ### this decreases the learning rate by a factor of gamma every step_size\n",
    "        self.scheduler = MultiStepLR(\n",
    "            self.optimizer,\n",
    "            list(range(0, self.trainer.max_epochs, self.step_size)),\n",
    "            gamma=self.gamma,\n",
    "        )\n",
    "\n",
    "        return [self.optimizer], [{\"scheduler\": self.scheduler, \"interval\": \"epoch\"}]\n",
    "        \n",
    "    #### need to add these two things in case the scheduler is used ####\n",
    "    def lr_scheduler_step(self, scheduler, metric) -> None:\n",
    "        if self.scheduler_name != \"none\":\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def on_epoch_end(self) -> None:\n",
    "        if self.scheduler_name != \"none\":\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def process_batch(self, batch, step: str = \"train\"):\n",
    "        \"\"\"Passes and logs a batch for a given type of step (test, train, validation)\"\"\"\n",
    "        \n",
    "        # get data (batch includes labels, which we don't need)\n",
    "        _, noisy_x = batch\n",
    "\n",
    "        # pass through network\n",
    "        # logits have no activation applied\n",
    "        return self(noisy_x)\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"What do do with a training batch\"\"\"\n",
    "        \n",
    "        return self.process_batch(batch, step=\"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        '''Validation step (at the end of each epoch)'''\n",
    "        \n",
    "        return self.process_batch(batch, step=\"val\")\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \n",
    "        '''Test step is essentially the same as a validation step in this instance'''\n",
    "        return self.process_batch(batch, step=\"test\")\n",
    "\n",
    "    def activation_maps(self, x, depth: int=0) -> np.ndarray:\n",
    "        \n",
    "        '''Gets output activation of an arbitary CNN layer'''\n",
    "        \n",
    "        i = 0\n",
    "        # input layer\n",
    "        x = self.activation(self.input_cnn(x))\n",
    "        \n",
    "        if depth == 0:\n",
    "            return x.detach().numpy()\n",
    "        \n",
    "        i += 1\n",
    "        x = F.max_pool2d(x, 2)\n",
    "            \n",
    "        # pass through CNN and return when you reach the appropriate depth\n",
    "        for layer in self.decoding_layers:\n",
    "            x = self.activation(layer(x))\n",
    "            if i == depth:\n",
    "                return x.detach().numpy()\n",
    "            i += 1\n",
    "            x = F.max_pool2d(x, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9d8a77-00d6-4f76-abc8-df7905b3d743",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Entire network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f34768c2-8f0b-4623-8401-2b21c5b182e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, \n",
    "                 kernel_size: int = 3, \n",
    "                 dropout: float = 0.25,\n",
    "                 cnn_layer_dims: list = [128, 64, 32],\n",
    "                 padding: int = 1, \n",
    "                 stride: int = 2,\n",
    "                 image_input_channels: int = 1,\n",
    "                 latent_dim: int = 32,\n",
    "                 input_xy: int = 28, \n",
    "                 lr: float = 1e-4, \n",
    "                 weight_decay: float = 0., \n",
    "                 eps: float = 5e-7, \n",
    "                 activation: torch.nn.modules.activation = F.gelu,\n",
    "                 use_wandb: bool=False,\n",
    "                 scheduler_name: str = \"none\",\n",
    "                 step_size: int = 5,\n",
    "                 gamma: float = 0.5,\n",
    "                 output_padding: int = 1,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        ### Always need to call above function first in order\n",
    "        ### to properly initialize a model\n",
    "        '''Basic CNN to classify fashion MNIST\n",
    "        We aren't going to both with some of the fancier stuff from the MLP, but it's easy enough to apply here too\n",
    "        '''\n",
    "        \n",
    "        # model parameters\n",
    "        self.cnn_dims = cnn_layer_dims\n",
    "        self.image_input_channels = image_input_channels\n",
    "        self.activation = activation\n",
    "        self.lr = lr\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.dropout = dropout\n",
    "        self.n_channels = len(cnn_layer_dims)\n",
    "        self.scheduler_name = scheduler_name\n",
    "        # if using a scheduler\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        \n",
    "        # log using WandB or TensorBoard\n",
    "        self.use_wandb = use_wandb\n",
    "\n",
    "        # what the input data looks like (allows construction of graph for logging)\n",
    "        # (batch_size, channels, height, width)\n",
    "        self.example_input_array = torch.zeros(\n",
    "            (1, image_input_channels, input_xy, input_xy,),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.encoder = Encoder(\n",
    "                  cnn_layer_dims=cnn_layer_dims, \n",
    "                  input_xy=input_xy, \n",
    "                  activation=activation, \n",
    "                  n_channels=n_channels,\n",
    "                  use_wandb=use_wandb,\n",
    "                  dropout=dropout, \n",
    "                  padding=padding,\n",
    "                  eps=eps, lr=lr, \n",
    "                  weight_decay=weight_decay,\n",
    "                  scheduler_name=scheduler_name,\n",
    "                  gamma=gamma,\n",
    "                  step_size=step_size,\n",
    "        )\n",
    "\n",
    "        # get output dimensions of encoder\n",
    "        self.encoded_cxy, self.flat_dim = self._flat_layer_size()\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "                  cnn_layer_dims=cnn_layer_dims[::-1], \n",
    "                  input_xy=self.encoded_cxy[-1], \n",
    "                  activation=activation, \n",
    "                  n_input_channels=cnn_layer_dims[-1],\n",
    "                  image_input_channels=image_input_channels,\n",
    "                  use_wandb=use_wandb,\n",
    "                  dropout=dropout, \n",
    "                  padding=padding,\n",
    "                  eps=eps, lr=lr, \n",
    "                  weight_decay=weight_decay,\n",
    "                  scheduler_name=scheduler_name,\n",
    "                  gamma=gamma,\n",
    "                  step_size=step_size,\n",
    "                  output_padding=output_padding,\n",
    "        )\n",
    "\n",
    "\n",
    "        self.latent_encoder = nn.Linear(self.flat_dim, self.latent_dim)\n",
    "        self.latent_decoder = nn.Linear(self.latent_dim, self.flat_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''Determines how data is passed through the network, \n",
    "           i.e creates the connectivity of the network'''\n",
    "        \n",
    "        # encode\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # put into latent space\n",
    "        x = nn.Flatten()(x)\n",
    "        x = self.activation(self.latent_encoder(x))\n",
    "\n",
    "        # bring out of latent space\n",
    "        x = self.activation(self.latent_decoder(x))\n",
    "\n",
    "        # import pdb; pdb.set_trace()\n",
    "\n",
    "        # decode\n",
    "        x = x.reshape(x.shape[0], \n",
    "                      -1, \n",
    "                      self.encoded_cxy[-1], \n",
    "                      self.encoded_cxy[-1])\n",
    "\n",
    "        # import pdb; pdb.set_trace()\n",
    "        \n",
    "        x = self.decoder(x)\n",
    "\n",
    "        # import pdb; pdb.set_trace()\n",
    "\n",
    "        return nn.ReLU()(x)\n",
    "\n",
    "    def configure_optimizers(self) -> (list, list):\n",
    "        \"\"\"Set up the optimizer and potential learning rate scheduler\"\"\"\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            eps=self.eps,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "\n",
    "        if self.scheduler_name == \"none\":\n",
    "            return self.optimizer\n",
    "\n",
    "        ### this decreases the learning rate by a factor of gamma every step_size\n",
    "        self.scheduler = MultiStepLR(\n",
    "            self.optimizer,\n",
    "            list(range(0, self.trainer.max_epochs, self.step_size)),\n",
    "            gamma=self.gamma,\n",
    "        )\n",
    "\n",
    "        return [self.optimizer], [{\"scheduler\": self.scheduler, \"interval\": \"epoch\"}]\n",
    "        \n",
    "    #### need to add these two things in case the scheduler is used ####\n",
    "    def lr_scheduler_step(self, scheduler, metric) -> None:\n",
    "        if self.scheduler_name != \"none\":\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def on_epoch_end(self) -> None:\n",
    "        if self.scheduler_name != \"none\":\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def process_batch(self, batch, step: str = \"train\"):\n",
    "        \"\"\"Passes and logs a batch for a given type of step (test, train, validation)\"\"\"\n",
    "        \n",
    "        # get data (batch includes labels, which we don't need)\n",
    "        x, noisy_x = batch\n",
    "\n",
    "        # pass through network\n",
    "        denoised_x = self(noisy_x)\n",
    "\n",
    "        # flatten to get easy MSE\n",
    "        denoised_x = nn.Flatten()(denoised_x)\n",
    "        x = nn.Flatten()(x)\n",
    "\n",
    "        # import pdb; pdb.set_trace()\n",
    "        \n",
    "        loss = self.loss_fn(x, denoised_x)\n",
    "\n",
    "        self.log(f\"{step}_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"What do do with a training batch\"\"\"\n",
    "        \n",
    "        return self.process_batch(batch, step=\"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        '''Validation step (at the end of each epoch)'''\n",
    "        \n",
    "        return self.process_batch(batch, step=\"val\")\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \n",
    "        '''Test step is essentially the same as a validation step in this instance'''\n",
    "        return self.process_batch(batch, step=\"test\")\n",
    "\n",
    "    def _flat_layer_size(self) -> int:\n",
    "        \n",
    "        '''Gets the dimension of the flattened CNN output layer'''\n",
    "        \n",
    "        x = self.example_input_array\n",
    "        \n",
    "        # Pass the input tensor through the CNN layers\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        array_dim = x.size()\n",
    "        # Calculate the flattened layer dimension\n",
    "        flattened_dim = x.view(1, -1).size(1)\n",
    "\n",
    "        return array_dim, flattened_dim\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef01ded2-09ea-44bb-bbff-ed413b806860",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "181cff3d-1554-474d-bb59-3c007845f0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyper parameters \n",
    "lr = 1e-3\n",
    "eps = 1e-8\n",
    "weight_decay = 1e-6\n",
    "dropout = 0.25\n",
    "cnn_layer_dims  = [128,]\n",
    "n_cnn_layers = len(cnn_layer_dims)\n",
    "latent_dim = 16\n",
    "activation = F.gelu\n",
    "n_channels = 1\n",
    "stride = 2\n",
    "kernel_size = 3\n",
    "padding = 0\n",
    "output_padding = 1\n",
    "scheduler_name = \"none\"\n",
    "gamma = 0.5\n",
    "step_size = 5\n",
    "\n",
    "## WandB stuff\n",
    "# log with WandB or TensorBoard\n",
    "use_wandb = False\n",
    "# do hyperparameter sweep with WandB\n",
    "use_sweep = False\n",
    "# WandB project name\n",
    "project_name = 'FashionMNIST_AE'\n",
    "# WandB lab name\n",
    "entity = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e3dcfd0-39cc-43c2-806f-be7df9ce685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder(latent_dim=latent_dim,\n",
    "                  cnn_layer_dims=cnn_layer_dims, \n",
    "                  input_xy=input_xy, \n",
    "                  activation=activation, \n",
    "                  image_input_channels=n_channels,\n",
    "                  use_wandb=use_wandb,\n",
    "                  dropout=dropout, \n",
    "                  eps=eps, \n",
    "                  lr=lr, \n",
    "                  weight_decay=weight_decay,\n",
    "                  scheduler_name=scheduler_name,\n",
    "                  gamma=gamma,\n",
    "                  step_size=step_size,\n",
    "                  stride=stride,\n",
    "                  padding=padding,\n",
    "                  output_padding=output_padding,\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d60d27-f63c-4a92-8561-4ec44df16922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bb6a899-eb09-407d-9af0-476bc6f42fbc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23090e19-681d-4286-867b-12598da0ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aab58c32-b400-40ab-8eaa-b5a64160bb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator_name = \"mps\"\n",
    "# accelerator_name = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f15ca5fe-8bd4-4e27-91e1-f35cb533df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boilerplate to get GPU if possible\n",
    "if accelerator_name == \"mps\":\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "elif accelerator_name == \"cuda\":\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2fa5dfc-45e3-4f06-9b69-1f3d261a65f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44fcca20-f958-4675-8659-e9d68ebaa791",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_wandb:\n",
    "    %load_ext tensorboard\n",
    "    cnn_logger = TensorBoardLogger(\"ae_logs\", name=\"simple_mnist_fashion_ae\")\n",
    "    run_name = \"ae_cnn\"\n",
    "else:\n",
    "    logger_kwargs = {\n",
    "        \"resume\": \"allow\",\n",
    "        \"config\": model_hparams,\n",
    "    }\n",
    "    cnn_logger = WandbLogger(project=project_name, entity=entity, **logger_kwargs)\n",
    "    cnn_run_name = cnn_logger.experiment.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1cf356a7-8b6f-4b19-8a77-4240a247322e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "#### necessary for newer PTL versions\n",
    "devices = 1\n",
    "accelerator = \"gpu\" if devices == 1 else \"cpu\"\n",
    "\n",
    "# make the trainer\n",
    "trainer = pl.Trainer(\n",
    "    devices=devices,\n",
    "    accelerator=accelerator,\n",
    "    max_epochs=num_epochs,\n",
    "    log_every_n_steps=1,\n",
    "    logger=cnn_logger,\n",
    "    # reload_dataloaders_every_epoch=True,\n",
    "    callbacks=[\n",
    "        # ModelCheckpoint(\n",
    "        #     save_weights_only=False,\n",
    "        #     mode=\"min\",\n",
    "        #     monitor=\"val_acc\",\n",
    "        #     save_top_k=1,\n",
    "        #     every_n_epochs=1,\n",
    "        #     save_on_train_epoch_end=False,\n",
    "        #     dirpath=f\"/AE_Checkpoints/{run_name}/\",\n",
    "        #     filename=f\"ae_checkpoint_{run_name}\",\n",
    "        # ),\n",
    "        LearningRateMonitor(\"epoch\"),\n",
    "        progress.TQDMProgressBar(refresh_rate=1),\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            min_delta=0,\n",
    "            patience=10,\n",
    "            verbose=False,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "trainer.logger._log_graph = True\n",
    "trainer.logger._default_hp_metric = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c39e4c9c-946c-4042-8a55-0292cfa27dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonterry/miniforge3/envs/pytorch_python10/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory ae_logs/simple_mnist_fashion_ae/version_2/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name           | Type    | Params | In sizes       | Out sizes     \n",
      "-----------------------------------------------------------------------------\n",
      "0 | loss_fn        | MSELoss | 0      | ?              | ?             \n",
      "1 | encoder        | Encoder | 1.3 K  | [1, 1, 28, 28] | [1, 128, 6, 6]\n",
      "2 | decoder        | Decoder | 148 K  | [1, 128, 6, 6] | [1, 1, 28, 28]\n",
      "3 | latent_encoder | Linear  | 73.7 K | [1, 4608]      | [1, 16]       \n",
      "4 | latent_decoder | Linear  | 78.3 K | [1, 16]        | [1, 4608]     \n",
      "-----------------------------------------------------------------------------\n",
      "302 K     Trainable params\n",
      "0         Non-trainable params\n",
      "302 K     Total params\n",
      "1.208     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonterry/miniforge3/envs/pytorch_python10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/jasonterry/miniforge3/envs/pytorch_python10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c09f4e36574cd09a4179e0a8f9cfd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c2cb8d-227c-45a2-b100-23800bf3bc42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94471401-4adb-4fba-a579-2a453569cb87",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07e8c71c-c1d1-4ab3-992f-48412ebbcbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonterry/miniforge3/envs/pytorch_python10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d5ee27f4f8b4b65a98911f59b0ff4a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       Test metric             DataLoader 0\n",
      "\n",
      "        test_loss           0.01606205478310585\n",
      "\n",
      "[{'test_loss': 0.01606205478310585}]\n"
     ]
    }
   ],
   "source": [
    "## Get test metrics\n",
    "test_results = trainer.test(model, test_loader)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d08360f9-1b26-41d2-a981-78f861982ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 30478), started 0:26:49 ago. (Use '!kill 30478' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c4da537c1651ddae\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c4da537c1651ddae\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## open up TensorBoard\n",
    "if not use_wandb:\n",
    "    %tensorboard --logdir ae_logs --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb10142f-3634-492b-a770-7c9d15d80735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee4841c2-4f9f-481d-abb3-34b34fc11281",
   "metadata": {},
   "source": [
    "## Look at images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "735c0a72-6b46-4a43-ad72-3e7d3cabe4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting a prameters\n",
    "scale_factor = 1.5\n",
    "\n",
    "labels = 16 * scale_factor\n",
    "ticks = 10 * scale_factor\n",
    "# ticks = 10 * scale_factor\n",
    "legends = 12 * scale_factor\n",
    "text = 14 * scale_factor\n",
    "titles = 22 * scale_factor\n",
    "lw = 3 * scale_factor\n",
    "ps = 200 * scale_factor\n",
    "cmap = 'magma'\n",
    "\n",
    "colors = ['firebrick', 'steelblue', 'darkorange', 'darkviolet', 'cyan', 'magenta', 'darkgreen', 'deeppink']\n",
    "markers = ['x', 'o', '+', '>', '*', 'D', '4']\n",
    "linestyles = ['-', '--', ':', '-.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "980120f4-4106-419a-992a-232126aab4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_panel(images: list, labels: list=[\"Noisy\", \"Denoised\", \"Clean\"], cmap: str='viridis', show_ticks: bool=True):\n",
    "    \n",
    "    '''Plots a 3x1 panel of images'''\n",
    "    \n",
    "    mplrc('xtick', labelsize=ticks) \n",
    "    mplrc('ytick', labelsize=ticks)\n",
    "    mplrc('axes', titlesize=titles)\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(16., 12.))\n",
    "    \n",
    "    for (i, image) in enumerate(images):\n",
    "        ax = axs[i]\n",
    "        \n",
    "        ax.imshow(image, cmap=cmap)\n",
    "        if len(labels) > i and labels[i] != '':\n",
    "            ax.set_title(labels[i])\n",
    "            \n",
    "        if not show_ticks:\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        elif i > 0:\n",
    "            ax.set_yticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9d76588-ba7a-4f1f-81b1-7c64eeae70dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do inference on test set\n",
    "## need to turn into torch tensor first\n",
    "# X_test_infer = torch.from_numpy(X_test).float()\n",
    "X_noisy_test_infer = torch.from_numpy(noisy_X_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4e8e9f50-ea06-47cb-b584-6f57ed83cf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_noisy_pred_tensor = model(X_noisy_test_infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f42d9796-af35-453b-b112-82d2b852c3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_noisy_pred = X_noisy_pred_tensor.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e781f682-2d8a-4f51-9a43-94b26529f3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1200 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 0\n",
    "plot_image_panel([noisy_X_test[i].squeeze(), X_noisy_pred[i].squeeze(), X_test[i].squeeze()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c7a95554-feb3-4762-a06e-9262e8f21fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## broke kernel for some reason ##\n",
    "# rand_latent = torch.randn((1, model.latent_dim)).to(device)\n",
    "# decoded_latent = model.latent_encoder(rand_latent)\n",
    "# decoded_latent = nodel.decoder(decoded_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af97cea2-db26-421e-ba04-d992afc484a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "f8b13074-933b-4878-9865-6724d18e3862",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef601107-b660-4726-b727-77d0781457b5",
   "metadata": {},
   "source": [
    "# Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79af75e9-d60e-4d6a-805d-a9f612e9715f",
   "metadata": {},
   "source": [
    "Autoencoders learn deterministic mapping into and out of the latent space. VAEs learn *probabilistic* mapping. That is, they learn a latent space distribution (often a normal distribution). The encoder maps into this (learned) distribution, and the decoder samples out of this distribution to give a final result.\n",
    "\n",
    "The extra power here comes from the probabilistic latent space. The network is (hopefully) trained such that any sample from the latent space can be decoded properly. If I put in Gaussian noise into the decoder, I should get something coherent out (like an approximate MNIST image). This allows the generation of an arbitrary number of outputs.\n",
    "\n",
    "<img src=\"imgs/vae.png\" style=\"height:300px\" class=\"center\" alt=\"vae\"/><br>\n",
    "\n",
    "Schematic of VAE. Everything is done probabilistically. The encoder outputs a mean ($\\mu$) and standard deviations ($\\sigma$) that describe a normal distrubtion, sampling outputs from this distrubtion. This is equivalent to sampling latent variables that are passed through the decoder. The decoder takes multiple samples from this latent space and decodes them into the output space. This defines an output *distrubtion*. The training objective for a VAE is to maximize the evidence lower bound (ELBO), which is a lower-bound approximation to the log-likelihood of the data (we use log-likelihood because it turns multiplication into addition). \n",
    "\n",
    "$$\\mathrm{ELBO} = KL[q(z|x) || p(z)] - \\mathbb{E}[log(p(x | z))$$ where\n",
    "\n",
    "$$KL[q(z|x) || p(z)] = \\frac{1}{2} \\left[\\sum_{i = 1}^{D}(\\sigma_{i}^{2} + \\mu_{i}^{2} - log(\\sigma_{i}^{2}) -1)\\right]$$ and $\\mathbb{E}[log(p(x | z))$ represents the reconstruction loss (not MSE!).\n",
    "\n",
    "where $\\sigma_{i}$ and $\\mu_{i}$ are the standard deviation and mean of the normal distrubtion describing the $i^{th}$ component of the latent space. The KL divergence means the divergence of $q(z|x)$ from $p(z)$, i.e., measures the distribution of the latent space given the input and the expected output (assumed normal distribution with $\\mu = 0$ and $\\sigma = 1$). This encourages the encoder to approximate a standard normal distrubtion while giving it the freedom to add some minor adjustments.\n",
    "\n",
    "After training, we can simple feed Gaussian noise into the decoder to generate novel samples. This is the first example of generative networks that we have seen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e52e014-0c89-43a0-b32e-e7ce32414b81",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a1e4f24-b267-409d-ae3c-397e0d2a89e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kl_divergence(z: torch.Tensor, mu: torch.Tensor, sigma: torch.Tensor) -> float:\n",
    "    \"\"\"This calculates the KL divergence\"\"\"\n",
    "\n",
    "    return 0.5 * (sigma ** 2 + mu ** 2 - torch.log(sigma) - 1.).sum()\n",
    "\n",
    "\n",
    "def get_reconstruction_loss(x: torch.Tensor, decoded_x: torch.Tensor, epsilon: nn.Parameter):\n",
    "    \"\"\"Gets reconstruction loss of latent space from a standard distrubtion\"\"\"\n",
    "\n",
    "    # get predicted distribtion\n",
    "    exp_scale = torch.exp(epsilon)\n",
    "    dist = torch.distributions.Normal(decoded_x, exp_scale)\n",
    "\n",
    "    # measure get p(x|z) using the predicted distribution\n",
    "    log_pxz = dist.log_prob(x)\n",
    "    \n",
    "    return log_pxz.sum(dim=(1, 2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c1ef42-278d-4944-bb9e-4e2b4b4272c1",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7903dc85-5adb-4440-be1a-fb6b6aa6bd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, \n",
    "                 kernel_size: int = 3, \n",
    "                 dropout: float = 0.25,\n",
    "                 cnn_layer_dims: list = [128, 64, 32],\n",
    "                 padding: int = 1, \n",
    "                 stride: int = 2,\n",
    "                 image_input_channels: int = 1,\n",
    "                 latent_dim: int = 32,\n",
    "                 input_xy: int = 28, \n",
    "                 lr: float = 1e-4, \n",
    "                 weight_decay: float = 0., \n",
    "                 eps: float = 5e-7, \n",
    "                 activation: torch.nn.modules.activation = F.gelu,\n",
    "                 use_wandb: bool=False,\n",
    "                 scheduler_name: str = \"none\",\n",
    "                 step_size: int = 5,\n",
    "                 gamma: float = 0.5,\n",
    "                 output_padding: int = 1,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        ### Always need to call above function first in order\n",
    "        ### to properly initialize a model\n",
    "        '''Basic CNN to classify fashion MNIST\n",
    "        We aren't going to both with some of the fancier stuff from the MLP, but it's easy enough to apply here too\n",
    "        '''\n",
    "        \n",
    "        # model parameters\n",
    "        self.cnn_dims = cnn_layer_dims\n",
    "        self.image_input_channels = image_input_channels\n",
    "        self.activation = activation\n",
    "        self.lr = lr\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.dropout = dropout\n",
    "        self.n_channels = len(cnn_layer_dims)\n",
    "        self.scheduler_name = scheduler_name\n",
    "        # if using a scheduler\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # standard normal (attempted p(z))\n",
    "        self.normal_distribtion = torch.distributions.Normal(0., 1.)\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # log using WandB or TensorBoard\n",
    "        self.use_wandb = use_wandb\n",
    "\n",
    "        # what the input data looks like (allows construction of graph for logging)\n",
    "        # (batch_size, channels, height, width)\n",
    "        self.example_input_array = torch.zeros(\n",
    "            (1, image_input_channels, input_xy, input_xy,),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "\n",
    "        # we still use our old encoder/decoder\n",
    "        self.encoder = Encoder(\n",
    "                  cnn_layer_dims=cnn_layer_dims, \n",
    "                  input_xy=input_xy, \n",
    "                  activation=activation, \n",
    "                  n_channels=n_channels,\n",
    "                  use_wandb=use_wandb,\n",
    "                  dropout=dropout, \n",
    "                  padding=padding,\n",
    "                  eps=eps, lr=lr, \n",
    "                  weight_decay=weight_decay,\n",
    "                  scheduler_name=scheduler_name,\n",
    "                  gamma=gamma,\n",
    "                  step_size=step_size,\n",
    "        )\n",
    "\n",
    "        # get output dimensions of encoder\n",
    "        self.encoded_cxy, self.flat_dim = self._flat_layer_size()\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "                  cnn_layer_dims=cnn_layer_dims[::-1], \n",
    "                  input_xy=self.encoded_cxy[-1], \n",
    "                  activation=activation, \n",
    "                  n_input_channels=cnn_layer_dims[-1],\n",
    "                  image_input_channels=image_input_channels,\n",
    "                  use_wandb=use_wandb,\n",
    "                  dropout=dropout, \n",
    "                  padding=padding,\n",
    "                  eps=eps, lr=lr, \n",
    "                  weight_decay=weight_decay,\n",
    "                  scheduler_name=scheduler_name,\n",
    "                  gamma=gamma,\n",
    "                  step_size=step_size,\n",
    "                  output_padding=output_padding,\n",
    "        )\n",
    "\n",
    "\n",
    "        self.mu = nn.Linear(self.flat_dim, self.latent_dim)\n",
    "        self.sigma = nn.Linear(self.flat_dim, self.latent_dim)\n",
    "        self.epsilon = nn.Parameter(torch.Tensor([0.]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''Determines how data is passed through the network, \n",
    "           i.e creates the connectivity of the network'''\n",
    "        \n",
    "        # encode\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # put into latent space\n",
    "        x = nn.Flatten()(x)\n",
    "\n",
    "        # get distribution of latent space\n",
    "        mu_hat = self.mu(x)\n",
    "        sigma_hat = torch.exp(self.sigma(x))\n",
    "\n",
    "        # decode\n",
    "        x = x.reshape(x.shape[0], \n",
    "                      -1, \n",
    "                      self.encoded_cxy[-1], \n",
    "                      self.encoded_cxy[-1])\n",
    "\n",
    "        # import pdb; pdb.set_trace()\n",
    "        \n",
    "        x = self.decoder(x)\n",
    "        return nn.ReLU()(x), mu_hat, sigma_hat\n",
    "\n",
    "    def configure_optimizers(self) -> (list, list):\n",
    "        \"\"\"Set up the optimizer and potential learning rate scheduler\"\"\"\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            eps=self.eps,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "\n",
    "        if self.scheduler_name == \"none\":\n",
    "            return self.optimizer\n",
    "\n",
    "        ### this decreases the learning rate by a factor of gamma every step_size\n",
    "        self.scheduler = MultiStepLR(\n",
    "            self.optimizer,\n",
    "            list(range(0, self.trainer.max_epochs, self.step_size)),\n",
    "            gamma=self.gamma,\n",
    "        )\n",
    "\n",
    "        return [self.optimizer], [{\"scheduler\": self.scheduler, \"interval\": \"epoch\"}]\n",
    "        \n",
    "    #### need to add these two things in case the scheduler is used ####\n",
    "    def lr_scheduler_step(self, scheduler, metric) -> None:\n",
    "        if self.scheduler_name != \"none\":\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def on_epoch_end(self) -> None:\n",
    "        if self.scheduler_name != \"none\":\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def process_batch(self, batch, step: str = \"train\"):\n",
    "        \"\"\"Passes and logs a batch for a given type of step (test, train, validation)\"\"\"\n",
    "        \n",
    "        # get data (batch includes labels, which we don't need)\n",
    "        x, _ = batch\n",
    "\n",
    "        # pass through network\n",
    "        x_pred, mu_hat, sigma_hat = self(x)\n",
    "\n",
    "        # get latent variables\n",
    "        z = mu_hat + sigma_hat * self.normal_distribtion.sample(mu_hat.shape).to(self.device)\n",
    "\n",
    "        recon_loss = get_reconstruction_loss(x, x_pred, self.epsilon).mean()\n",
    "        kl_divergence = get_kl_divergence(z, mu_hat, sigma_hat)\n",
    "\n",
    "        loss = kl_divergence - recon_loss\n",
    "\n",
    "        self.log(f\"{step}_loss\", loss)\n",
    "        self.log(f\"{step}_avg_reconstruction_loss\", recon_loss)\n",
    "        self.log(f\"{step}_kl_divergence\", kl_divergence)\n",
    "        self.log(f\"{step}_avg_mu\", mu_hat.mean())\n",
    "        self.log(f\"{step}_avg_sigma\", sigma_hat.mean())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"What do do with a training batch\"\"\"\n",
    "        \n",
    "        return self.process_batch(batch, step=\"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        '''Validation step (at the end of each epoch)'''\n",
    "        \n",
    "        return self.process_batch(batch, step=\"val\")\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \n",
    "        '''Test step is essentially the same as a validation step in this instance'''\n",
    "        return self.process_batch(batch, step=\"test\")\n",
    "\n",
    "    def _flat_layer_size(self) -> int:\n",
    "        \n",
    "        '''Gets the dimension of the flattened CNN output layer'''\n",
    "        \n",
    "        x = self.example_input_array\n",
    "        \n",
    "        # Pass the input tensor through the CNN layers\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        array_dim = x.size()\n",
    "        # Calculate the flattened layer dimension\n",
    "        flattened_dim = x.view(1, -1).size(1)\n",
    "\n",
    "        return array_dim, flattened_dim\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265a3839-81db-4479-90c4-9d7e4ec4d55e",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "128731a0-fe89-4316-89ef-3fb81e44b999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyper parameters \n",
    "lr = 1e-3\n",
    "eps = 1e-8\n",
    "weight_decay = 1e-6\n",
    "dropout = 0.25\n",
    "cnn_layer_dims  = [128,]\n",
    "n_cnn_layers = len(cnn_layer_dims)\n",
    "latent_dim = 16\n",
    "activation = F.gelu\n",
    "n_channels = 1\n",
    "stride = 2\n",
    "kernel_size = 3\n",
    "padding = 0\n",
    "output_padding = 1\n",
    "scheduler_name = \"none\"\n",
    "gamma = 0.5\n",
    "step_size = 5\n",
    "\n",
    "## WandB stuff\n",
    "# log with WandB or TensorBoard\n",
    "use_wandb = False\n",
    "# do hyperparameter sweep with WandB\n",
    "use_sweep = False\n",
    "# WandB project name\n",
    "project_name = 'FashionMNIST_VAE'\n",
    "# WandB lab name\n",
    "entity = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a28b3149-62d9-4200-8ad7-e0a7ca0177e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model = VariationalAutoencoder(latent_dim=latent_dim,\n",
    "                  cnn_layer_dims=cnn_layer_dims, \n",
    "                  input_xy=input_xy, \n",
    "                  activation=activation, \n",
    "                  image_input_channels=n_channels,\n",
    "                  use_wandb=use_wandb,\n",
    "                  dropout=dropout, \n",
    "                  eps=eps, \n",
    "                  lr=lr, \n",
    "                  weight_decay=weight_decay,\n",
    "                  scheduler_name=scheduler_name,\n",
    "                  gamma=gamma,\n",
    "                  step_size=step_size,\n",
    "                  stride=stride,\n",
    "                  padding=padding,\n",
    "                  output_padding=output_padding,\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d1e623-010b-4bbc-8fe6-2e9c8cf0ea32",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "406fb799-ba31-49b6-be22-cdefad32463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8242d068-da74-49cf-a851-c5589af1cf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator_name = \"mps\"\n",
    "# accelerator_name = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a77a820a-7bad-4647-99dd-82fbabff95f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boilerplate to get GPU if possible\n",
    "if accelerator_name == \"mps\":\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "elif accelerator_name == \"cuda\":\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ab7b73b9-571f-4f01-a551-38f145142074",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model = vae_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6729aad4-5aa1-400b-928d-04a7750c0cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VariationalAutoencoder(\n",
       "  (encoder): Encoder(\n",
       "    (input_cnn): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (encoding_layers): ModuleList()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (input_cnn): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (decoding_layers): ModuleList(\n",
       "      (0): ConvTranspose2d(128, 1, kernel_size=(3, 3), stride=(2, 2), output_padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (mu): Linear(in_features=4608, out_features=16, bias=True)\n",
       "  (sigma): Linear(in_features=4608, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11fab0e8-2ab0-4645-8b51-a65038bfe2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_wandb:\n",
    "    %load_ext tensorboard\n",
    "    vae_logger = TensorBoardLogger(\"vae_logs\", name=\"simple_mnist_fashion_vae\")\n",
    "    run_name = \"vae_cnn\"\n",
    "else:\n",
    "    logger_kwargs = {\n",
    "        \"resume\": \"allow\",\n",
    "        \"config\": model_hparams,\n",
    "    }\n",
    "    vae_logger = WandbLogger(project=project_name, entity=entity, **logger_kwargs)\n",
    "    vae_run_name = vae_logger.experiment.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "08acbadf-fd8e-47fb-b263-b7f0036604e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "#### necessary for newer PTL versions\n",
    "devices = 1\n",
    "accelerator = \"cpu\" if device.type == \"cpu\" else \"gpu\"\n",
    "\n",
    "# make the trainer\n",
    "vae_trainer = pl.Trainer(\n",
    "    devices=devices,\n",
    "    accelerator=accelerator,\n",
    "    max_epochs=num_epochs,\n",
    "    log_every_n_steps=1,\n",
    "    logger=vae_logger,\n",
    "    # reload_dataloaders_every_epoch=True,\n",
    "    callbacks=[\n",
    "        # ModelCheckpoint(\n",
    "        #     save_weights_only=False,\n",
    "        #     mode=\"min\",\n",
    "        #     monitor=\"val_acc\",\n",
    "        #     save_top_k=1,\n",
    "        #     every_n_epochs=1,\n",
    "        #     save_on_train_epoch_end=False,\n",
    "        #     dirpath=f\"/AE_Checkpoints/{run_name}/\",\n",
    "        #     filename=f\"ae_checkpoint_{run_name}\",\n",
    "        # ),\n",
    "        LearningRateMonitor(\"epoch\"),\n",
    "        progress.TQDMProgressBar(refresh_rate=1),\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            min_delta=0,\n",
    "            patience=10,\n",
    "            verbose=False,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "vae_trainer.logger._log_graph = True\n",
    "vae_trainer.logger._default_hp_metric = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e42c144-96a5-4551-9f00-2739cc1a594e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name    | Type    | Params | In sizes       | Out sizes     \n",
      "----------------------------------------------------------------------\n",
      "0 | encoder | Encoder | 1.3 K  | [1, 1, 28, 28] | [1, 128, 6, 6]\n",
      "1 | decoder | Decoder | 148 K  | [1, 128, 6, 6] | [1, 1, 28, 28]\n",
      "2 | mu      | Linear  | 73.7 K | [1, 4608]      | [1, 16]       \n",
      "3 | sigma   | Linear  | 73.7 K | [1, 4608]      | [1, 16]       \n",
      "----------------------------------------------------------------------\n",
      "297 K     Trainable params\n",
      "0         Non-trainable params\n",
      "297 K     Total params\n",
      "1.190     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef43da703c7407ea535a2377286c605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vae_trainer.fit(vae_model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456d2ab7-0352-406a-b8b6-b07f6449b458",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
