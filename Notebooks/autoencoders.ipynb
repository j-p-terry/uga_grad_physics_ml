{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9e94a75-f431-431e-bc1f-3ca8e7d9a382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm \n",
    "from matplotlib import rc as mplrc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import PIL\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint, progress\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger, TensorBoardLogger\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, random_split\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51feab6e-a744-475a-be91-b7442966093b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1641024b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Set seed for reproducibility\n",
    "np.random.seed(123)\n",
    "random.seed(123)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d476e7c3-373c-4ed4-8b57-a6324e1eccc1",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "### Latent Space\n",
    "A latent space is a mathematical representation of data that captures its important features or patterns. It's a way to simplify complex data and extract useful information. The information is, in a sense, compressed into a lower dimensional space that still contains enough information to perform whatever task you want. By encoding into a latent space, essential features from the data are extracted while superfluous data is ignored. Decoding from the latent space uses the essential features to reconstruct a higher dimensional output. This is the basis for many computer vision and natural language processing tasks.\n",
    "\n",
    "A common task is attempting to output the input. This can be useful for anomaly detection or denoising (with slight modifications to input data). I can, for example, take a 28x28 image, compress it through a series of convolutions into a 16 component latent space vector, then upsample it as I convolve to output a 28x28 image. In this way, I learn how to map input data into a useful representation (encoding) and map this representation into a desired ouput (decoding). This is also the basis for many language models, such as sequence-to-sequence models (translation - universal grammar?)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5abfca-c8b8-455b-989c-0ae9cb20c9cf",
   "metadata": {},
   "source": [
    "<img src=\"imgs/autoencoder_visualization.svg\" style=\"height:600px\" class=\"center\" alt=\"ae\"/><br>\n",
    "\n",
    "A simple representation of an autoencoder. Think of it as a bottle kneck into a smaller latent space and an upsample out of this space back into the output space (whatever that may be). In this case, it's attempting to recreate the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f202e41-8649-431d-af20-354e8a5c5a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3d5e0f5-4576-4244-9cdc-03350275e8ec",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f0e8ada-8b2b-4f4b-8946-39e621286bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download fashion MNIST\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root='./data', train=True, download=True,)\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root='./data', train=False, download=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70fcd63-0333-4421-b09f-fda5c6b39c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78b8b348-05aa-45de-8930-53c12e1c9b37",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e060aa7-ba26-4ce2-bd88-03a66d1df4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_dataset.data.detach().numpy()\n",
    "X_test = test_dataset.data.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5564ed80-2554-4688-98a3-e5dcbf028dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "X_train = (X_train - np.min(X_train)) / np.max(X_train - np.min(X_train))\n",
    "X_test = (X_test - np.min(X_test)) / np.max(X_test - np.min(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54723644-f271-4abc-be3c-ff28a3edce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PyTorch uses Dataset objects to load the data during training and testing\n",
    "class ImageDataset(Dataset):\n",
    "\n",
    "    \"\"\"Data set\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        noisy_X: np.ndarray,\n",
    "        accelerator_name: str = \"mps\",\n",
    "        \n",
    "    ):\n",
    "        '''Assign data'''\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.noisy_X = noisy_X.astype(np.float32)\n",
    "\n",
    "        if accelerator_name == \"mps\":\n",
    "            self.device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        elif accelerator_name == \"cuda:0\":\n",
    "            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        '''function to get the length of the dataset'''\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        '''return an x, y pair'''\n",
    "        x_, noisy_x_ = self.X[idx].astype(np.float32), self.noisy_X[idx].astype(np.float32)\n",
    "\n",
    "        return torch.from_numpy(x_).float().to(self.device), torch.from_numpy(noisy_x_).float().to(self.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30beec86-be98-40cb-8135-050e48d5ddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into validation and training data\n",
    "val_split = 0.2\n",
    "X_train, X_val = train_test_split(X_train,\n",
    "                                  test_size=val_split,\n",
    "                                  random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dba08c7-4955-42bf-9f15-5399c90d5c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add channel axes (N, H, W) -> (N, C, H, W) because C = 1 in this case\n",
    "X_train = X_train[:, np.newaxis, :, :].astype(np.float32)\n",
    "X_test = X_test[:, np.newaxis, :, :].astype(np.float32)\n",
    "X_val = X_val[:, np.newaxis, :, :].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f0d61d9-d254-4ddc-9e9d-c56de802e47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_X_train = X_train.copy().astype(np.float32)\n",
    "noisy_X_test = X_test.copy().astype(np.float32)\n",
    "noisy_X_val = X_val.copy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e48aca1b-9a7c-40cc-adad-1e134bdc8fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_level = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fd1d857-2178-40fe-9686-30685fdd2ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_X_train += noise_level * np.random.standard_normal(X_train.shape)\n",
    "noisy_X_train -= np.min(noisy_X_train)\n",
    "noisy_X_train /= np.max(noisy_X_train)\n",
    "\n",
    "noisy_X_val += noise_level * np.random.standard_normal(X_val.shape)\n",
    "noisy_X_val -= np.min(noisy_X_val)\n",
    "noisy_X_val /= np.max(noisy_X_val)\n",
    "\n",
    "noisy_X_test += noise_level * np.random.standard_normal(X_test.shape)\n",
    "noisy_X_test -= np.min(noisy_X_test)\n",
    "noisy_X_test /= np.max(noisy_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "519d273b-4cfe-4d4f-81a1-6e8354de48c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get H = W\n",
    "input_xy = X_train.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6437c506-d28f-4989-bfcd-b46405c4b37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7c5a18c-2f62-4f81-91db-31d315b653aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Now we actually make the dataset and dataloader in PyTorch fashion\n",
    "train_data = ImageDataset(X_train, noisy_X_train)\n",
    "val_data = ImageDataset(X_val, noisy_X_val)\n",
    "test_data = ImageDataset(X_test, noisy_X_test)\n",
    "\n",
    "# make the loader\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd4cfee-7105-413b-998c-7cad00fac219",
   "metadata": {},
   "source": [
    "# Define Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9df722-b663-4969-8131-6e17bddce346",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b570c9fd-003c-47b4-844a-1535586077ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, \n",
    "                 kernel_size: int = 3, \n",
    "                 dropout: float = 0.25,\n",
    "                 cnn_layer_dims: list = [126, 64, 32],\n",
    "                 padding: int = 1, \n",
    "                 stride: int = 2,\n",
    "                 n_channels: int = 1,\n",
    "                 input_xy: int = 28, \n",
    "                 lr: float = 1e-4, \n",
    "                 weight_decay: float = 0., \n",
    "                 eps: float = 5e-7, \n",
    "                 activation: torch.nn.modules.activation = nn.GELU(),\n",
    "                 use_wandb: bool=False,\n",
    "                 scheduler_name: str = \"none\",\n",
    "                 step_size: int = 5,\n",
    "                 gamma: float = 0.5,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        ### Always need to call above function first in order\n",
    "        ### to properly initialize a model\n",
    "        '''Basic CNN to classify fashion MNIST\n",
    "        We aren't going to both with some of the fancier stuff from the MLP, but it's easy enough to apply here too\n",
    "        '''\n",
    "        \n",
    "        # model parameters\n",
    "        self.cnn_dims = cnn_layer_dims\n",
    "        self.activation = activation\n",
    "        self.lr = lr\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.dropout = dropout\n",
    "        self.n_cnn_layers = len(cnn_layer_dims)\n",
    "        self.scheduler_name = scheduler_name\n",
    "        # if using a scheduler\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # log using WandB or TensorBoard\n",
    "        self.use_wandb = use_wandb\n",
    "        \n",
    "        # what the input data looks like (allows construction of graph for logging)\n",
    "        # (batch_size, channels, height, width)\n",
    "        self.example_input_array = torch.zeros(\n",
    "            (1, n_channels, input_xy, input_xy,),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        #### Construct the layers #####\n",
    "        ## get input layer\n",
    "        ## shape = (C, H, W) -> (cnn_layer_dim, H, W)\n",
    "        self.input_cnn = nn.Conv2d(n_channels, cnn_layer_dims[0], \n",
    "                                   kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        \n",
    "        ## make CNN hidden layers\n",
    "        self.encoding_layers = []\n",
    "        for i in range(1, self.n_cnn_layers):\n",
    "            self.encoding_layers.append(nn.Conv2d(cnn_layer_dims[i-1], cnn_layer_dims[i], \n",
    "                                       kernel_size=kernel_size, stride=stride, padding=padding))\n",
    "\n",
    "        self.encoding_layers = nn.ModuleList(self.encoding_layers)\n",
    "\n",
    "    #     self.init_weights()\n",
    "\n",
    "    # def init_weights(self) -> None:\n",
    "    #     ### does some fancy layer weight initialization\n",
    "    #     for layer in self.encoding_layers:\n",
    "    #         if isinstance(layer, nn.Linear):\n",
    "    #             nn.init.xavier_uniform_(layer.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''Determines how data is passed through the network, \n",
    "           i.e creates the connectivity of the network'''\n",
    "        \n",
    "        ## send through input layer and activate\n",
    "        x = self.activation(self.input_cnn(x))\n",
    "        ## do max pooling (2x2)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "            \n",
    "        # pass through CNN\n",
    "        for layer in self.encoding_layers:\n",
    "            # pass through layer and activate\n",
    "            x = layer(x)\n",
    "            x = self.activation(x)\n",
    "            # pool\n",
    "            x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self) -> (list, list):\n",
    "        \"\"\"Set up the optimizer and potential learning rate scheduler\"\"\"\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            eps=self.eps,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "\n",
    "        if self.scheduler_name == \"none\":\n",
    "            return self.optimizer\n",
    "\n",
    "        ### this decreases the learning rate by a factor of gamma every step_size\n",
    "        self.scheduler = MultiStepLR(\n",
    "            self.optimizer,\n",
    "            list(range(0, self.trainer.max_epochs, self.step_size)),\n",
    "            gamma=self.gamma,\n",
    "        )\n",
    "\n",
    "        return [self.optimizer], [{\"scheduler\": self.scheduler, \"interval\": \"epoch\"}]\n",
    "        \n",
    "    #### need to add these two things in case the scheduler is used ####\n",
    "    def lr_scheduler_step(self, scheduler, metric) -> None:\n",
    "        if self.scheduler_name != \"none\":\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def on_epoch_end(self) -> None:\n",
    "        if self.scheduler_name != \"none\":\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def process_batch(self, batch, step: str = \"train\"):\n",
    "        \"\"\"Passes and logs a batch for a given type of step (test, train, validation)\"\"\"\n",
    "        \n",
    "        # get data (batch includes labels, which we don't need)\n",
    "        _, noisy_x = batch\n",
    "\n",
    "        # pass through network\n",
    "        # logits have no activation applied\n",
    "        return self(noisy_x)\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"What do do with a training batch\"\"\"\n",
    "        \n",
    "        return self.process_batch(batch, step=\"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        '''Validation step (at the end of each epoch)'''\n",
    "        \n",
    "        return self.process_batch(batch, step=\"val\")\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \n",
    "        '''Test step is essentially the same as a validation step in this instance'''\n",
    "        return self.process_batch(batch, step=\"test\")\n",
    "\n",
    "    def activation_maps(self, x, depth: int=0) -> np.ndarray:\n",
    "        \n",
    "        '''Gets output activation of an arbitary CNN layer'''\n",
    "        \n",
    "        i = 0\n",
    "        # input layer\n",
    "        x = self.activation(self.input_cnn(x))\n",
    "        \n",
    "        if depth == 0:\n",
    "            return x.detach().numpy()\n",
    "        \n",
    "        i += 1\n",
    "        x = F.max_pool2d(x, 2)\n",
    "            \n",
    "        # pass through CNN and return when you reach the appropriate depth\n",
    "        for layer in self.encoding_layers:\n",
    "            x = self.activation(layer(x))\n",
    "            if i == depth:\n",
    "                return x.detach().numpy()\n",
    "            i += 1\n",
    "            x = F.max_pool2d(x, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7caf409-ab0a-4df8-b54b-0ed5a4efeae5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c46f1e2-36bf-4c4c-8071-61079deddf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, \n",
    "                 kernel_size: int = 3, \n",
    "                 dropout: float = 0.25,\n",
    "                 cnn_layer_dims: list = [32, 64, 128],\n",
    "                 padding: int = 1, \n",
    "                 stride: int = 2,\n",
    "                 image_input_channels: int = 1,\n",
    "                 n_input_channels: int = 128,\n",
    "                 input_xy: int = 4, \n",
    "                 lr: float = 1e-4, \n",
    "                 weight_decay: float = 0., \n",
    "                 eps: float = 5e-7, \n",
    "                 activation: torch.nn.modules.activation = F.gelu,\n",
    "                 use_wandb: bool=False,\n",
    "                 scheduler_name: str = \"none\",\n",
    "                 step_size: int = 5,\n",
    "                 gamma: float = 0.5,\n",
    "                 output_padding: int = 1,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        ### Always need to call above function first in order\n",
    "        ### to properly initialize a model\n",
    "        '''Basic CNN to classify fashion MNIST\n",
    "        We aren't going to both with some of the fancier stuff from the MLP, but it's easy enough to apply here too\n",
    "        '''\n",
    "        \n",
    "        # model parameters\n",
    "        self.cnn_dims = cnn_layer_dims\n",
    "        self.image_input_channels = image_input_channels\n",
    "        self.activation = activation\n",
    "        self.lr = lr\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.dropout = dropout\n",
    "        self.n_cnn_layers = len(cnn_layer_dims)\n",
    "        self.scheduler_name = scheduler_name\n",
    "        # if using a scheduler\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # log using WandB or TensorBoard\n",
    "        self.use_wandb = use_wandb\n",
    "\n",
    "        # what the input data looks like (allows construction of graph for logging)\n",
    "        # (batch_size, channels, height, width)\n",
    "        self.example_input_array = torch.zeros(\n",
    "            (1, n_input_channels, input_xy, input_xy,),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        #### Construct the layers #####\n",
    "        ## get input layer\n",
    "        ## shape = (C, H, W) -> (cnn_layer_dim, H, W)\n",
    "        self.input_cnn = nn.ConvTranspose2d(n_input_channels,\n",
    "                                            cnn_layer_dims[0], \n",
    "                                            kernel_size=kernel_size, \n",
    "                                            # output_padding=output_padding, \n",
    "                                            padding=padding, stride=stride\n",
    "                                          )\n",
    "\n",
    "        ## make CNN hidden layers\n",
    "        self.decoding_layers = []\n",
    "        for i in range(1, self.n_cnn_layers):\n",
    "\n",
    "            ### ConvTranspose2d __upscales__ the data with output padding\n",
    "            self.decoding_layers.append(nn.ConvTranspose2d(cnn_layer_dims[i-1],\n",
    "                                                           cnn_layer_dims[i], \n",
    "                                                           kernel_size=kernel_size, \n",
    "                                                           output_padding=output_padding, \n",
    "                                                           padding=padding, stride=stride\n",
    "                                                          ))\n",
    "\n",
    "            # self.decoding_layers.append(nn.Conv2d(cnn_layer_dims[i], cnn_layer_dims[i], \n",
    "                                       # kernel_size=kernel_size, stride=stride, padding=padding))\n",
    "\n",
    "        self.decoding_layers.append(nn.ConvTranspose2d(cnn_layer_dims[-1],\n",
    "                                                       self.image_input_channels, \n",
    "                                                       kernel_size=kernel_size, \n",
    "                                                       output_padding=output_padding, \n",
    "                                                       padding=padding, stride=stride,\n",
    "                                                    ))\n",
    "\n",
    "        # self.decoding_layers.append(nn.Tanh())\n",
    "\n",
    "        self.decoding_layers = nn.ModuleList(self.decoding_layers)\n",
    "\n",
    "        # self.init_weights()\n",
    "\n",
    "    # def init_weights(self) -> None:\n",
    "        # ### does some fancy layer weight initialization\n",
    "        # for layer in self.decoding_layers:\n",
    "        #     if isinstance(layer, nn.Linear):\n",
    "        #         nn.init.xavier_uniform_(layer.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''Determines how data is passed through the network, \n",
    "           i.e creates the connectivity of the network'''\n",
    "        ## send through input layer and activate\n",
    "        x = self.activation(self.input_cnn(x))\n",
    "        # pass through CNN\n",
    "        for layer in self.decoding_layers:\n",
    "            # pass through layer\n",
    "            x = layer(x)\n",
    "            x = self.activation(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self) -> (list, list):\n",
    "        \"\"\"Set up the optimizer and potential learning rate scheduler\"\"\"\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            eps=self.eps,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "\n",
    "        if self.scheduler_name == \"none\":\n",
    "            return self.optimizer\n",
    "\n",
    "        ### this decreases the learning rate by a factor of gamma every step_size\n",
    "        self.scheduler = MultiStepLR(\n",
    "            self.optimizer,\n",
    "            list(range(0, self.trainer.max_epochs, self.step_size)),\n",
    "            gamma=self.gamma,\n",
    "        )\n",
    "\n",
    "        return [self.optimizer], [{\"scheduler\": self.scheduler, \"interval\": \"epoch\"}]\n",
    "        \n",
    "    #### need to add these two things in case the scheduler is used ####\n",
    "    def lr_scheduler_step(self, scheduler, metric) -> None:\n",
    "        if self.scheduler_name != \"none\":\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def on_epoch_end(self) -> None:\n",
    "        if self.scheduler_name != \"none\":\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def process_batch(self, batch, step: str = \"train\"):\n",
    "        \"\"\"Passes and logs a batch for a given type of step (test, train, validation)\"\"\"\n",
    "        \n",
    "        # get data (batch includes labels, which we don't need)\n",
    "        _, noisy_x = batch\n",
    "\n",
    "        # pass through network\n",
    "        # logits have no activation applied\n",
    "        return self(noisy_x)\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"What do do with a training batch\"\"\"\n",
    "        \n",
    "        return self.process_batch(batch, step=\"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        '''Validation step (at the end of each epoch)'''\n",
    "        \n",
    "        return self.process_batch(batch, step=\"val\")\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \n",
    "        '''Test step is essentially the same as a validation step in this instance'''\n",
    "        return self.process_batch(batch, step=\"test\")\n",
    "\n",
    "    def activation_maps(self, x, depth: int=0) -> np.ndarray:\n",
    "        \n",
    "        '''Gets output activation of an arbitary CNN layer'''\n",
    "        \n",
    "        i = 0\n",
    "        # input layer\n",
    "        x = self.activation(self.input_cnn(x))\n",
    "        \n",
    "        if depth == 0:\n",
    "            return x.detach().numpy()\n",
    "        \n",
    "        i += 1\n",
    "        x = F.max_pool2d(x, 2)\n",
    "            \n",
    "        # pass through CNN and return when you reach the appropriate depth\n",
    "        for layer in self.decoding_layers:\n",
    "            x = self.activation(layer(x))\n",
    "            if i == depth:\n",
    "                return x.detach().numpy()\n",
    "            i += 1\n",
    "            x = F.max_pool2d(x, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9d8a77-00d6-4f76-abc8-df7905b3d743",
   "metadata": {},
   "source": [
    "## Entire network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f34768c2-8f0b-4623-8401-2b21c5b182e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, \n",
    "                 kernel_size: int = 3, \n",
    "                 dropout: float = 0.25,\n",
    "                 cnn_layer_dims: list = [128, 64, 32],\n",
    "                 padding: int = 1, \n",
    "                 stride: int = 2,\n",
    "                 image_input_channels: int = 1,\n",
    "                 latent_dim: int = 32,\n",
    "                 input_xy: int = 28, \n",
    "                 lr: float = 1e-4, \n",
    "                 weight_decay: float = 0., \n",
    "                 eps: float = 5e-7, \n",
    "                 activation: torch.nn.modules.activation = F.gelu,\n",
    "                 use_wandb: bool=False,\n",
    "                 scheduler_name: str = \"none\",\n",
    "                 step_size: int = 5,\n",
    "                 gamma: float = 0.5,\n",
    "                 output_padding: int = 1,\n",
    "                 use_l2: bool = False,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        ### Always need to call above function first in order\n",
    "        ### to properly initialize a model\n",
    "        '''Basic CNN to classify fashion MNIST\n",
    "        We aren't going to both with some of the fancier stuff from the MLP, but it's easy enough to apply here too\n",
    "        '''\n",
    "        \n",
    "        # model parameters\n",
    "        self.cnn_dims = cnn_layer_dims\n",
    "        self.image_input_channels = image_input_channels\n",
    "        self.activation = activation\n",
    "        self.lr = lr\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.dropout = dropout\n",
    "        self.n_channels = len(cnn_layer_dims)\n",
    "        self.scheduler_name = scheduler_name\n",
    "        # if using a scheduler\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        \n",
    "        # log using WandB or TensorBoard\n",
    "        self.use_wandb = use_wandb\n",
    "\n",
    "        # add L2 regularization\n",
    "        self.use_l2 = use_l2\n",
    "\n",
    "        # what the input data looks like (allows construction of graph for logging)\n",
    "        # (batch_size, channels, height, width)\n",
    "        self.example_input_array = torch.zeros(\n",
    "            (1, image_input_channels, input_xy, input_xy,),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.encoder = Encoder(\n",
    "                  cnn_layer_dims=cnn_layer_dims, \n",
    "                  input_xy=input_xy, \n",
    "                  activation=activation, \n",
    "                  n_channels=n_channels,\n",
    "                  use_wandb=use_wandb,\n",
    "                  dropout=dropout, \n",
    "                  padding=padding,\n",
    "                  eps=eps, lr=lr, \n",
    "                  weight_decay=weight_decay,\n",
    "                  scheduler_name=scheduler_name,\n",
    "                  gamma=gamma,\n",
    "                  step_size=step_size,\n",
    "        )\n",
    "\n",
    "        # get output dimensions of encoder\n",
    "        self.encoded_cxy, self.flat_dim = self._flat_layer_size()\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "                  cnn_layer_dims=cnn_layer_dims[::-1], \n",
    "                  input_xy=self.encoded_cxy[-1], \n",
    "                  activation=activation, \n",
    "                  n_input_channels=cnn_layer_dims[-1],\n",
    "                  image_input_channels=image_input_channels,\n",
    "                  use_wandb=use_wandb,\n",
    "                  dropout=dropout, \n",
    "                  padding=padding,\n",
    "                  eps=eps, lr=lr, \n",
    "                  weight_decay=weight_decay,\n",
    "                  scheduler_name=scheduler_name,\n",
    "                  gamma=gamma,\n",
    "                  step_size=step_size,\n",
    "                  output_padding=output_padding,\n",
    "        )\n",
    "\n",
    "\n",
    "        self.latent_encoder = nn.Linear(self.flat_dim, self.latent_dim)\n",
    "        self.latent_decoder = nn.Linear(self.latent_dim, self.flat_dim)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        ### does some fancy layer weight initialization\n",
    "        nn.init.xavier_uniform_(self.latent_decoder.weight)\n",
    "        nn.init.xavier_uniform_(self.latent_encoder.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''Determines how data is passed through the network, \n",
    "           i.e creates the connectivity of the network'''\n",
    "        \n",
    "        # encode\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # put into latent space\n",
    "        x = nn.Flatten()(x)\n",
    "        x = self.activation(self.latent_encoder(x))\n",
    "\n",
    "        # bring out of latent space\n",
    "        x = self.activation(self.latent_decoder(x))\n",
    "\n",
    "        # decode\n",
    "        x = x.reshape(x.shape[0], \n",
    "                      -1, \n",
    "                      self.encoded_cxy[-1], \n",
    "                      self.encoded_cxy[-1])\n",
    "        \n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return nn.ReLU()(x)\n",
    "\n",
    "    def configure_optimizers(self) -> (list, list):\n",
    "        \"\"\"Set up the optimizer and potential learning rate scheduler\"\"\"\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            eps=self.eps,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "\n",
    "        if self.scheduler_name == \"none\":\n",
    "            return self.optimizer\n",
    "\n",
    "        ### this decreases the learning rate by a factor of gamma every step_size\n",
    "        self.scheduler = MultiStepLR(\n",
    "            self.optimizer,\n",
    "            list(range(0, self.trainer.max_epochs, self.step_size)),\n",
    "            gamma=self.gamma,\n",
    "        )\n",
    "\n",
    "        return [self.optimizer], [{\"scheduler\": self.scheduler, \"interval\": \"epoch\"}]\n",
    "        \n",
    "    #### need to add these two things in case the scheduler is used ####\n",
    "    def lr_scheduler_step(self, scheduler, metric) -> None:\n",
    "        if self.scheduler_name != \"none\":\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def on_epoch_end(self) -> None:\n",
    "        if self.scheduler_name != \"none\":\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def process_batch(self, batch, step: str = \"train\"):\n",
    "        \"\"\"Passes and logs a batch for a given type of step (test, train, validation)\"\"\"\n",
    "        \n",
    "        # get data (batch includes labels, which we don't need)\n",
    "        x, noisy_x = batch\n",
    "\n",
    "        # pass through network\n",
    "        denoised_x = self(noisy_x)\n",
    "\n",
    "        # flatten to get easy MSE\n",
    "        denoised_x = nn.Flatten()(denoised_x)\n",
    "        x = nn.Flatten()(x)\n",
    "\n",
    "        loss = self.loss_fn(x, denoised_x)\n",
    "\n",
    "        if self.use_l2:\n",
    "            # Add L2 regularization to the loss\n",
    "            l2_regularization = 0.0  # You can adjust the regularization strength\n",
    "            for param in self.parameters():\n",
    "                l2_regularization += torch.norm(param, 2)  # L2 norm of each parameter\n",
    "            self.log(f\"{step}_L2\", l2_regularization)\n",
    "            loss += l2_regularization\n",
    "\n",
    "        self.log(f\"{step}_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"What do do with a training batch\"\"\"\n",
    "        \n",
    "        return self.process_batch(batch, step=\"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        '''Validation step (at the end of each epoch)'''\n",
    "        \n",
    "        return self.process_batch(batch, step=\"val\")\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \n",
    "        '''Test step is essentially the same as a validation step in this instance'''\n",
    "        return self.process_batch(batch, step=\"test\")\n",
    "\n",
    "    def _flat_layer_size(self) -> int:\n",
    "        \n",
    "        '''Gets the dimension of the flattened CNN output layer'''\n",
    "        \n",
    "        x = self.example_input_array\n",
    "        \n",
    "        # Pass the input tensor through the CNN layers\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        array_dim = x.size()\n",
    "        # Calculate the flattened layer dimension\n",
    "        flattened_dim = x.view(1, -1).size(1)\n",
    "\n",
    "        return array_dim, flattened_dim\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef01ded2-09ea-44bb-bbff-ed413b806860",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "181cff3d-1554-474d-bb59-3c007845f0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyper parameters \n",
    "lr = 1e-3\n",
    "eps = 1e-8\n",
    "weight_decay = 1e-6\n",
    "dropout = 0.25\n",
    "cnn_layer_dims  = [64,]\n",
    "n_cnn_layers = len(cnn_layer_dims)\n",
    "latent_dim = 16\n",
    "activation = F.gelu\n",
    "n_channels = 1\n",
    "stride = 2\n",
    "kernel_size = 3\n",
    "padding = 0\n",
    "output_padding = 1\n",
    "scheduler_name = \"step\"\n",
    "gamma = 0.5\n",
    "step_size = 5\n",
    "use_l2 = False # L2 regularization\n",
    "\n",
    "## WandB stuff\n",
    "# log with WandB or TensorBoard\n",
    "use_wandb = False\n",
    "# do hyperparameter sweep with WandB\n",
    "use_sweep = False\n",
    "# WandB project name\n",
    "project_name = 'FashionMNIST_AE'\n",
    "# WandB lab name\n",
    "entity = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e3dcfd0-39cc-43c2-806f-be7df9ce685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder(latent_dim=latent_dim,\n",
    "                  cnn_layer_dims=cnn_layer_dims, \n",
    "                  input_xy=input_xy, \n",
    "                  activation=activation, \n",
    "                  image_input_channels=n_channels,\n",
    "                  use_wandb=use_wandb,\n",
    "                  dropout=dropout, \n",
    "                  eps=eps, \n",
    "                  lr=lr, \n",
    "                  weight_decay=weight_decay,\n",
    "                  scheduler_name=scheduler_name,\n",
    "                  gamma=gamma,\n",
    "                  step_size=step_size,\n",
    "                  stride=stride,\n",
    "                  padding=padding,\n",
    "                  output_padding=output_padding,\n",
    "                  use_l2=use_l2,\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d60d27-f63c-4a92-8561-4ec44df16922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bb6a899-eb09-407d-9af0-476bc6f42fbc",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23090e19-681d-4286-867b-12598da0ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aab58c32-b400-40ab-8eaa-b5a64160bb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator_name = \"mps\"\n",
    "# accelerator_name = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f15ca5fe-8bd4-4e27-91e1-f35cb533df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boilerplate to get GPU if possible\n",
    "if accelerator_name == \"mps\":\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "elif accelerator_name == \"cuda\":\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2fa5dfc-45e3-4f06-9b69-1f3d261a65f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44fcca20-f958-4675-8659-e9d68ebaa791",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_wandb:\n",
    "    %load_ext tensorboard\n",
    "    cnn_logger = TensorBoardLogger(\"ae_logs\", name=\"simple_mnist_fashion_ae\")\n",
    "    run_name = \"ae_cnn\"\n",
    "else:\n",
    "    logger_kwargs = {\n",
    "        \"resume\": \"allow\",\n",
    "        \"config\": model_hparams,\n",
    "    }\n",
    "    cnn_logger = WandbLogger(project=project_name, entity=entity, **logger_kwargs)\n",
    "    cnn_run_name = cnn_logger.experiment.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1cf356a7-8b6f-4b19-8a77-4240a247322e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "#### necessary for newer PTL versions\n",
    "devices = 1\n",
    "accelerator = \"gpu\" if devices == 1 else \"cpu\"\n",
    "\n",
    "# make the trainer\n",
    "trainer = pl.Trainer(\n",
    "    devices=devices,\n",
    "    accelerator=accelerator,\n",
    "    max_epochs=num_epochs,\n",
    "    log_every_n_steps=1,\n",
    "    logger=cnn_logger,\n",
    "    # reload_dataloaders_every_epoch=True,\n",
    "    callbacks=[\n",
    "        # ModelCheckpoint(\n",
    "        #     save_weights_only=False,\n",
    "        #     mode=\"min\",\n",
    "        #     monitor=\"val_acc\",\n",
    "        #     save_top_k=1,\n",
    "        #     every_n_epochs=1,\n",
    "        #     save_on_train_epoch_end=False,\n",
    "        #     dirpath=f\"/AE_Checkpoints/{run_name}/\",\n",
    "        #     filename=f\"ae_checkpoint_{run_name}\",\n",
    "        # ),\n",
    "        LearningRateMonitor(\"epoch\"),\n",
    "        progress.TQDMProgressBar(refresh_rate=1),\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            min_delta=0,\n",
    "            patience=10,\n",
    "            verbose=False,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "trainer.logger._log_graph = True\n",
    "trainer.logger._default_hp_metric = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c39e4c9c-946c-4042-8a55-0292cfa27dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name           | Type    | Params | In sizes       | Out sizes     \n",
      "-----------------------------------------------------------------------------\n",
      "0 | loss_fn        | MSELoss | 0      | ?              | ?             \n",
      "1 | encoder        | Encoder | 640    | [1, 1, 28, 28] | [1, 64, 6, 6] \n",
      "2 | decoder        | Decoder | 37.5 K | [1, 64, 6, 6]  | [1, 1, 28, 28]\n",
      "3 | latent_encoder | Linear  | 36.9 K | [1, 2304]      | [1, 16]       \n",
      "4 | latent_decoder | Linear  | 39.2 K | [1, 16]        | [1, 2304]     \n",
      "-----------------------------------------------------------------------------\n",
      "114 K     Trainable params\n",
      "0         Non-trainable params\n",
      "114 K     Total params\n",
      "0.457     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonterry/miniforge3/envs/pytorch_python10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/jasonterry/miniforge3/envs/pytorch_python10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e08d2bb6c1142109faab577f2824310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c2cb8d-227c-45a2-b100-23800bf3bc42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94471401-4adb-4fba-a579-2a453569cb87",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07e8c71c-c1d1-4ab3-992f-48412ebbcbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonterry/miniforge3/envs/pytorch_python10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59dca91b85ec426180a3aaa3bee3f912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       Test metric             DataLoader 0\n",
      "\n",
      "        test_loss          0.018359003588557243\n",
      "\n",
      "[{'test_loss': 0.018359003588557243}]\n"
     ]
    }
   ],
   "source": [
    "## Get test metrics\n",
    "test_results = trainer.test(model, test_loader)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d08360f9-1b26-41d2-a981-78f861982ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-44867db30d67b366\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-44867db30d67b366\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6001;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## open up TensorBoard\n",
    "if not use_wandb:\n",
    "    %tensorboard --logdir ae_logs --port 6001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb10142f-3634-492b-a770-7c9d15d80735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee4841c2-4f9f-481d-abb3-34b34fc11281",
   "metadata": {},
   "source": [
    "## Look at images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "735c0a72-6b46-4a43-ad72-3e7d3cabe4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting a prameters\n",
    "scale_factor = 1.5\n",
    "\n",
    "labels = 16 * scale_factor\n",
    "ticks = 10 * scale_factor\n",
    "# ticks = 10 * scale_factor\n",
    "legends = 12 * scale_factor\n",
    "text = 14 * scale_factor\n",
    "titles = 22 * scale_factor\n",
    "lw = 3 * scale_factor\n",
    "ps = 200 * scale_factor\n",
    "cmap = 'magma'\n",
    "\n",
    "colors = ['firebrick', 'steelblue', 'darkorange', 'darkviolet', 'cyan', 'magenta', 'darkgreen', 'deeppink']\n",
    "markers = ['x', 'o', '+', '>', '*', 'D', '4']\n",
    "linestyles = ['-', '--', ':', '-.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "980120f4-4106-419a-992a-232126aab4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_panel(images: list, labels: list=[\"Noisy\", \"Denoised\", \"Clean\"], cmap: str='viridis', show_ticks: bool=True):\n",
    "    \n",
    "    '''Plots a 3x1 panel of images'''\n",
    "    \n",
    "    mplrc('xtick', labelsize=ticks) \n",
    "    mplrc('ytick', labelsize=ticks)\n",
    "    mplrc('axes', titlesize=titles)\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(16., 12.))\n",
    "    \n",
    "    for (i, image) in enumerate(images):\n",
    "        ax = axs[i]\n",
    "        \n",
    "        ax.imshow(image, cmap=cmap)\n",
    "        if len(labels) > i and labels[i] != '':\n",
    "            ax.set_title(labels[i])\n",
    "            \n",
    "        if not show_ticks:\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        elif i > 0:\n",
    "            ax.set_yticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9d76588-ba7a-4f1f-81b1-7c64eeae70dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do inference on test set\n",
    "## need to turn into torch tensor first\n",
    "# X_test_infer = torch.from_numpy(X_test).float()\n",
    "X_noisy_test_infer = torch.from_numpy(noisy_X_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e8e9f50-ea06-47cb-b584-6f57ed83cf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_noisy_pred_tensor = model(X_noisy_test_infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f42d9796-af35-453b-b112-82d2b852c3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_noisy_pred = X_noisy_pred_tensor.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e781f682-2d8a-4f51-9a43-94b26529f3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1200 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 0\n",
    "plot_image_panel([noisy_X_test[i].squeeze(), X_noisy_pred[i].squeeze(), X_test[i].squeeze()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a719a387-3e36-4b8c-ad26-2ec6ff7eef1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef601107-b660-4726-b727-77d0781457b5",
   "metadata": {},
   "source": [
    "# Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79af75e9-d60e-4d6a-805d-a9f612e9715f",
   "metadata": {},
   "source": [
    "Autoencoders learn deterministic mapping into and out of the latent space. VAEs learn *probabilistic* mapping. That is, they learn a latent space distribution (often a normal distribution). The encoder maps into this (learned) distribution, and the decoder samples out of this distribution to give a final result.\n",
    "\n",
    "The extra power here comes from the probabilistic latent space. The network is (hopefully) trained such that any sample from the latent space can be decoded properly. If I put in Gaussian noise into the decoder, I should get something coherent out (like an approximate MNIST image). This allows the generation of an arbitrary number of outputs.\n",
    "\n",
    "<img src=\"imgs/vae.png\" style=\"height:300px\" class=\"center\" alt=\"vae\"/><br>\n",
    "\n",
    "Schematic of VAE. Everything is done probabilistically. The encoder outputs a mean ($\\mu$) and standard deviations ($\\sigma$) that describe a normal distrubtion, sampling outputs from this distrubtion. This is equivalent to sampling latent variables that are passed through the decoder. The decoder takes multiple samples from this latent space and decodes them into the output space. This defines an output *distrubtion*. The training objective for a VAE is to maximize the evidence lower bound (ELBO), which is a lower-bound approximation to the log-likelihood of the data (we use log-likelihood because it turns multiplication into addition). \n",
    "\n",
    "$$\\mathrm{ELBO} = KL[q(z|x) || p(z)] - \\mathbb{E}[log(p(x | z))]$$ where\n",
    "\n",
    "$$KL[q(z|x) || p(z)] = \\frac{1}{2} \\left[\\sum_{i = 1}^{D}(\\sigma_{i}^{2} + \\mu_{i}^{2} - log(\\sigma_{i}^{2}) -1)\\right]$$ and $\\mathbb{E}[log(p(x | z))$ represents the reconstruction loss (not MSE!).\n",
    "\n",
    "where $\\sigma_{i}$ and $\\mu_{i}$ are the standard deviation and mean of the normal distrubtion describing the $i^{th}$ component of the latent space. The KL divergence means the divergence of $q(z|x)$ from $p(z)$, i.e., measures the distribution of the latent space given the input and the expected output (assumed normal distribution with $\\mu = 0$ and $\\sigma = 1$). This encourages the encoder to approximate a standard normal distrubtion while giving it the freedom to add some minor adjustments. The intuition here is that reconstruction loss attempts to recreate the data while the KL divergence attempts to create a general distribution. So there's a tradeoff between reconstruction and generalizing.\n",
    "\n",
    "After training, we can simple feed Gaussian noise into the decoder to generate novel samples. This is the first example of generative networks that we have seen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e52e014-0c89-43a0-b32e-e7ce32414b81",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a1e4f24-b267-409d-ae3c-397e0d2a89e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kl_divergence(z: torch.Tensor, mu: torch.Tensor, sigma: torch.Tensor) -> float:\n",
    "    \"\"\"This calculates the KL divergence\"\"\"\n",
    "\n",
    "    return 0.5 * (sigma ** 2 + mu ** 2 - torch.log(sigma) - 1.).sum()\n",
    "\n",
    "\n",
    "def get_reconstruction_loss(x: torch.Tensor, decoded_x: torch.Tensor, epsilon: nn.Parameter) -> torch.Tensor:\n",
    "    \"\"\"Gets reconstruction loss of latent space from a standard distrubtion\"\"\"\n",
    "\n",
    "    # get predicted distribtion\n",
    "    exp_scale = torch.exp(epsilon)\n",
    "    dist = torch.distributions.Normal(decoded_x, exp_scale)\n",
    "\n",
    "    # measure get p(x|z) using the predicted distribution\n",
    "    log_pxz = dist.log_prob(x)\n",
    "    \n",
    "    return log_pxz.sum(dim=(1, 2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c1ef42-278d-4944-bb9e-4e2b4b4272c1",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7903dc85-5adb-4440-be1a-fb6b6aa6bd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, \n",
    "                 kernel_size: int = 3, \n",
    "                 dropout: float = 0.25,\n",
    "                 cnn_layer_dims: list = [128, 64, 32],\n",
    "                 padding: int = 1, \n",
    "                 stride: int = 2,\n",
    "                 image_input_channels: int = 1,\n",
    "                 latent_dim: int = 32,\n",
    "                 input_xy: int = 28, \n",
    "                 lr: float = 1e-4, \n",
    "                 weight_decay: float = 0., \n",
    "                 eps: float = 5e-7, \n",
    "                 activation: torch.nn.modules.activation = F.gelu,\n",
    "                 use_wandb: bool=False,\n",
    "                 scheduler_name: str = \"none\",\n",
    "                 step_size: int = 5,\n",
    "                 gamma: float = 0.5,\n",
    "                 output_padding: int = 1,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        ### Always need to call above function first in order\n",
    "        ### to properly initialize a model\n",
    "        '''Basic CNN to classify fashion MNIST\n",
    "        We aren't going to both with some of the fancier stuff from the MLP, but it's easy enough to apply here too\n",
    "        '''\n",
    "        \n",
    "        # model parameters\n",
    "        self.cnn_dims = cnn_layer_dims\n",
    "        self.image_input_channels = image_input_channels\n",
    "        self.activation = activation\n",
    "        self.lr = lr\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.dropout = dropout\n",
    "        self.n_channels = len(cnn_layer_dims)\n",
    "        self.scheduler_name = scheduler_name\n",
    "        # if using a scheduler\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # standard normal (attempted p(z))\n",
    "        self.normal_distribtion = torch.distributions.Normal(0., 1.)\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # log using WandB or TensorBoard\n",
    "        self.use_wandb = use_wandb\n",
    "\n",
    "        # what the input data looks like (allows construction of graph for logging)\n",
    "        # (batch_size, channels, height, width)\n",
    "        self.example_input_array = torch.zeros(\n",
    "            (1, image_input_channels, input_xy, input_xy,),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "\n",
    "        # we still use our old encoder/decoder\n",
    "        self.encoder = Encoder(\n",
    "                  cnn_layer_dims=cnn_layer_dims, \n",
    "                  input_xy=input_xy, \n",
    "                  activation=activation, \n",
    "                  n_channels=n_channels,\n",
    "                  use_wandb=use_wandb,\n",
    "                  dropout=dropout, \n",
    "                  padding=padding,\n",
    "                  eps=eps, lr=lr, \n",
    "                  weight_decay=weight_decay,\n",
    "                  scheduler_name=scheduler_name,\n",
    "                  gamma=gamma,\n",
    "                  step_size=step_size,\n",
    "        )\n",
    "\n",
    "        # get output dimensions of encoder\n",
    "        self.encoded_cxy, self.flat_dim = self._flat_layer_size()\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "                  cnn_layer_dims=cnn_layer_dims[::-1], \n",
    "                  input_xy=self.encoded_cxy[-1], \n",
    "                  activation=activation, \n",
    "                  n_input_channels=cnn_layer_dims[-1],\n",
    "                  image_input_channels=image_input_channels,\n",
    "                  use_wandb=use_wandb,\n",
    "                  dropout=dropout, \n",
    "                  padding=padding,\n",
    "                  eps=eps, lr=lr, \n",
    "                  weight_decay=weight_decay,\n",
    "                  scheduler_name=scheduler_name,\n",
    "                  gamma=gamma,\n",
    "                  step_size=step_size,\n",
    "                  output_padding=output_padding,\n",
    "        )\n",
    "\n",
    "\n",
    "        self.mu = nn.Linear(self.flat_dim, self.latent_dim)\n",
    "        self.sigma = nn.Linear(self.flat_dim, self.latent_dim)\n",
    "        self.epsilon = nn.Parameter(torch.Tensor([0.]))\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        ### does some fancy layer weight initialization\n",
    "        nn.init.xavier_uniform_(self.mu.weight)\n",
    "        nn.init.xavier_uniform_(self.sigma.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''Determines how data is passed through the network, \n",
    "           i.e creates the connectivity of the network'''\n",
    "        \n",
    "        # encode\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # put into latent space\n",
    "        x = nn.Flatten()(x)\n",
    "\n",
    "        # get distribution of latent space\n",
    "        mu_hat = self.mu(x)\n",
    "        sigma_hat = torch.exp(self.sigma(x))\n",
    "\n",
    "        # decode\n",
    "        x = x.reshape(x.shape[0], \n",
    "                      -1, \n",
    "                      self.encoded_cxy[-1], \n",
    "                      self.encoded_cxy[-1])\n",
    "\n",
    "        # import pdb; pdb.set_trace()\n",
    "        \n",
    "        x = self.decoder(x)\n",
    "        return nn.ReLU()(x), mu_hat, sigma_hat\n",
    "\n",
    "    def configure_optimizers(self) -> (list, list):\n",
    "        \"\"\"Set up the optimizer and potential learning rate scheduler\"\"\"\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            eps=self.eps,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "\n",
    "        if self.scheduler_name == \"none\":\n",
    "            return self.optimizer\n",
    "\n",
    "        ### this decreases the learning rate by a factor of gamma every step_size\n",
    "        self.scheduler = MultiStepLR(\n",
    "            self.optimizer,\n",
    "            list(range(0, self.trainer.max_epochs, self.step_size)),\n",
    "            gamma=self.gamma,\n",
    "        )\n",
    "\n",
    "        return [self.optimizer], [{\"scheduler\": self.scheduler, \"interval\": \"epoch\"}]\n",
    "        \n",
    "    #### need to add these two things in case the scheduler is used ####\n",
    "    def lr_scheduler_step(self, scheduler, metric) -> None:\n",
    "        if self.scheduler_name != \"none\":\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def on_epoch_end(self) -> None:\n",
    "        if self.scheduler_name != \"none\":\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def process_batch(self, batch, step: str = \"train\"):\n",
    "        \"\"\"Passes and logs a batch for a given type of step (test, train, validation)\"\"\"\n",
    "        \n",
    "        # get data (batch includes labels, which we don't need)\n",
    "        x, _ = batch\n",
    "\n",
    "        # pass through network\n",
    "        x_pred, mu_hat, sigma_hat = self(x)\n",
    "\n",
    "        # get latent variables\n",
    "        z = mu_hat + sigma_hat * self.normal_distribtion.sample(mu_hat.shape).to(self.device)\n",
    "\n",
    "        recon_loss = get_reconstruction_loss(x, x_pred, self.epsilon).mean()\n",
    "        kl_divergence = get_kl_divergence(z, mu_hat, sigma_hat)\n",
    "\n",
    "        loss = kl_divergence - recon_loss\n",
    "\n",
    "        self.log(f\"{step}_loss\", loss)\n",
    "        self.log(f\"{step}_avg_reconstruction_loss\", recon_loss)\n",
    "        self.log(f\"{step}_kl_divergence\", kl_divergence)\n",
    "        self.log(f\"{step}_avg_mu\", mu_hat.mean())\n",
    "        self.log(f\"{step}_avg_sigma\", sigma_hat.mean())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"What do do with a training batch\"\"\"\n",
    "        \n",
    "        return self.process_batch(batch, step=\"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        '''Validation step (at the end of each epoch)'''\n",
    "        \n",
    "        return self.process_batch(batch, step=\"val\")\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \n",
    "        '''Test step is essentially the same as a validation step in this instance'''\n",
    "        return self.process_batch(batch, step=\"test\")\n",
    "\n",
    "    def _flat_layer_size(self) -> int:\n",
    "        \n",
    "        '''Gets the dimension of the flattened CNN output layer'''\n",
    "        \n",
    "        x = self.example_input_array\n",
    "        \n",
    "        # Pass the input tensor through the CNN layers\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        array_dim = x.size()\n",
    "        # Calculate the flattened layer dimension\n",
    "        flattened_dim = x.view(1, -1).size(1)\n",
    "\n",
    "        return array_dim, flattened_dim\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265a3839-81db-4479-90c4-9d7e4ec4d55e",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "128731a0-fe89-4316-89ef-3fb81e44b999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyper parameters \n",
    "lr = 5e-4\n",
    "eps = 1e-8\n",
    "weight_decay = 1e-7\n",
    "dropout = 0.25\n",
    "cnn_layer_dims  = [128,]\n",
    "n_cnn_layers = len(cnn_layer_dims)\n",
    "latent_dim = 32\n",
    "activation = F.gelu\n",
    "n_channels = 1\n",
    "stride = 2\n",
    "kernel_size = 3\n",
    "padding = 0\n",
    "output_padding = 1\n",
    "scheduler_name = \"step\"\n",
    "gamma = 0.5\n",
    "step_size = 5\n",
    "\n",
    "## WandB stuff\n",
    "# log with WandB or TensorBoard\n",
    "use_wandb = False\n",
    "# do hyperparameter sweep with WandB\n",
    "use_sweep = False\n",
    "# WandB project name\n",
    "project_name = 'FashionMNIST_VAE'\n",
    "# WandB lab name\n",
    "entity = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a28b3149-62d9-4200-8ad7-e0a7ca0177e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model = VariationalAutoencoder(latent_dim=latent_dim,\n",
    "                  cnn_layer_dims=cnn_layer_dims, \n",
    "                  input_xy=input_xy, \n",
    "                  activation=activation, \n",
    "                  image_input_channels=n_channels,\n",
    "                  use_wandb=use_wandb,\n",
    "                  dropout=dropout, \n",
    "                  eps=eps, \n",
    "                  lr=lr, \n",
    "                  weight_decay=weight_decay,\n",
    "                  scheduler_name=scheduler_name,\n",
    "                  gamma=gamma,\n",
    "                  step_size=step_size,\n",
    "                  stride=stride,\n",
    "                  padding=padding,\n",
    "                  output_padding=output_padding,\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d1e623-010b-4bbc-8fe6-2e9c8cf0ea32",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "406fb799-ba31-49b6-be22-cdefad32463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8242d068-da74-49cf-a851-c5589af1cf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator_name = \"mps\"\n",
    "# accelerator_name = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a77a820a-7bad-4647-99dd-82fbabff95f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boilerplate to get GPU if possible\n",
    "if accelerator_name == \"mps\":\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "elif accelerator_name == \"cuda\":\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab7b73b9-571f-4f01-a551-38f145142074",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model = vae_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6729aad4-5aa1-400b-928d-04a7750c0cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VariationalAutoencoder(\n",
       "  (encoder): Encoder(\n",
       "    (input_cnn): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (encoding_layers): ModuleList()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (input_cnn): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (decoding_layers): ModuleList(\n",
       "      (0): ConvTranspose2d(128, 1, kernel_size=(3, 3), stride=(2, 2), output_padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (mu): Linear(in_features=4608, out_features=32, bias=True)\n",
       "  (sigma): Linear(in_features=4608, out_features=32, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "11fab0e8-2ab0-4645-8b51-a65038bfe2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "if not use_wandb:\n",
    "    %load_ext tensorboard\n",
    "    vae_logger = TensorBoardLogger(\"vae_logs\", name=\"simple_mnist_fashion_vae\")\n",
    "    run_name = \"vae_cnn\"\n",
    "else:\n",
    "    logger_kwargs = {\n",
    "        \"resume\": \"allow\",\n",
    "        \"config\": model_hparams,\n",
    "    }\n",
    "    vae_logger = WandbLogger(project=project_name, entity=entity, **logger_kwargs)\n",
    "    vae_run_name = vae_logger.experiment.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "08acbadf-fd8e-47fb-b263-b7f0036604e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "#### necessary for newer PTL versions\n",
    "devices = 1\n",
    "accelerator = \"cpu\" if device.type == \"cpu\" else \"gpu\"\n",
    "\n",
    "# make the trainer\n",
    "vae_trainer = pl.Trainer(\n",
    "    devices=devices,\n",
    "    accelerator=accelerator,\n",
    "    max_epochs=num_epochs,\n",
    "    log_every_n_steps=1,\n",
    "    logger=vae_logger,\n",
    "    # reload_dataloaders_every_epoch=True,\n",
    "    callbacks=[\n",
    "        # ModelCheckpoint(\n",
    "        #     save_weights_only=False,\n",
    "        #     mode=\"min\",\n",
    "        #     monitor=\"val_acc\",\n",
    "        #     save_top_k=1,\n",
    "        #     every_n_epochs=1,\n",
    "        #     save_on_train_epoch_end=False,\n",
    "        #     dirpath=f\"/AE_Checkpoints/{run_name}/\",\n",
    "        #     filename=f\"ae_checkpoint_{run_name}\",\n",
    "        # ),\n",
    "        LearningRateMonitor(\"epoch\"),\n",
    "        progress.TQDMProgressBar(refresh_rate=1),\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            min_delta=0,\n",
    "            patience=10,\n",
    "            verbose=False,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "vae_trainer.logger._log_graph = True\n",
    "vae_trainer.logger._default_hp_metric = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e42c144-96a5-4551-9f00-2739cc1a594e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name    | Type    | Params | In sizes       | Out sizes     \n",
      "----------------------------------------------------------------------\n",
      "0 | encoder | Encoder | 1.3 K  | [1, 1, 28, 28] | [1, 128, 6, 6]\n",
      "1 | decoder | Decoder | 148 K  | [1, 128, 6, 6] | [1, 1, 28, 28]\n",
      "2 | mu      | Linear  | 147 K  | [1, 4608]      | [1, 32]       \n",
      "3 | sigma   | Linear  | 147 K  | [1, 4608]      | [1, 32]       \n",
      "----------------------------------------------------------------------\n",
      "444 K     Trainable params\n",
      "0         Non-trainable params\n",
      "444 K     Total params\n",
      "1.780     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonterry/miniforge3/envs/pytorch_python10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/jasonterry/miniforge3/envs/pytorch_python10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c76a7eff0eb34f0991e6cf9e05151fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    }
   ],
   "source": [
    "vae_trainer.fit(vae_model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456d2ab7-0352-406a-b8b6-b07f6449b458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f05e78c4-690e-408f-bcaf-6ac83b08af12",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7e425c4c-7247-4622-99ff-1c4acab34f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonterry/miniforge3/envs/pytorch_python10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ebc804a47f74dfe9f70c5d2026efc63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Test metric                 DataLoader 0\n",
      "\n",
      "        test_avg_mu           -0.00018213927978649735\n",
      "test_avg_reconstruction_loss     566.9366455078125\n",
      "       test_avg_sigma            0.7072133421897888\n",
      "     test_kl_divergence           -2.4510657787323\n",
      "         test_loss               -569.3875732421875\n",
      "\n",
      "[{'test_loss': -569.3875732421875, 'test_avg_reconstruction_loss': 566.9366455078125, 'test_kl_divergence': -2.4510657787323, 'test_avg_mu': -0.00018213927978649735, 'test_avg_sigma': 0.7072133421897888}]\n"
     ]
    }
   ],
   "source": [
    "## Get test metrics\n",
    "vae_test_results = vae_trainer.test(vae_model, test_loader)\n",
    "print(vae_test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "69a0afb6-61b6-4327-96c4-dd920224ac32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c4da537c1651ddae\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c4da537c1651ddae\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6002;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## open up TensorBoard\n",
    "if not use_wandb:\n",
    "    %tensorboard --logdir vae_logs --port 6002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81081204-52be-47de-ac92-0660a6412c74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92176d09-de4b-434c-b7cf-940c5c5c2a4b",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d473cbc-5cf5-4cc2-b7a1-b971e365dca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_input = torch.randn(vae_model.encoded_cxy).float()#.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8788f32e-574b-44c3-8389-9c7388d2c7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_output = vae_model.decoder(random_input).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ae7e2000-b263-4e7c-a81b-15369db2403e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x31d0d1510>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(random_output.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3b8d11-bdea-4b99-bfff-bee3b39c4141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bb7d8b-e8c6-4751-8ac3-e3faa8536ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef085d1-73ce-47ef-a873-c621706f882e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
