{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22446334-7805-4a42-a0e5-1166c4e1c63c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4081cf-a6b9-443f-b577-7b83320af637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460ea292-0ed3-4225-848d-0258ab8bed01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a45113-fecf-49f2-adb9-25be6310c863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7739191b-c250-4839-9ded-388a28bdbec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cab99854-cb77-499b-b490-c34acb734dab",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92c7174e-9526-49ee-ac22-aabe1fce923f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting a prameters\n",
    "scale_factor = 1.5\n",
    "\n",
    "labels = 16 * scale_factor\n",
    "ticks = 10 * scale_factor\n",
    "# ticks = 10 * scale_factor\n",
    "legends = 12 * scale_factor\n",
    "text = 14 * scale_factor\n",
    "titles = 22 * scale_factor\n",
    "lw = 3 * scale_factor\n",
    "ps = 200 * scale_factor\n",
    "cmap = 'magma'\n",
    "\n",
    "colors = ['firebrick', 'steelblue', 'darkorange', 'darkviolet', 'cyan', 'magenta', 'darkgreen', 'deeppink']\n",
    "markers = ['x', 'o', '+', '>', '*', 'D', '4']\n",
    "linestyles = ['-', '--', ':', '-.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff14fed2-13ce-4cc9-b896-ad55ce737793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import PIL\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint, progress\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger, TensorBoardLogger\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, random_split\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2846ea57-5c58-450e-83b8-03be2cc61dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12e1ee5d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Set seed for reproducibility\n",
    "np.random.seed(123)\n",
    "random.seed(123)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39a9765-378a-43e7-900e-17ca30fc2d66",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab0bacc2-e140-405e-8d0e-755abbd4507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define loading transformations\n",
    "transforms = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    # T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "### Download fashion MNIST\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root='./data', train=True, download=True,\n",
    "    transform=transforms)\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root='./data', train=False, download=True,\n",
    "    transform=transforms)\n",
    "\n",
    "### make data loaders\n",
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbcbf8b7-ea95-45d3-a4a0-8754f1d4ff92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonterry/miniforge3/envs/pytorch_python10/lib/python3.10/site-packages/torchvision/datasets/mnist.py:75: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n"
     ]
    }
   ],
   "source": [
    "xy = int(train_dataset.train_data.size(1) * train_dataset.train_data.size(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b584a-1dcb-4138-bc44-d831dbabc925",
   "metadata": {},
   "source": [
    "## Define models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3175b3-3a47-4c16-a9e6-7cfe1fbc44f8",
   "metadata": {},
   "source": [
    "### Individual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c498f004-30bf-4d9b-a1eb-677bdd1a9eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = 784,\n",
    "        output_dim: int = 1,\n",
    "        lr: float = 1e-4,\n",
    "        weight_decay: float = 1e-8,\n",
    "        adam_eps: float = 1e-7,\n",
    "        weight_init: str = \"xavier\",\n",
    "        num_mlp_layers: int = 5,\n",
    "        mlp_layer_dim: int = 128,\n",
    "        activation: str = \"gelu\",\n",
    "        leaky_relu_frac: float = 0.2,\n",
    "        dropout: float = 0.2,\n",
    "        loss = nn.BCELoss(),\n",
    "        feature_factor: float = 2.,\n",
    "        final_activation = nn.Sigmoid(),\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.adam_eps = adam_eps\n",
    "        self.weight_init = weight_init\n",
    "        self.num_mlp_layers = num_mlp_layers\n",
    "        self.mlp_layer_dim = mlp_layer_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.output_layers = nn.ModuleList()\n",
    "        self.activation_layers = nn.ModuleList()\n",
    "\n",
    "        for _i in range(num_mlp_layers):\n",
    "            if _i > 0:\n",
    "                # import pdb; pdb.set_trace()\n",
    "                self.layers.append(nn.Linear(self.layers[-1].out_features, \n",
    "                                             int(feature_factor*self.layers[-1].out_features)))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(self.input_dim, mlp_layer_dim))\n",
    "\n",
    "            self.activation_layers.append(\n",
    "                nn.GELU() if activation == \"gelu\" else nn.LeakyReLU(leaky_relu_frac, inplace=True)\n",
    "            )\n",
    "            self.activation_layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        self.output_layers.append(nn.Linear(self.layers[-1].out_features, self.output_dim))\n",
    "        self.output_layers.append(final_activation)\n",
    "\n",
    "        self.init_weights()  # Initialize the weights\n",
    "\n",
    "        self.loss = loss\n",
    "\n",
    "    def init_weights(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                if self.weight_init == \"xavier\":\n",
    "                    nn.init.xavier_uniform_(layer.weight)  # Apply Xavier initialization\n",
    "                elif self.weight_init == \"kaiming\":\n",
    "                    nn.init.kaiming_uniform_(layer.weight)  # Apply Kaiming initialization\n",
    "                elif self.weight_init == \"orthogonal\":\n",
    "                    nn.init.orthogonal_(layer.weight)  # Apply orthogonal initialization\n",
    "\n",
    "                nn.init.constant_(layer.bias, 0.0)\n",
    "\n",
    "    def forward(self, z):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            z = layer(z)\n",
    "            z = self.activation_layers[i](z)\n",
    "        for layer in self.output_layers:\n",
    "            z = layer(z)\n",
    "        return z\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.parameters(), lr=self.lr, eps=self.adam_eps, weight_decay=self.weight_decay\n",
    "        )\n",
    "        return self.optimizer\n",
    "\n",
    "\n",
    "class Generator(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = 64,\n",
    "        output_dim: int = 784,\n",
    "        lr: float = 1e-4,\n",
    "        weight_decay: float = 1e-8,\n",
    "        adam_eps: float = 1e-7,\n",
    "        weight_init: str = \"xavier\",\n",
    "        num_mlp_layers: int = 3,\n",
    "        mlp_layer_dim: int = 128,\n",
    "        activation: str = \"gelu\",\n",
    "        leaky_relu_frac: float = 0.2,\n",
    "        dropout: float = 0.2,\n",
    "        loss = nn.BCELoss(),\n",
    "        feature_factor: float = 2.,\n",
    "        final_activation = nn.Tanh(),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = MLP(input_dim=input_dim,\n",
    "                         output_dim=output_dim,\n",
    "                         lr=lr,\n",
    "                         weight_decay=weight_decay,\n",
    "                         adam_eps=adam_eps,\n",
    "                         weight_init=weight_init,\n",
    "                         num_mlp_layers=num_mlp_layers,\n",
    "                         mlp_layer_dim=mlp_layer_dim,\n",
    "                         activation=activation,\n",
    "                         leaky_relu_frac=leaky_relu_frac,\n",
    "                         dropout=dropout,\n",
    "                         loss=loss,\n",
    "                         feature_factor=feature_factor,\n",
    "                         final_activation=final_activation,\n",
    "                        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.parameters(), lr=self.lr, eps=self.adam_eps, weight_decay=self.weight_decay\n",
    "        )\n",
    "        return self.optimizer\n",
    "        \n",
    "\n",
    "class Discriminator(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = 784,\n",
    "        output_dim: int = 1,\n",
    "        lr: float = 1e-4,\n",
    "        weight_decay: float = 1e-8,\n",
    "        adam_eps: float = 1e-7,\n",
    "        weight_init: str = \"xavier\",\n",
    "        num_mlp_layers: int = 3,\n",
    "        mlp_layer_dim: int = 128,\n",
    "        activation: str = \"gelu\",\n",
    "        leaky_relu_frac: float = 0.2,\n",
    "        dropout: float = 0.2,\n",
    "        loss = nn.BCELoss(),\n",
    "        feature_factor: float = 2.,\n",
    "        final_activation = nn.Sigmoid(),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = MLP(input_dim=input_dim,\n",
    "                         output_dim=output_dim,\n",
    "                         lr=lr,\n",
    "                         weight_decay=weight_decay,\n",
    "                         adam_eps=adam_eps,\n",
    "                         weight_init=weight_init,\n",
    "                         num_mlp_layers=num_mlp_layers,\n",
    "                         mlp_layer_dim=mlp_layer_dim,\n",
    "                         activation=activation,\n",
    "                         leaky_relu_frac=leaky_relu_frac,\n",
    "                         dropout=dropout,\n",
    "                         loss=loss,\n",
    "                         feature_factor=feature_factor,\n",
    "                         final_activation=final_activation,\n",
    "                        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.parameters(), lr=self.lr, eps=self.adam_eps, weight_decay=self.weight_decay\n",
    "        )\n",
    "        return self.optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d253758f-8281-4878-8dab-c1ca40448bb4",
   "metadata": {},
   "source": [
    "### Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54c14c8c-8d22-4f4c-a033-a9b57b80aca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_dim: int = 784,\n",
    "        latent_dim: int = 64,\n",
    "        lr: float = 1e-4,\n",
    "        weight_decay: float = 1e-8,\n",
    "        adam_eps: float = 1e-7,\n",
    "        weight_init: str = \"xavier\",\n",
    "        use_batchnorm: bool = False,\n",
    "        num_mlp_layers: int = 3,\n",
    "        mlp_layer_dim: int = 128,\n",
    "        activation: str = \"gelu\",\n",
    "        dropout: float = 0.2,\n",
    "        padded_spectra: bool = False,\n",
    "        encoder_name: str = \"\",\n",
    "        # **kwargs: dict,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # do this because we are using multiple optimizers\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        self.img_dim = img_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.classifier = Discriminator(\n",
    "            input_dim=img_dim,\n",
    "            feature_factor=0.5,\n",
    "        )\n",
    "\n",
    "        self.generator = Generator(\n",
    "            input_dim=latent_dim,\n",
    "            output_dim=img_dim,\n",
    "            feature_factor=2.,\n",
    "        )\n",
    "\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.adam_eps = adam_eps\n",
    "        self.weight_init = weight_init\n",
    "        self.activation = activation\n",
    "\n",
    "\n",
    "    def process_batch(self, x, step: str = \"train\"):\n",
    "        \n",
    "        # get optimizers (different for each model)\n",
    "        d_opt, g_opt = self.optimizers()\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        ##### train discrimator ####\n",
    "        # these are real (hence, 1)\n",
    "        x_real = x.view(-1, self.img_dim).float().to(self.device)\n",
    "        y_real = torch.ones(batch_size, 1).float().to(self.device)\n",
    "\n",
    "        # classify real data\n",
    "        real_classification = self.classifier(x_real)\n",
    "\n",
    "        # get classifier loss\n",
    "        real_loss = self.classifier.model.loss(real_classification, y_real)\n",
    "\n",
    "        # now use fake data (y = 0)/random input\n",
    "        z = torch.randn(batch_size, self.latent_dim).float().to(device)\n",
    "        \n",
    "        # send random data through generator\n",
    "        x_fake = self.generator(z)\n",
    "        y_fake = torch.zeros(batch_size, 1).float().to(device)\n",
    "    \n",
    "        fake_classification = self.classifier(x_fake)\n",
    "        fake_loss = self.classifier.model.loss(fake_classification, y_fake)\n",
    "\n",
    "        class_loss = 0.5 * (fake_loss + real_loss)\n",
    "\n",
    "        if step == \"train\":\n",
    "            d_opt.zero_grad()\n",
    "            self.manual_backward(class_loss)\n",
    "            d_opt.step()\n",
    "\n",
    "            \n",
    "        ##### train generator ####\n",
    "        # random input\n",
    "        z = torch.randn(batch_size, self.latent_dim).float().to(device)\n",
    "\n",
    "        # 1 because we are trying to trick classifier\n",
    "        gen_labels = torch.ones(batch_size, 1).to(device)\n",
    "\n",
    "        # generate some data\n",
    "        gen_output = self.generator(z)\n",
    "        # classify data\n",
    "        class_output = self.classifier(gen_output)\n",
    "        gen_loss = self.classifier.model.loss(class_output, gen_labels)\n",
    "\n",
    "        if step == \"train\":\n",
    "            g_opt.zero_grad()\n",
    "            self.manual_backward(gen_loss)\n",
    "            g_opt.step()\n",
    "\n",
    "        self.log(f\"{step}_d_loss\", class_loss)\n",
    "\n",
    "        self.log(f\"{step}_g_loss\", gen_loss)\n",
    "\n",
    "        return {f\"{step}_d_loss\": class_loss, f\"{step}_g_loss\": gen_loss}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # import pdb; pdb.set_trace()\n",
    "        x, _ = batch\n",
    "        return self.process_batch(x, step=\"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        return self.process_batch(x, step=\"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        return self.process_batch(x, step=\"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        ## one optimizer for each model\n",
    "        self.d_opt = torch.optim.Adam(\n",
    "            self.classifier.parameters(), lr=self.lr, eps=self.adam_eps, weight_decay=self.weight_decay\n",
    "        )\n",
    "        self.g_opt = torch.optim.Adam(\n",
    "            self.generator.parameters(), lr=self.lr, eps=self.adam_eps, weight_decay=self.weight_decay\n",
    "        )\n",
    "        return self.d_opt, self.g_opt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a270db-9555-4a50-8359-acc2bae51143",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad49eb37-8b40-4698-8d22-6380871fad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "accelerator_name = \"mps\"\n",
    "accelerator_name = \"cpu\"\n",
    "\n",
    "# boilerplate to get GPU if possible\n",
    "if accelerator_name == \"mps\":\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "elif accelerator_name == \"cuda\":\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f41ff0c-7bce-445b-b1e4-c1bcdb623b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "use_wandb = False\n",
    "if not use_wandb:\n",
    "    %load_ext tensorboard\n",
    "    cnn_logger = TensorBoardLogger(\"gan_logs\", name=\"mnist_fashion_gan\")\n",
    "    run_name = \"cnn\"\n",
    "else:\n",
    "    logger_kwargs = {\n",
    "        \"resume\": \"allow\",\n",
    "        \"config\": model_hparams,\n",
    "    }\n",
    "    cnn_logger = WandbLogger(project=project_name, entity=entity, **logger_kwargs)\n",
    "    cnn_run_name = cnn_logger.experiment.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3e76dad-6314-4bb8-bbf3-dd0c2df9e9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "#### necessary for newer PTL versions\n",
    "devices = 1 \n",
    "accelerator = \"gpu\" if accelerator_name != \"cpu\" else \"cpu\"\n",
    "\n",
    "# make the trainer\n",
    "trainer = pl.Trainer(\n",
    "    devices=devices,\n",
    "    accelerator=accelerator,\n",
    "    max_epochs=num_epochs,\n",
    "    log_every_n_steps=1,\n",
    "    logger=cnn_logger,\n",
    "    # reload_dataloaders_every_epoch=True,\n",
    "    callbacks=[\n",
    "        LearningRateMonitor(\"epoch\"),\n",
    "        progress.TQDMProgressBar(refresh_rate=1),\n",
    "        EarlyStopping(\n",
    "            monitor=\"train_g_loss\",\n",
    "            min_delta=0,\n",
    "            patience=10,\n",
    "            verbose=False,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "trainer.logger._log_graph = True\n",
    "trainer.logger._default_hp_metric = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40c02fca-ae0a-4e5d-a789-abe89c81f072",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 64\n",
    "num_mlp_layers = 3\n",
    "mlp_layer_dim = 128\n",
    "gan = GAN(img_dim=xy, latent_dim=latent_dim, mlp_layer_dim=mlp_layer_dim, num_mlp_layers=num_mlp_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8957e2f7-6b0b-4da5-97f4-1923ce017259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name       | Type          | Params\n",
      "---------------------------------------------\n",
      "0 | classifier | Discriminator | 110 K \n",
      "1 | generator  | Generator     | 575 K \n",
      "---------------------------------------------\n",
      "685 K     Trainable params\n",
      "0         Non-trainable params\n",
      "685 K     Total params\n",
      "2.744     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d959a6558e44b77a4250dd99ef05cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(gan, train_dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed093855-08dc-4455-b15b-072ac1b97a1c",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "36f5df4f-b22c-44db-9e3a-2789fcf37a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonterry/miniforge3/envs/pytorch_python10/lib/python3.10/site-packages/pytorch_lightning/loggers/tensorboard.py:190: UserWarning: Could not log computational graph to TensorBoard: The `model.example_input_array` attribute is not set or `input_array` was not given.\n",
      "  rank_zero_warn(\n",
      "/Users/jasonterry/miniforge3/envs/pytorch_python10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2edc3b44aa54553a8a28627b613c83a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       test_d_loss          0.6979624032974243\n",
      "       test_g_loss          0.6921571493148804\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[{'test_d_loss': 0.6979624032974243, 'test_g_loss': 0.6921571493148804}]\n"
     ]
    }
   ],
   "source": [
    "## Get test metrics\n",
    "test_results = trainer.test(gan, test_loader)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cb6b8564-7a26-4064-b204-6fc1a62e0725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6008 (pid 99629), started 0:00:17 ago. (Use '!kill 99629' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c4da537c1651ddae\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c4da537c1651ddae\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## open up TensorBoard\n",
    "if not use_wandb:\n",
    "    %tensorboard --logdir gan_logs --port 6008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838a1cb4-b862-45f5-8dd0-ff134996314b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12df960c-e192-4b63-8fee-e852ad7ca8b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6db15892-0520-4056-861e-25c17d0fde5d",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98f5d100-3146-4a45-a9ee-9f0aa5c173eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_imgs = 10\n",
    "random_inputs = torch.randn(num_imgs, latent_dim).float().to(device)\n",
    "\n",
    "generated_imgs = gan.generator(random_inputs).view(num_imgs, 1, 28, 28).detach().cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1120ce67-bc9e-409c-b595-547ae0fb97f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10., 10.))\n",
    "\n",
    "img_index = 1\n",
    "plt.imshow(generated_imgs[img_index], cmap=\"viridis\")\n",
    "\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7950e4c7-f6e6-42e6-885b-d26ba2390b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10., 10.))\n",
    "\n",
    "img_index = 0\n",
    "plt.imshow(train_dataset[img_index][0].numpy().squeeze(), cmap=\"viridis\")\n",
    "\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875dd689-4a41-491d-ae7e-04b5821f7462",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
