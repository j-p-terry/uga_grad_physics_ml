{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f72d1f4-c9cf-4b21-89fa-05b2b5be9a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint, progress\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acb95698-61d7-48d6-ae1f-2b5aa3673c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10c3e8370>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Set seed for reproducibility\n",
    "np.random.seed(123)\n",
    "random.seed(123)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc175da7-49ea-472f-bd7d-23854b09015b",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f78907f-7132-4898-8efd-a1a6a388d1fa",
   "metadata": {},
   "source": [
    "\"Attention is All You Need\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624bd657-8312-41c5-b3d1-55ffff3f977b",
   "metadata": {},
   "source": [
    "<img src=\"imgs/transformer.png\" style=\"height:600px\" class=\"center\" alt=\"transformer\"/><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b0b7a8-379e-4b27-8064-8b2dada46349",
   "metadata": {},
   "source": [
    "Involves no recurrence. Instead, uses multi-head self-attention. This is a significant improvement in the basic attention mechanism we saw with RNNs.\n",
    "\n",
    "Generally, the attention mechanism focuses on learning 3 components: the \"Query Vector\" (Q; $d_{k}$), \"Key Vector\" (K; $d_{k}$), and the \"Value Vector\" (V; $d_{v}$). These represent the word that is being considered, the other words that will be \"attended to,\" and the information learned about all of the words, respectively. In the context of encoder-decoder attention, the queries come from the decoder, while the keys and values come from the encoder.\n",
    "\n",
    "Adding multiple attention mechanisms, i.e., multiple learnable query, key, and value matrices, can improve performance in the same way that collaborating with a team across fields can be useful. This is \"multi-head attention.\" While this embeds each element into a smaller dimensional space (by a factor of the number of heads) than single-head attention, the use of multiple spaces allows the model to capture diversity more effectively.\n",
    "\n",
    "<img src=\"imgs/multihead_attention.png\" style=\"height:500px\" class=\"center\" alt=\"mh_attention\"/><br>\n",
    "\n",
    "By using the context vector along with the hidden state, we add the global context of the current state with respect to the entire past, thereby allowing information to flow across time or be ignored.\n",
    "\n",
    "Another great aspect of attention is that it is easily visualized, so you can see where the network is focusing on.\n",
    "\n",
    "Simply put, self-attention looks at how the *inputs* interact with one another rather than how the hidden states evolve over time. This gives the model much more information about the context of an input vector (e.g., word) itself rather than the hidden state it results in, giving it the ability to dynamically focus on different parts of the input sequence. This reduces the \"path length\" between words, allowing information to flow across long ranges in the sequence. It also comes with the benefit that this can be done completely in parallel, which provides a huge speedup. ($O(1)$!!!)\n",
    "\n",
    "$$\\rm{Attention(Q, K, V)} = \\rm{softmax}\\left( \\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right)V$$\n",
    "\n",
    "However, this all comes with the caveat that now we lose a lot of the positional information that RNNs easily capture. This leads to the idea of \"positional encoding\" wherein explicit information about the position is added to each embedded word. In the original transformer:\n",
    "\n",
    "$$PE(pos, 2i) = sin(pos/10000^{2i/d_{model}})$$\n",
    "$$PE(pos, 2i+1) = cos(pos/10000^{2i/d_{model}})$$\n",
    "\n",
    "where $pos$ is the position, $i$ is the diemsnsion of the embedding and $d_{model}$ is the output dimension. This can be made more complex and/or be learned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a287bf3d-3daa-4d13-ac59-1c61c6468643",
   "metadata": {},
   "source": [
    "All of these improvements now make transformers the go-to natural language processing algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb9bdef-8106-4d71-bebc-7311971870b9",
   "metadata": {},
   "source": [
    "# Vision Transformers (ViT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45de6854-b4c9-479e-a557-c1cdcf299896",
   "metadata": {},
   "source": [
    "Based on the realization that regions of an image (\"patches\") can replace typical embeddings in a transformer. This makes the image a sequence of patches (\"An Image is Worth 16x16 Words\"). These patches are then projected into a higher-dimensional space using a learnable network. The network then proceeds like a normal transformer.\n",
    "\n",
    "<img src=\"imgs/vit.png\" style=\"height:500px\" class=\"center\" alt=\"vit\"/><br>\n",
    "\n",
    "Similar to attention is traditional sequences, the attention can be visualized across the image.\n",
    "\n",
    "<img src=\"imgs/vit_attention.png\" style=\"height:600px\" class=\"center\" alt=\"vit_attention\"/><br>\n",
    "\n",
    "Positional encoding is still necessary, but the actual form of it is relatively irrelevant. Often, the exact positional encoding is learned.\n",
    "\n",
    "<img src=\"imgs/vit_pos_enc.png\" style=\"height:600px\" class=\"center\" alt=\"vit_pos_enc\"/><br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This removes convolutions from the process entirely. It is *not a convolutional neural network*. It is an entirely different type of architecture. When trained on a sufficient amount of data, it can outperform CNNs in many contexts. More recently, there have been efforts to more effectively join the two paradigms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a06517-3ec2-4077-88e6-2cd7bf216b10",
   "metadata": {},
   "source": [
    "__Homework__: apply ViT to Fashion MNIST. Compare with CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3f1ef7-b6cc-4e08-aa41-e6d7e9adad09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
