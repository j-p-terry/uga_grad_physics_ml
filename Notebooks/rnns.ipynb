{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab913925-ab30-4adc-a9e1-866a10b02e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm \n",
    "from matplotlib import rc as mplrc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import PIL\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint, progress\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger, TensorBoardLogger\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, random_split\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "# import torchvision\n",
    "# import torchvision.datasets as datasets\n",
    "# import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d3a36e8-7521-4891-b775-7ac63cd96138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1178fa4d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Set seed for reproducibility\n",
    "np.random.seed(123)\n",
    "random.seed(123)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d11645c-322b-4545-8b3c-83093c98f0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting a prameters\n",
    "scale_factor = 1.5\n",
    "\n",
    "labels = 16 * scale_factor\n",
    "ticks = 10 * scale_factor\n",
    "# ticks = 10 * scale_factor\n",
    "legends = 12 * scale_factor\n",
    "text = 14 * scale_factor\n",
    "titles = 22 * scale_factor\n",
    "lw = 3 * scale_factor\n",
    "ps = 200 * scale_factor\n",
    "cmap = 'magma'\n",
    "\n",
    "colors = ['firebrick', 'steelblue', 'darkorange', 'darkviolet', 'cyan', 'magenta', 'darkgreen', 'deeppink']\n",
    "markers = ['x', 'o', '+', '>', '*', 'D', '4']\n",
    "linestyles = ['-', '--', ':', '-.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e5456c1-8e96-45bb-be03-1541ac2e36ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb780740-80cd-4c7c-a76e-99aeb6724d87",
   "metadata": {},
   "source": [
    "# About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb31065-cd67-4eb7-8897-336738acb701",
   "metadata": {},
   "source": [
    "## Recurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b8bbc9-6116-42b2-bfd0-b182c5e023f1",
   "metadata": {},
   "source": [
    "Recurrent neural networks (RNNs) are the traditional (pre-transformer) way to deal with time-series/ordered data, including language. Examples include forecasting and translation. \n",
    "\n",
    "The key point of recurrence is that the outputs from the previous step are used as additional inputs to the current step. That is, the network takes in both typical features as well as previous predictions. The input features ($x_{t}$) and previous state ($h_{t-1}%$) go in, the recurrent block creates a hidden state vector ($h_{t}%$) that can be passed out to a prediction ($o_{t}$; perhaps going through an additional output layer) and as an additional input to the next step, $t+1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f6d4f1-cf5a-414d-a60e-83f7832c8cff",
   "metadata": {},
   "source": [
    "<img src=\"imgs/gru.png\" style=\"height:400px\" class=\"center\" alt=\"gru\"/><br>\n",
    "\n",
    "Example of an RNN using a gated recurrent unit (GRU). This is a fairly common architecture that attempts to improve on the long short-term memomory unit (LSTM), which was one of the original methods.\n",
    "\n",
    "The point isn't to understand the specifics within the unit, but to understand that there is a complex interplay of previous outputs (which in turn depend on the previous previous outputs, and so on) and the current inputs. This allows the unit to remember things that are important and forget things that are irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd717d9c-5963-49c0-8626-530127341d0d",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "#### How do you put words into a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aba79d-867f-4925-ac07-b98e0cf1f34b",
   "metadata": {},
   "source": [
    "Neural networks take in numerical data. Obviously, words (and other features) aren't inherently numerical. We have to do something about that if we want to be able to do anything with words and neural networks.\n",
    "\n",
    "This is where embedding comes in. In general, embedding is the process of taking data and translating it into a different space. In this case, we take words and embed them in a high-dimensional vector space.\n",
    "\n",
    "How do we do this? Naively, we might do something like give each word some vector, perhaps by alphabetical order according to some dictionary. We could also spell the words by embedding with the alphabet. The issue here though is that embeddings gives essentially no information about the meaning of the word itself. Sure, prefixes etc. can help you infer the word's meaning, but not all words have meaningful prefixes and they often are attached to words without obvious linguistic meaning. \n",
    "\n",
    "We need to be more creative if we want this to work. Ideally, we will embed the words such that the embedding contains meaningful information about the word itself and its relationship with other words in the vocabulary. I don't care how a word is spelled; I care about what it means. \n",
    "\n",
    "A traditional way to do this is with Word2Vec. While somewhat simple, it is still powerful and fairly quick. The idea is that words are \"tokenized\" (this is the naive way described above), which allows them to have a unique representation, even if it's not meaningful. Using these tokens, we encode into a latent space and decode back into token space. This is often done with filling in the blank (i.e. mask a word and use the surrounding words to predict it) or skip gram (i.e. use a single word to predict the surrounding words). This is unsupervised because all you need is a bunch of sentences that you can then mask at will. When we want to pass the words through our model, we embed them into the newly trained latent space. This latent space is extraordinarily powerful and allows you to do math with words, e.g.\n",
    "\n",
    "$$King - Man + Woman = Queen$$\n",
    "\n",
    "We are now prepared to move forward with training. \n",
    "\n",
    "While this particular example considers words, there are other instances in which you want to do embedding. Sometimes, this doesn't require the training of an embedding network. For example, my embedding could just be a typical feature vector (this is what we will do)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fe62a0-5fc0-46e1-8883-28c2de35a64c",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df53a8d6-1308-40cb-8e54-73889dc8f578",
   "metadata": {},
   "source": [
    "Attention has quickly become one of the most powerful concepts in machine learning. Its introduction qualitatively improved performance and led to the introduction of the transformer.\n",
    "\n",
    "As one can imagine, RNNs can eventually \"forget\" old information (technically, they don't retain long-term dependencies). This can be a huge problem because the old data can contain essential context and information that is relevant to current or future predictions. Think about those recipe websites where some crucial overview is given at the top, followed by 1834890 pages of discussions on how it was received by different species of animals at different times across latitudes when the temperature of the ambient air was modified with a space heater, only eventually giving the recipe itself. The intermediate information is absolutely useless for cooking purposes. The important information is at the very beginning. As people, we have the luxury of just scrolling past all the worthless junk, but networks have to process all of it and can forget the what we needed to remember because of all the intervening junk. We know it's not important because we've read these blogs before and hate them. The computer doesn't know this, so it may pay too much attention to the useless stuff.\n",
    "\n",
    "Attention is a method to address this.\n",
    "\n",
    "<img src=\"imgs/attention.jpeg\" style=\"height:300px\" class=\"center\" alt=\"attention\"/><br>\n",
    "\n",
    "Remember, RNNs have a hidden state, $h_{t}$, that serves as its memory for a given time step and can be used to compute the output. It is updated as the sequence is passed through the model. Oftentimes, only the last hidden state is used for predictions. However, by considering *all* previous hidden states, we can see what the model thought was important going all the way back to the beginning. This allows the model to assign \"attention scores\" to each hidden state component, giving the model the ability to focus on specific embeddings more so than others. Mathematically, these scores (call them $\\alpha_{t}$) are passed through the softmax activation function (sums components to 1), making a \"context vector.\"\n",
    "\n",
    "$$c_{i} = \\sum_{t} \\alpha_{t} h_{t, i}$$\n",
    "\n",
    "that is, the $i^{th}$ component of the context vector is a weighted sum of the $i^{th}$ component from each hidden state in the sequence with weights determined by the attention scores. This context vector is then used in conjunction with previous outputs to make the current prediction.\n",
    "\n",
    "We'll go into much more detail when we talk about transformers.\n",
    "<!-- The attention process involves learning. Each component is represented by a \"query vector\", $Q = X \\times W_{Q}, \\in \\mathbb{R}^{n\\times d_{q}}$, where $X \\in \\mathbb{R}^{n\\times  d_{model}}$ is the input sequence of embeddings, $n$ is the length of the sequence, $d_{q}$ is the dimension of the query vector, which is just the dimension of the hidden state ($d_{model}$). All other components in the sequence are represented by \"key vectors\", $K = \\frac{1}{\\sqrt{d_{k}}} X \\times W_{K}, \\in \\mathbb{R}^{n\\times d_{k}}$ ($d_{k} = d_{q} = d_{model}$). The each element also gets a \"value vector,\" $V = X \\times W_{V}, \\in \\mathbb{R}^{n\\times d_{v}}$ that maps the projections of the input sequence into a learned space. In the context of encoder-decoder attention, the queries come from the decoder, while the keys and values come from the encoder. All of the $W$ are learned in order to represent the queries, keys, and values in an effective manner. $\\frac{1}{\\sqrt{d_{k}}}Q\\times K^{T}$ gives you the attention scores of the $i^{th}$ entry in the sequence with the $j^{th}$ key (embedding). The attention weights are calculated by applying softmax. Finally, $\\mathrm{Attention\\; Output} = C \\times V$. -->\n",
    "\n",
    "<!-- <img src=\"imgs/attention_flow.jpeg\" style=\"height:600px\" class=\"center\" alt=\"attention\"/><br> -->\n",
    "\n",
    "<!-- Adding multiple attention mechanisms, i.e., multiple learnable query, key, and value matrices, can improve performance in the same way that collaborating with a team across fields can be useful. This is \"multi-head attention.\" Now, $d_{k} = d_{q} = d_{model}/h$, where $h$ is the number of attention heads. While this embeds each element into a smaller dimensional space than single-head attention, the use of multiple spaces allows the model to capture diversity more effectively.\n",
    "\n",
    "By using the context vector along with the hidden state, we add the global context of the current state with respect to the entire past, thereby allowing information to flow across time or be ignored.\n",
    "\n",
    "Another great aspect of attention is that it is easily visualized, so you can see where the network is focusing on.\n",
    "\n",
    "Self-attention is a more advanced version that we'll get into when talking about the Transformer. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe998326-49f0-40c9-bb4b-714ce19172f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8443970b-ec5c-45d0-86dc-a40ceb076f8a",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "979ab4ec-f99b-43f8-a30f-fc58e2b4e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeseriesDataset(Dataset):   \n",
    "    \"\"\"\n",
    "    We have to make this dataset a bit different than previous datasets because we are now using series\n",
    "    Instead of just getting a single x, y pair, we get multiple values of X at one time \n",
    "    The legnth of that sequence is seq_length\n",
    "    The output is then the y entry at the end of the sequence\n",
    "    i.e, we use a series of X data to predict the final output of that sequence\n",
    "    \"\"\"\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, seq_length: int = 1, accelerator_name: str = \"mps\",):\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        if accelerator_name == \"mps\":\n",
    "            self.device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        elif accelerator_name == \"cuda:0\":\n",
    "            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "    def __len__(self):\n",
    "        # we don't get as many sequences as there are datapoints\n",
    "        return len(self.X) - (self.seq_length - 1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # we grab a *sequence* rather than a single datapoint\n",
    "        x_ = self.X[index:index+self.seq_length].astype(np.float32)\n",
    "        \n",
    "        # then get the output at the *end* of the sequence\n",
    "        y_ = self.y[index+self.seq_length-1].astype(np.float32)\n",
    "        \n",
    "        return torch.from_numpy(x_).float().to(self.device), torch.from_numpy(np.array([y_])).float().to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2306690-e30c-4160-ab21-9f7412d2e3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### we'll use a canonical time-series dataset\n",
    "# https://www.kaggle.com/datasets/uciml/electric-power-consumption-data-set/\n",
    "data_name = \"household_power_consumption.txt\"\n",
    "data_dir = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25afa708-e62c-43d9-8705-2d90b6571ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv(\n",
    "    f\"{data_dir}{data_name}\", \n",
    "    sep=';', \n",
    "    parse_dates={'dt' : ['Date', 'Time']}, # merge time data entries (dd/mm/yyyy and hh:mm:ss) to get order\n",
    "    dayfirst=True,\n",
    "    na_values=['nan','?'], # impute with nans\n",
    "    index_col='dt' # order by time\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65313b7d-b7b5-49fe-8237-a2b5cf6f0940",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adb6359-8980-4698-918a-04730c4e29d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resample = df.resample('h').mean() # averages over each hour (reduces data and flucatuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa295a1-d6e6-422b-83d3-14508fd9147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f262f740-87e0-451a-b269-805c314e76e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data without any nans\n",
    "X = df_resample.dropna().copy()\n",
    "# remember, RNNs use previous outputs (y) as inputs\n",
    "# so we don't drop the corresponding data from the input\n",
    "# instead, we shift the input forward one so that\n",
    "# we predict the *next* input using all current features\n",
    "y = X['Global_active_power'].copy().shift(-1).ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edde7f3-942a-44cf-b456-1b0c8696b7a8",
   "metadata": {},
   "source": [
    "### Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b53d1e-6104-42da-a4aa-56ae9c7914a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = list(X.index)\n",
    "for i, time in enumerate(times):\n",
    "    times[i] = time.value * 1e-9 # turn TimeStamp objects into typical Unix Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f1c3a0-0b31-45e2-aa70-81fa36a60771",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10., 7.5))\n",
    "\n",
    "log = False\n",
    "vmin, vmax = None, None\n",
    "\n",
    "# make a log normalizer\n",
    "norm = LogNorm(vmin, vmax) if log else None\n",
    "\n",
    "plt.scatter(X['Global_active_power'].values, y.values, lw=lw, c=times, norm=norm, alpha=0.5, cmap=\"magma\")\n",
    "\n",
    "plt.plot(X['Global_active_power'].values, X['Global_active_power'].values, c=\"gray\", alpha=0.5, lw=lw-1, ls=\":\")\n",
    "\n",
    "cbar = plt.colorbar(fraction=0.045, pad=0.005)\n",
    "cbar.ax.set_ylabel(\"Time [Unix]\", rotation=270, fontsize=legends)\n",
    "cbar.ax.tick_params(labelsize=ticks)\n",
    "cbar.ax.get_yaxis().labelpad = 40\n",
    "\n",
    "plt.xlabel(\"Current Power [kW]\", fontsize=labels)\n",
    "plt.ylabel(\"Next Power [kW]\", fontsize=labels)\n",
    "\n",
    "plt.xticks(fontsize=ticks)\n",
    "plt.yticks(fontsize=ticks)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb890e0-1b52-4650-8e16-c233e39d61ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10., 7.5))\n",
    "\n",
    "log = False\n",
    "vmin, vmax = None, None\n",
    "\n",
    "# make a log normalizer\n",
    "norm = LogNorm(vmin, vmax) if log else None\n",
    "\n",
    "plt.scatter(times, X['Global_active_power'].values, lw=lw, c=y.values, alpha=1., cmap=\"Reds\")\n",
    "\n",
    "cbar = plt.colorbar(fraction=0.045, pad=0.005)\n",
    "cbar.ax.set_ylabel(\"Next power [kW]\", rotation=270, fontsize=legends)\n",
    "cbar.ax.tick_params(labelsize=ticks)\n",
    "cbar.ax.get_yaxis().labelpad = 40\n",
    "\n",
    "plt.xlabel(\"Time [Unix]\", fontsize=labels)\n",
    "plt.ylabel(\"Current Power [kW]\", fontsize=labels)\n",
    "\n",
    "\n",
    "plt.xticks(fontsize=ticks)\n",
    "plt.yticks(fontsize=ticks)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122e2c93-abcd-4011-a02a-55c96ebca4af",
   "metadata": {},
   "source": [
    "### Split data and make loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb20730-47f8-4f73-9a17-9d5b3b476bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into validation and training data\n",
    "val_split = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                   y,\n",
    "                                                   test_size=val_split,\n",
    "                                                   random_state=123,\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9df33aa-7d98-438a-b96f-c98632146e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into validation and training data\n",
    "val_split = 0.2\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, \n",
    "                                                   y_train,\n",
    "                                                   test_size=val_split,\n",
    "                                                   random_state=123,\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8a2f4c-90bc-455b-b14f-5ea947282bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = StandardScaler()\n",
    "preprocessing.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723a51a5-1cc9-4d37-ad68-82f19307e521",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocessing.transform(X_train)\n",
    "# if type(y_train) != np.ndarray:\n",
    "y_train = y_train.to_numpy()\n",
    "    # y_train = y_train.values.reshape((-1, 1))\n",
    "# else:\n",
    "    # y_train = y_train.reshape((-1, 1))\n",
    "\n",
    "X_val = preprocessing.transform(X_val)\n",
    "# y_val = y_train.values.reshape((-1, 1))\n",
    "# if type(y_train) != np.ndarray:\n",
    "y_val = y_val.to_numpy()#.reshape((-1, 1))\n",
    "\n",
    "X_test = preprocessing.transform(X_test)\n",
    "# if type(y_train) != np.ndarray:\n",
    "# y_test = y_train.values.reshape((-1, 1))\n",
    "y_test = y_test.to_numpy()#.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f076f37-9cf9-4ff6-8f73-b680a11fdf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dce9e6-b719-4b22-a418-4b64a26f9a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b893b206-0f17-48c1-a82a-adfe3fcdc4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator_name = \"mps\"\n",
    "# accelerator_name = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb92310a-c752-46b5-83ad-bc39dd3b9b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Now we actually make the dataset and dataloader in PyTorch fashion\n",
    "train_data = TimeseriesDataset(X_train, y_train, accelerator_name=accelerator_name, seq_length=seq_length)\n",
    "val_data = TimeseriesDataset(X_val, y_val, accelerator_name=accelerator_name, seq_length=seq_length)\n",
    "test_data = TimeseriesDataset(X_test, y_test, accelerator_name=accelerator_name, seq_length=seq_length)\n",
    "\n",
    "# make the loader\n",
    "train_loader = DataLoader(train_data, shuffle=False, batch_size=batch_size) # don't shuffle order!!!!\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6495201-9b8c-40ad-82ff-cf995c9cc713",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14554fd-ae0f-4ff8-ba0f-6aca11257d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fbe6afd-881d-46a7-b970-ad819b407c31",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71ae972-918b-432d-a8ba-ea6262edf695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, \n",
    "                 num_features: int = 7,\n",
    "                 seq_length = 25,\n",
    "                 num_rnn_layers = 5,\n",
    "                 hid_dim: int = 64,\n",
    "                 dropout: float = 0.25,\n",
    "                 lr: float = 1e-4, \n",
    "                 weight_decay: float = 0., \n",
    "                 eps: float = 5e-7, \n",
    "                 use_wandb: bool=False,\n",
    "                 scheduler_name: str = \"none\",\n",
    "                 step_size: int = 5,\n",
    "                 gamma: float = 0.5,\n",
    "                 batch_size: int = 32,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # this will make our lives easier because we pass tensors through multiple times\n",
    "        # self.automatic_optimization = False\n",
    "        \n",
    "        # model parameters\n",
    "        self.num_features = num_features\n",
    "        self.hid_dim = hid_dim\n",
    "        self.seq_length = seq_length\n",
    "        self.num_rnn_layers = num_rnn_layers\n",
    "        self.lr = lr\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.dropout = dropout\n",
    "        self.scheduler_name = scheduler_name\n",
    "        # if using a scheduler\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # log using WandB or TensorBoard\n",
    "        self.use_wandb = use_wandb\n",
    "\n",
    "        # what the input data looks like (allows construction of graph for logging)\n",
    "        # (batch_size, channels, height, width)\n",
    "\n",
    "        self.rnn = nn.GRU(input_size=num_features, \n",
    "                            hidden_size=hid_dim,\n",
    "                            num_layers=num_rnn_layers, \n",
    "                            dropout=dropout, \n",
    "                            batch_first=True,\n",
    "                          )\n",
    "        self.output_layer = nn.Linear(hid_dim, 1)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        ### initialize hidden state\n",
    "        self.h = self.init_hidden(self.batch_size)\n",
    "\n",
    "        self.example_input_array = [torch.zeros(\n",
    "                                    (batch_size, self.seq_length, self.num_features),\n",
    "                                    dtype=torch.float32\n",
    "                                    ),\n",
    "                                    self.h,\n",
    "                                   ]\n",
    "                     \n",
    "    def init_weights(self) -> None:\n",
    "        ### does some fancy layer weight initialization\n",
    "        nn.init.xavier_uniform_(self.output_layer.weight)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        '''Determines how data is passed through the network, \n",
    "           i.e creates the connectivity of the network'''\n",
    "        ### do I need to pass through hidden state?\n",
    "        x, _ = self.rnn(x, h.detach()) # gets the output and hidden state\n",
    "        # self.h = h.clone().to(self.device)\n",
    "        return self.output_layer(nn.GELU()(x[:,-1, :]))\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.num_rnn_layers, \n",
    "                           batch_size, \n",
    "                           self.hid_dim).float().to(self.device)\n",
    "\n",
    "    def configure_optimizers(self) -> (list, list):\n",
    "        \"\"\"Set up the optimizer and potential learning rate scheduler\"\"\"\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            eps=self.eps,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "\n",
    "        if self.scheduler_name == \"none\":\n",
    "            return self.optimizer\n",
    "\n",
    "        ### this decreases the learning rate by a factor of gamma every step_size\n",
    "        self.scheduler = MultiStepLR(\n",
    "            self.optimizer,\n",
    "            list(range(0, self.trainer.max_epochs, self.step_size)),\n",
    "            gamma=self.gamma,\n",
    "        )\n",
    "\n",
    "        return [self.optimizer], [{\"scheduler\": self.scheduler, \"interval\": \"epoch\"}]\n",
    "        \n",
    "    #### need to add these two things in case the scheduler is used ####\n",
    "    def lr_scheduler_step(self, scheduler, metric) -> None:\n",
    "        if self.scheduler_name != \"none\":\n",
    "            self.scheduler.step()\n",
    "\n",
    "    # def on_train_epoch_end(self):\n",
    "        # \"\"\"Reset hidden state after going through entire batch\"\"\"\n",
    "        # self.h = self.init_hidden(self.batch_size)\n",
    "\n",
    "    # def on_validation_epoch_end(self):\n",
    "        # self.h = self.init_hidden(self.batch_size)\n",
    "\n",
    "    # def on_test_epoch_end(self):\n",
    "        # self.h = self.init_hidden(self.batch_size)\n",
    "\n",
    "    def on_epoch_end(self) -> None:\n",
    "        # self.h = self.init_hidden(self.batch_size)\n",
    "        if self.scheduler_name != \"none\":\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def process_batch(self, batch, step: str = \"train\"):\n",
    "        \"\"\"Passes and logs a batch for a given type of step (test, train, validation)\"\"\"\n",
    "        \n",
    "        # get data (batch includes next step, which we don't need)\n",
    "        x, y = batch\n",
    "        # import pdb; pdb.set_trace()\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        h = self.init_hidden(batch_size)\n",
    "        # pass through network\n",
    "        yhat = self(x, h)\n",
    "        # import pdb; pdb.set_trace()\n",
    "\n",
    "        # get loss\n",
    "        loss = nn.MSELoss()(y, yhat)\n",
    "\n",
    "        self.log(f\"{step}_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"What do do with a training batch\"\"\"\n",
    "        \n",
    "        return self.process_batch(batch, step=\"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        '''Validation step (at the end of each epoch)'''\n",
    "        \n",
    "        return self.process_batch(batch, step=\"val\")\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \n",
    "        '''Test step is essentially the same as a validation step in this instance'''\n",
    "        return self.process_batch(batch, step=\"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b90d3b-dc8c-4332-8238-2211412e417d",
   "metadata": {},
   "source": [
    "# Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f8c9d1-5b46-4c82-947d-7c00f8ccca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyper parameters (fashion MNIST)\n",
    "lr = 5e-4\n",
    "eps = 1e-8\n",
    "weight_decay = 1e-6\n",
    "dropout = 0.25\n",
    "hidden_dim = 64\n",
    "seq_length = 25\n",
    "num_features = len(X.columns)\n",
    "scheduler_name = \"step\"\n",
    "gamma = 0.5\n",
    "step_size = 5\n",
    "num_rnn_layers = 5\n",
    "\n",
    "## WandB stuff\n",
    "# log with WandB or TensorBoard\n",
    "use_wandb = False\n",
    "# do hyperparameter sweep with WandB\n",
    "use_sweep = False\n",
    "# WandB project name\n",
    "project_name = \"basic_rnn\"\n",
    "# WandB lab name\n",
    "entity = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3be93bd-c7f3-448d-a9d6-e6061662a74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(num_features=num_features,\n",
    "            seq_length=seq_length,\n",
    "            num_rnn_layers=num_rnn_layers,\n",
    "            hid_dim=hidden_dim,\n",
    "            use_wandb=use_wandb,\n",
    "            dropout=dropout, \n",
    "            eps=eps, \n",
    "            lr=lr, \n",
    "            weight_decay=weight_decay,\n",
    "            scheduler_name=scheduler_name,\n",
    "            gamma=gamma,\n",
    "            step_size=step_size,\n",
    "            batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e713f18-1e42-4254-b743-a91e0762f627",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3e51e7-265c-4e6e-a0f6-d8303b63ae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae2e6d5-4bb1-4e6b-b577-0f95368b9b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boilerplate to get GPU if possible\n",
    "if accelerator_name == \"mps\":\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "elif accelerator_name == \"cuda\":\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be29b54-f172-4fa6-9d3a-005ab1118726",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_wandb:\n",
    "    # %load_ext tensorboard\n",
    "    cnn_logger = TensorBoardLogger(\"rnn_logs\", name=\"simple_rnn\")\n",
    "    run_name = \"rnn\"\n",
    "else:\n",
    "    logger_kwargs = {\n",
    "        \"resume\": \"allow\",\n",
    "        \"config\": model_hparams,\n",
    "    }\n",
    "    cnn_logger = WandbLogger(project=project_name, entity=entity, **logger_kwargs)\n",
    "    cnn_run_name = cnn_logger.experiment.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40546804-860e-4843-844d-d9991d7928ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### necessary for newer PTL versions\n",
    "devices = 1\n",
    "accelerator = \"gpu\" if devices == 1 else \"cpu\"\n",
    "\n",
    "# make the trainer\n",
    "trainer = pl.Trainer(\n",
    "    devices=devices,\n",
    "    accelerator=accelerator,\n",
    "    max_epochs=num_epochs,\n",
    "    log_every_n_steps=1,\n",
    "    logger=cnn_logger,\n",
    "    # reload_dataloaders_every_epoch=True,\n",
    "    callbacks=[\n",
    "        # ModelCheckpoint(\n",
    "        #     save_weights_only=False,\n",
    "        #     mode=\"min\",\n",
    "        #     monitor=\"val_acc\",\n",
    "        #     save_top_k=1,\n",
    "        #     every_n_epochs=1,\n",
    "        #     save_on_train_epoch_end=False,\n",
    "        #     dirpath=f\"/RNN_Checkpoints/{run_name}/\",\n",
    "        #     filename=f\"rnn_checkpoint_{run_name}\",\n",
    "        # ),\n",
    "        LearningRateMonitor(\"epoch\"),\n",
    "        progress.TQDMProgressBar(refresh_rate=1),\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            min_delta=0,\n",
    "            patience=15,\n",
    "            verbose=False,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "trainer.logger._log_graph = True\n",
    "trainer.logger._default_hp_metric = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84a66ac-d6a2-4d68-a055-e747ee19d30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17de9002-2cb8-47ee-a506-fd4be6657119",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e4617c-8140-41dc-a71b-ecd91765f74f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17146c0d-e51e-426e-8f48-c0b9c2b424d7",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0053dcb-7324-4616-97a0-edbcb571d067",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get test metrics\n",
    "test_results = trainer.test(model, test_loader)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9514da55-40ba-480f-b3b9-3e61b70abbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## open up TensorBoard\n",
    "if not use_wandb:\n",
    "    %tensorboard --logdir rnn_logs --port 6001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a9b986-59ca-4b83-ad97-526215bd5485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_loader):\n",
    "    \"\"\"Generate predictions for the test data.\"\"\"\n",
    "    \n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize an empty list to collect predictions\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():  # disable gradient computation during prediction\n",
    "        for batch in test_loader:\n",
    "            # Assuming batch is a tuple where the first item is the input data\n",
    "            inputs = batch[0].to(torch.device(\"cpu\"))\n",
    "\n",
    "            # Use the model to generate predictions\n",
    "            h = model.init_hidden(len(inputs)).to(torch.device(\"cpu\"))\n",
    "            predictions = model(inputs, h)\n",
    "\n",
    "            # Store the predictions\n",
    "            all_predictions.append(predictions)\n",
    "\n",
    "    # Concatenate all predictions into a single tensor for convenience\n",
    "    return torch.cat(all_predictions, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77af0e2f-f430-4f23-95e8-398f8af68a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = predict(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e87086-2cae-4d80-a5c6-b294a17fa585",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = y_preds.detach().cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d200005e-e404-4b16-82f5-479e8b9a1abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "errs = (y_test - y_preds) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab06ea7e-da5b-4b15-92e1-5a89ee550fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10., 7.5))\n",
    "\n",
    "log = True\n",
    "vmin, vmax = None, None\n",
    "\n",
    "# make a log normalizer for colorbar\n",
    "norm = LogNorm(vmin, vmax) if log else None\n",
    "\n",
    "plt.scatter(y_test, \n",
    "            y_preds, \n",
    "            lw=lw, \n",
    "            c=errs, \n",
    "            alpha=1., \n",
    "            cmap=\"Greens_r\",\n",
    "            norm=norm,\n",
    "           )\n",
    "\n",
    "\n",
    "cbar = plt.colorbar(fraction=0.045, pad=0.005,)\n",
    "cbar.ax.set_ylabel(\"Squared Error\", rotation=270, fontsize=legends)\n",
    "cbar.ax.tick_params(labelsize=ticks)\n",
    "cbar.ax.get_yaxis().labelpad = 40\n",
    "\n",
    "plt.xlabel(r\"$y_{t}$\", fontsize=labels)\n",
    "plt.ylabel(r\"$\\hat{y_{t}}$\", fontsize=labels)\n",
    "\n",
    "\n",
    "plt.xticks(fontsize=ticks)\n",
    "plt.yticks(fontsize=ticks)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957a95b6-0d77-4b54-a651-4ee1b979ac18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
